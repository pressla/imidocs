{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Imitrix Docs","text":"<p>Welcome to the official documentation hub for Imitrix. This documentation serves as a comprehensive collection of guides, cheat sheets, and reference materials for various tools and technologies.</p>"},{"location":"#available-documentation","title":"Available Documentation","text":""},{"location":"#kubernetes-resources","title":"Kubernetes Resources","text":"<ul> <li>Kubernetes Cheat Sheet - Quick reference guide for common Kubernetes commands and operations</li> <li>Kind Kubernetes - Guide for working with Kind (Kubernetes in Docker)</li> <li>RKE2 Deployment - Instructions for deploying RKE2 clusters</li> <li>Kubernetes Deployment Patterns - Guide to deployment strategies and patterns in Kubernetes</li> </ul>"},{"location":"#development-tools","title":"Development Tools","text":"<ul> <li>Vim Commands - Reference guide for Vim text editor commands</li> <li>MkDocs Cheat Sheet - Quick reference for MkDocs documentation system</li> <li>Streamlit Guide - Guide for working with Streamlit applications</li> </ul>"},{"location":"#documentation-features","title":"Documentation Features","text":"<p>This documentation is built with MkDocs using the Material theme and includes support for:</p> <ul> <li>Mermaid diagrams for visualizing workflows and architectures</li> <li>PlantUML for UML diagrams</li> <li>Code syntax highlighting</li> <li>Advanced markdown features</li> </ul> <p>Feel free to navigate through the sections using the navigation menu to find detailed information about specific topics.</p>"},{"location":"30-100/","title":"Containers","text":"<p>Containers are the building blocks for Kubernetes-based cloud native applications. If we make a comparison with OOP and Java, container images are like classes, and containers are like objects. The same way we can extend classes to reuse and alter behavior, we can have container images that extend other container images to reuse and alter behavior. The same way we can do object composition and use functionality, we can do container compositions by putting containers into a Pod and using collaborating containers.</p> <p>If we continue the comparison, Kubernetes would be like the JVM but spread over multiple hosts, and it would be responsible for running and managing the containers. Init containers would be something like object constructors; DaemonSets would be similar to daemon threads that run in the background (like the Java Garbage Collector, for example). A Pod would be something similar to an Inversion of Control (IoC) context (Spring Framework, for example), where multiple running objects share a managed lifecycle and can access one another directly.</p> <p>The parallel doesn't go much further, but the point is that containers play a fundamental role in Kubernetes, and creating modularized, reusable, single-purpose container images is fundamental to the long-term success of any project and even the containers' ecosystem as a whole. Apart from the technical characteristics of a container image that provide packaging and isolation, what does a container represent, and what is its purpose in the context of a distributed application? Here are a few suggestions on how to look at containers:</p> <ul> <li>A container image is the unit of functionality that addresses a single concern.</li> <li>A container image is owned by one team and has its own release cycle.</li> <li>A container image is self-contained and defines and carries its runtime dependencies.</li> <li>A container image is immutable, and once it is built, it does not change; it is configured.</li> <li>A container image defines its resource requirements and external dependencies.</li> <li>A container image has well-defined APIs to expose its functionality.</li> <li>A container typically runs as a single Unix process.</li> <li>A container is disposable and safe to scale up or down at any moment.</li> </ul> <p>In addition to all these characteristics, a proper container image is modular. It is parameterized and created for reuse in the different environments in which it is going to run. Having small, modular, and reusable container images leads to the creation of more specialized and stable container images in the long term, similar to a great reusable library in the programming language world.</p>"},{"location":"30-100/#pods","title":"Pods","text":"<p>Looking at the characteristics of containers, we can see that they are a perfect match for implementing the microservices principles. A container image provides a single unit of functionality, belongs to a single team, has an independent release cycle, and provides deployment and runtime isolation. Most of the time, one microservice corresponds to one container image.</p> <p>However, most cloud native platforms offer another primitive for managing the lifecycle of a group of containers\u2014in Kubernetes, it is called a Pod. A Pod is an atomic unit of scheduling, deployment, and runtime isolation for a group of containers. All containers in a Pod are always scheduled to the same host, are deployed and scaled together, and can also share filesystem, networking, and process namespaces. This joint lifecycle allows the containers in a Pod to interact with one another over the filesystem or through networking via localhost or host interprocess communication mechanisms if desired (for performance reasons, for example). A Pod also represents a security boundary for an application. While it is possible to have containers with varying security parameters in the same Pod, typically all containers would have the same access level, network segmentation, and identity.</p> <p>As you can see in Figure 1-2, at development and build time, a microservice corresponds to a container image that one team develops and releases. But at runtime, a microservice is represented by a Pod, which is the unit of deployment, placement, and scaling. The only way to run a container\u2014whether for scale or migration\u2014is through the Pod abstraction. Sometimes a Pod contains more than one container. In one such example, a containerized microservice uses a helper container at runtime, as Chapter 16, \"Sidecar\", demonstrates.</p> <p>Containers, Pods, and their unique characteristics offer a new set of patterns and principles for designing microservices-based applications. We saw some of the characteristics of well-designed containers; now let's look at some characteristics of a Pod:</p> <ul> <li> <p>A Pod is the atomic unit of scheduling. That means the scheduler tries to find a host that satisfies the requirements of all containers that belong to the Pod (we cover some specifics around init containers in Chapter 15, \"Init Container\"). If you create a Pod with many containers, the scheduler needs to find a host that has enough resources to satisfy all container demands combined. This scheduling process is described in Chapter 6, \"Automated Placement\".</p> </li> <li> <p>A Pod ensures colocation of containers. Thanks to the colocation, containers in the same Pod have additional means to interact with one another. The most common ways of communicating include using a shared local filesystem for exchanging data, using the localhost network interface, or using some host interprocess communication (IPC) mechanism for high-performance interactions.</p> </li> <li> <p>A Pod has an IP address, name, and port range that are shared by all containers belonging to it. That means containers in the same Pod have to be carefully configured to avoid port clashes, in the same way that parallel, running Unix processes have to take care when sharing the networking space on a host.</p> </li> </ul> <p>A Pod is the atom of Kubernetes where your application lives, but you don't access Pods directly\u2014that is where Services enter the scene.</p>"},{"location":"30-100/#services","title":"Services","text":"<p>Pods are ephemeral. They come and go at any time for all sorts of reasons (e.g., scaling up and down, failing container health checks, node migrations). A Pod IP address is known only after it is scheduled and started on a node. A Pod can be rescheduled to a different node if the existing node it is running on is no longer healthy. This means the Pod's network address may change over the life of an application, and there is a need for another primitive for discovery and load balancing.</p> <p>That's where the Kubernetes Services come into play. The Service is another simple but powerful Kubernetes abstraction that binds the Service name to an IP address and port number permanently. So a Service represents a named entry point for accessing an application. In the most common scenario, the Service serves as the entry point for a set of Pods, but that might not always be the case. The Service is a generic primitive, and it may also point to functionality provided outside the Kubernetes cluster. As such, the Service primitive can be used for Service discovery and load balancing, and it allows altering implementations and scaling without affecting Service consumers. We explain Services in detail in Chapter 13, \"Service Discovery\".</p>"},{"location":"30-100/#labels","title":"Labels","text":"<p>We have seen that a microservice is a container image at build time but is represented by a Pod at runtime. So what is an application that consists of multiple microservices? Here, Kubernetes offers two more primitives that can help you define the concept of an application: labels and namespaces.</p> <p>Before microservices, an application corresponded to a single deployment unit with a single versioning scheme and release cycle. There was a single file for an application in a .war, .ear, or some other packaging format. But then, applications were split into microservices, which are independently developed, released, run, restarted, or scaled. With microservices, the notion of an application diminishes, and there are no key artifacts or activities that we have to perform at the application level. But if you still need a way to indicate that some independent services belong to an application, labels can be used. Let's imagine that we have split one monolithic application into three microservices and another one into two microservices.</p> <p>We now have five Pod definitions (and maybe many more Pod instances) that are independent of the development and runtime points of view. However, we may still need to indicate that the first three Pods represent an application and the other two Pods represent another application. Even the Pods may be independent, to provide a business value, but they may depend on one another. For example, one Pod may contain the containers responsible for the frontend, and the other two Pods are responsible for providing the backend functionality. If either of these Pods is down, the application is useless from a business point of view. Using label selectors gives us the ability to query and identify a set of Pods and manage it as one logical unit.</p> <p>Here are a few examples where labels can be useful:</p> <ul> <li> <p>Labels are used by ReplicaSets to keep some instances of a specific Pod running. That means every Pod definition needs to have a unique combination of labels used for scheduling.</p> </li> <li> <p>Labels are also heavily used by the scheduler. The scheduler uses labels for colocating or spreading Pods to the nodes that satisfy the Pods' requirements.</p> </li> <li> <p>A label can indicate a logical grouping of a set of Pods and give an application identity to them.</p> </li> <li> <p>In addition to the preceding typical use cases, labels can be used to store metadata. It may be difficult to predict what a label could be used for, but it is best to have enough labels to describe all important aspects of the Pods. For example, having labels to indicate the logical group of an application, the business characteristics and criticality, the specific runtime platform dependencies such as hardware architecture, or location preferences are all useful.</p> </li> </ul> <p>Later, these labels can be used by the scheduler for more fine-grained scheduling, or the same labels can be used from the command line for managing the matching Pods at scale. However, you should not go overboard and add too many labels in advance. You can always add them later if needed. Removing labels is much riskier as there is no straightforward way of finding out what a label is used for and what unintended effect such an action may cause.</p>"},{"location":"30-100/#annotations","title":"Annotations","text":"<p>Another primitive very similar to labels is the annotation. Like labels, annotations are organized as a map, but they are intended for specifying nonsearchable metadata and for machine usage rather than human.</p> <p>The information on the annotations is not intended for querying and matching objects. Instead, it is intended for attaching additional metadata to objects from various tools and libraries we want to use. Some examples of using annotations include build IDs, release IDs, image information, timestamps, Git branch names, pull request numbers, image hashes, registry addresses, author names, tooling information, and more. So while labels are used primarily for query matching and performing actions on the matching resources, annotations are used to attach metadata that can be consumed by a machine.</p>"},{"location":"30-100/#namespaces","title":"Namespaces","text":"<p>Another primitive that can also help manage a group of resources is the Kubernetes namespace. As we have described, a namespace may seem similar to a label, but in reality, it is a very different primitive with different characteristics and purposes.</p> <p>Kubernetes namespaces allow you to divide a Kubernetes cluster (which is usually spread across multiple hosts) into a logical pool of resources. Namespaces provide scopes for Kubernetes resources and a mechanism to apply authorizations and other policies to a subsection of the cluster. The most common use case of namespaces is representing different software environments such as development, testing, integration testing, or production. Namespaces can also be used to achieve multitenancy and provide isolation for team workspaces, projects, and even specific applications. But ultimately, for a greater isolation of certain environments, namespaces are not enough, and having separate clusters is common. Typically, there is one nonproduction Kubernetes cluster used for some environments (development, testing, and integration testing) and another production Kubernetes cluster to represent performance testing and production environments.</p> <p>Let's look at some of the characteristics of namespaces and how they can help us in different scenarios:</p> <ul> <li> <p>A namespace is managed as a Kubernetes resource.</p> </li> <li> <p>A namespace provides scope for resources such as containers, Pods, Services, or ReplicaSets. The names of resources need to be unique within a namespace but not across them.</p> </li> <li> <p>By default, namespaces provide scope for resources, but nothing isolates those resources and prevents access from one resource to another. For example, a Pod from a development namespace can access another Pod from a production namespace as long as the Pod IP address is known. \"Network isolation across namespaces for creating a lightweight multitenancy solution is described in Chapter 24, \"Network Segmentation\".</p> </li> <li> <p>Some other resources, such as namespaces, nodes, and PersistentVolumes, do not belong to namespaces and should have unique cluster-wide names.</p> </li> <li> <p>Each Kubernetes Service belongs to a namespace and gets a corresponding Domain Name Service (DNS) record that has the namespace in the form of ..svc.cluster.local. So the namespace name is in the URL of every Service belonging to the given namespace. That's one reason it is vital to name namespaces wisely. <li> <p>ResourceQuotas provide constraints that limit the aggregated resource consumption per namespace. With ResourceQuotas, a cluster administrator can control the number of objects per type that are allowed in a namespace. For example, a developer namespace may allow only five ConfigMaps, five Secrets, five Services, five ReplicaSets, five PersistentVolumeClaims, and ten Pods.</p> </li> <li> <p>ResourceQuotas can also limit the total sum of computing resources we can request in a given namespace. For example, in a cluster with a capacity of 32 GB RAM and 16 cores, it is possible to allocate 16 GB RAM and 8 cores for the production namespace, 8 GB RAM and 4 cores for the staging environment, 4 GB RAM and 2 cores for development, and the same amount for testing namespaces. The ability to impose resource constraints decoupled from the shape and the limits of the underlying infrastructure is invaluable.</p> </li>"},{"location":"30-100/#discussion","title":"Discussion","text":"<p>We've only briefly covered a few of the main Kubernetes concepts we use in this book. However, there are more primitives used by developers on a day-by-day basis. For example, if you create a containerized service, there are plenty of Kubernetes abstractions you can use to reap all the benefits of Kubernetes. Keep in mind, these are only a few of the objects used by application developers to integrate a containerized service into Kubernetes. There are plenty of other concepts used primarily by cluster administrators for managing Kubernetes.</p> <p>With time, these new primitives give birth to new ways of solving problems, and some of these repetitive solutions become patterns. Throughout this book, rather than describing each Kubernetes resource in detail, we will focus on concepts that are proven as patterns.</p>"},{"location":"30-100/#more-information","title":"More Information","text":"<ul> <li>The Twelve-Factor App</li> <li>CNCF Cloud Native Definition v1.0</li> <li>Hexagonal Architecture</li> <li>Domain-Driven Design: Tackling Complexity in the Heart of Software</li> <li>Best Practices for Writing Dockerfiles</li> <li>Principles of Container-Based Application Design</li> <li>General Container Image Guidelines</li> </ul>"},{"location":"30-100/#part-i-foundational-patterns","title":"PART I: Foundational Patterns","text":"<p>Foundational patterns describe a number of fundamental principles that containerized applications must comply with in order to become good cloud-native citizens. Adhering to these principles will help ensure that your applications are suitable for automation in cloud-native platforms such as Kubernetes.</p> <p>The patterns described in the following chapters represent the foundational building blocks of distributed container-based Kubernetes-native applications:</p> <ul> <li>Chapter 2, \"Predictable Demands\", explains why every container should declare its resource requirements and stay confined to the indicated resource boundaries.</li> <li>Chapter 3, \"Declarative Deployment\", describes the different application deployment strategies that can be expressed in a declarative way.</li> <li>Chapter 4, \"Health Probe\", dictates that every container should implement specific APIs to help the platform observe and maintain the application healthily.</li> <li>Chapter 5, \"Managed Lifecycle\", explains why a container should have a way to read the events coming from the platform and conform by reacting to those events.</li> <li>Chapter 6, \"Automated Placement\", introduces the Kubernetes scheduling algorithm and the ways to influence the placement decisions from the outside.</li> </ul>"},{"location":"30-100/#chapter-2-predictable-demands","title":"Chapter 2: Predictable Demands","text":"<p>The foundation of successful application deployment, management, and coexistence on a shared cloud environment is dependent on identifying and declaring the application resource requirements and runtime dependencies. This Predictable Demands pattern indicates how you should declare application requirements, whether they are hard runtime dependencies or resource requirements. Declaring your requirements is essential for Kubernetes to find the right place for your application within the cluster.</p>"},{"location":"30-100/#problem","title":"Problem","text":"<p>Kubernetes can manage applications written in different programming languages as long as the application can be run in a container. However, different languages have different resource requirements. Typically, a compiled language runs faster and often requires less memory compared to just-in-time runtimes or interpreted languages. Considering that many modern programming languages in the same category have similar resource requirements, from a resource consumption point of view, more important aspects are the domain, the business logic of an application, and the actual implementation details.</p> <p>Besides resource requirements, application runtimes also have dependencies on platform-managed capabilities like data storage or application configuration.</p>"},{"location":"30-100/#solution","title":"Solution","text":"<p>Knowing the runtime requirements for a container is important mainly for two reasons. First, with all the runtime dependencies defined and resource demands envisaged, Kubernetes can make intelligent decisions about where to place a container on the cluster for the most efficient hardware utilization. In an environment with shared resources among a large number of processes with different priorities, the only way to ensure a successful coexistence is to know the demands of every process in advance.</p> <p>Container resource profiles are also essential for capacity planning. Based on the particular service demands and the total number of services, we can do some capacity planning for different environments and come up with the most cost-effective host profiles to satisfy the entire cluster demand. Service resource profiles and capacity planning go hand in hand for successful cluster management in the long term.</p>"},{"location":"30-100/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>One of the most common runtime dependencies is file storage for saving application state. Container filesystems are ephemeral and are lost when a container is shut down. Kubernetes offers volume as a Pod-level storage utility that survives container restarts.</p> <p>The most straightforward type of volume is emptyDir, which lives as long as the Pod lives. When the Pod is removed, its content is also lost. The volume needs to be backed by another kind of storage mechanism to survive Pod restarts. If your application needs to read or write files to such long-lived storage, you must declare that dependency explicitly in the container definition using volumes.</p> <p>The scheduler evaluates the kind of volume a Pod requires, which affects where the Pod gets placed. If the Pod needs a volume that is not provided by any node on the cluster, the Pod is not scheduled at all. Volumes are an example of a runtime dependency that affects what kind of infrastructure a Pod can run and whether the Pod can be scheduled at all.</p> <p>A similar dependency happens when you ask Kubernetes to expose a container port on a specific port on the host system through hostPort. The usage of a hostPort creates another runtime dependency on the nodes and limits where a Pod can be scheduled. hostPort reserves the port on each node in the cluster and is limited to a maximum of one Pod scheduled per node. Because of port conflicts, you can scale to as many Pods as there are nodes in the Kubernetes cluster.</p> <p>Configurations are another type of dependency. Almost every application needs some configuration information, and the recommended solution offered by Kubernetes is through ConfigMaps. Your services need to have a strategy for consuming settings\u2014either through environment variables or the filesystem. In either case, this introduces a runtime dependency of your container to the named ConfigMaps. If not all of the expected ConfigMaps are created, the containers are scheduled on a node, but they do not start up.</p> <p>Similar to ConfigMaps, Secrets offer a slightly more secure way of distributing environment-specific configurations to a container. The way to consume a Secret is the same as it is for ConfigMaps, and using a Secret introduces the same kind of dependency from a container to a namespace.</p> <p>While the creation of ConfigMap and Secret objects are simple deployment tasks we have to perform, cluster nodes provide storage and port numbers. Some of these dependencies limit where a Pod gets scheduled (if anywhere at all), and other dependencies may prevent the Pod from starting up. When designing your containerized applications with such dependencies, always consider the runtime constraints they will create later.</p>"},{"location":"30-100/#resource-profiles","title":"Resource Profiles","text":"<p>Specifying container dependencies such as ConfigMap, Secret, and volumes is straightforward. We need some more thinking and experimentation for figuring out the resource requirements of a container. Compute resources in the context of Kubernetes are defined as something that can be requested by, allocated to, and consumed from a container. The resources are categorized as compressible (i.e., can be throttled, such as CPU or network bandwidth) and incompressible (i.e., cannot be throttled, such as memory).</p> <p>Making the distinction between compressible and incompressible resources is important. If your containers consume too many compressible resources such as CPU, they are throttled, but if they use too many incompressible resources (such as memory), they are killed (as there is no other way to ask an application to release allocated memory).</p> <p>Based on the nature and the implementation details of your application, you have to specify the minimum amount of resources that are needed (called requests) and the maximum amount it can grow up to (the limits). Every container definition can specify the amount of CPU and memory it needs in the form of a request and limit. At a high level, the concept of requests/limits is similar to soft/hard limits. For example, similarly, we define heap size for a Java application by using the -Xms and -Xmx command-line options.</p> <p>The requests amount (but not limits) is used by the scheduler when placing Pods to nodes. For a given Pod, the scheduler considers only nodes that still have enough capacity to accommodate the Pod and all of its containers by summing up the requested resource amounts. In that sense, the requests field of each container affects where a Pod can be scheduled or not.</p> <p>The following types of resources can be used as keys in the requests and limits specification:</p> <p>memory This type is for the heap memory demands of your application, including volumes of type emptyDir with the configuration medium: Memory. Memory resources are incompressible, so containers that exceed their configured memory limit will trigger the Pod to be evicted; i.e., it gets deleted and recreated potentially on a different node.</p> <p>cpu The cpu type is used to specify the range of needed CPU cycles for your application. However, it is a compressible resource, which means that in an overcommit situation for a node, all assigned CPU slots of all running containers are throttled relative to their specified requests. Therefore, it is highly recommended that you set requests for the CPU resource but no limits so that they can benefit from all excess CPU resources that otherwise would be wasted.</p> <p>ephemeral-storage Every node has some filesystem space dedicated for ephemeral storage that holds logs and writable container layers. emptyDir volumes that are not stored in a memory filesystem also use ephemeral storage. With this request and limit type, you can specify the application's minimal and maximal needs. ephemeral-storage resources are not compressible and will cause a Pod to be evicted from the node if it uses more storage than specified in its limit.</p> <p>hugepage- Huge pages are large, contiguous pre-allocated pages of memory that can be mounted as volumes. Depending on your Kubernetes node configuration, several sizes of huge pages are available, like 2 MB and 1 GB pages. You can specify a request and limit for how many of a certain type of huge pages you want to consume (e.g., hugepages-1Gi: 2Gi for requesting two 1 GB huge pages). Huge pages can't be overcommitted, so the request and limit must be the same. <p>Depending on whether you specify the requests, the limits, or both, the platform offers three types of Quality of Service (QoS):</p> <p>Best-Effort Pods that do not have any requests and limits set for its containers have a QoS of Best-Effort. Such a Best-Effort Pod is considered the lowest priority and is most likely killed first when the node where the Pod is placed runs out of incompressible resources.</p> <p>Burstable A Pod that defines an unequal amount for requests and limits values (and limits is larger than requests, as expected) are tagged as Burstable. Such a Pod has minimal resource guarantees but is also willing to consume more resources up to its limit when available. When the node is under incompressible resource pressure, these Pods are likely to be killed if no Best-Effort Pods remain.</p> <p>Guaranteed A Pod that has an equal amount of request and limit resources belongs to the Guaranteed QoS category. These are the highest-priority Pods and are guaranteed not to be killed before Best-Effort and Burstable Pods. This QoS mode is the best option for your application's memory resources, as it entails the least surprise and avoids out-of-memory triggered evictions.</p> <p>So the resource characteristics you define or omit for the containers have a direct impact on its QoS and define the relative importance of the Pod in the event of resource starvation. Define your Pod resource requirements with this consequence in mind.</p>"},{"location":"30-100/#recommendations-for-cpu-and-memory-resources","title":"Recommendations for CPU and Memory Resources","text":"<p>While you have many options for declaring the memory and CPU needs of your applications, we and others recommend the following rules:</p> <ul> <li>For memory, always set requests equal to limits.</li> <li>For CPU, set requests but no limits.</li> </ul> <p>See the blog post \"For the Love of God, Stop Using CPU Limits on Kubernetes\" for a more in-depth explanation of why you should not use limits for the CPU, and see the blog post \"What Everyone Should Know About Kubernetes Memory Limits\" for more details about the recommended memory settings.</p>"},{"location":"30-100/#pod-priority","title":"Pod Priority","text":"<p>We explained how container resource declarations also define Pods' QoS and affect the order in which the Kubelet kills the container in a Pod in case of resource starvation. Two other related concepts are Pod priority and preemption. Pod priority allows you to indicate the importance of a Pod relative to other Pods, which affects the order in which Pods are scheduled.</p> <p>Pod priority affects the order in which the scheduler places Pods on nodes. First, the priority admission controller uses the priorityClassName field to populate the priority value for new Pods. When multiple Pods are waiting to be placed, the scheduler sorts the queue of pending Pods by highest priority first. Any pending Pod is picked before any other pending Pod with lower priority in the scheduling queue, and if there are no constraints preventing it from scheduling, the Pod gets scheduled.</p> <p>Here comes the critical part. If there are no nodes with enough capacity to place a Pod, the scheduler can preempt (remove) lower-priority Pods from nodes to free up resources and place Pods with higher priority. As a result, the higher-priority Pod might be scheduled sooner than Pods with a lower priority if all other scheduling requirements are met. This algorithm effectively enables cluster administrators to control which Pods are more critical workloads and place them first by allowing the scheduler to evict Pods with lower priority to make room on a worker node for higher-priority Pods. If a Pod cannot be scheduled, the scheduler continues with the placement of other lower-priority Pods.</p> <p>Suppose you want your Pod to be scheduled with a particular priority but don't want to evict any existing Pods. In that case, you can mark a PriorityClass with the field preemptionPolicy: Never. Pods assigned to this priority class will not trigger any eviction of running Pods but will still get scheduled according to their priority value.</p> <p>Pod QoS (discussed previously) and Pod priority are two orthogonal features that are not connected and have only a little overlap. QoS is used primarily by the Kubelet to preserve node stability when available compute resources are low. The Kubelet first considers QoS and then the PriorityClass of Pods before eviction. On the other hand, the scheduler eviction logic ignores the QoS of Pods entirely when choosing preemption targets. The scheduler attempts to pick a set of Pods with the lowest priority possible that satisfies the needs of higher-priority Pods waiting to be placed.</p> <p>When Pods have a priority specified, it can have an undesired effect on other Pods that are evicted. For example, while a Pod's graceful termination policies are respected, the PodDisruptionBudget as discussed in Chapter 10, \"Singleton Service\", is not guaranteed, which could break a lower-priority clustered application that relies on a quorum of Pods.</p> <p>Another concern is a malicious or uninformed user who creates Pods with the highest possible priority and evicts all other Pods. To prevent that, ResourceQuota has been extended to support PriorityClass, and higher-priority numbers are reserved for critical system-Pods that should not usually be preempted or evicted.</p> <p>In conclusion, Pod priorities should be used with caution because user-specified numerical priorities that guide the scheduler and Kubelet about which Pods to place or to kill are subject to gaming by users. Any change could affect many Pods and could prevent the platform from delivering predictable service-level agreements.</p>"},{"location":"30-100/#project-resources","title":"Project Resources","text":"<p>Kubernetes is a self-service platform that enables developers to run applications as they see suitable on the designated isolated environments. However, working in a shared multitenanted platform also requires the presence of specific boundaries and control units to prevent some users from consuming all the platform's resources. One such tool is ResourceQuota, which provides constraints for limiting the aggregated resource consumption in a namespace. With ResourceQuotas, the cluster administrators can limit the total sum of computing resources (CPU, memory) and storage consumed. It can also limit the total number of objects (such as ConfigMaps, Secrets, Pods, or Services) created in a namespace.</p> <p>Another helpful tool in this area is LimitRange, which allows you to set resource usage limits for each type of resource. In addition to specifying the minimum and maximum permitted amounts for different resource types and the default values for these resources, it also allows you to control the ratio between the requests and limits, also known as the overcommit level.</p> <p>LimitRanges help control the container resource profiles so that no containers require more resources than a cluster node can provide. LimitRanges can also prevent cluster users from creating containers that consume many resources, making the nodes not allocatable for other containers. Considering that the requests (and not limits) are the primary container characteristic the scheduler uses for placing, LimitRequestRatio allows you to control the amount of difference between the requests and limits of containers. A big combined gap between requests and limits increases the chances of overcommitting on the node and may degrade application performance when many containers simultaneously require more resources than initially requested.</p> <p>Keep in mind that other shared node-level resources such as process IDs (PIDs) can be exhausted before hitting any resource limits. Kubernetes allows you to reserve a number of node PIDs for the system use and ensure that they are never exhausted by user workloads. Similarly, Pod PID limits allow a cluster administrator to limit the number of processes running in a Pod. We are not reviewing these in details here as they are set as Kubelet configurations options by cluster administrators and are not used by application developers.</p>"},{"location":"30-100/#capacity-planning","title":"Capacity Planning","text":"<p>Considering that containers may have different resource profiles in different environments, and a varied number of instances, it is evident that capacity planning for a multipurpose environment is not straightforward. For example, for best hardware utilization, on a nonproduction cluster, you may have mainly Best-Effort and Burstable containers. In such a dynamic environment, many containers are starting up and shutting down at the same time, and even if a container gets killed by the platform during resource starvation, it is not fatal. On the production cluster, where we want things to be more stable and predictable, the containers may be mainly of the Guaranteed type, and some may be Burstable. If a container gets killed, that is most likely a sign that the capacity of the cluster should be increased.</p> <p>Of course, in a real-life scenario, the more likely reason you are using a platform such as Kubernetes is that there are many more services to manage, some of which are about to retire, and some of which are still in the design and development phase. Even if it is a continually moving target, based on a similar approach as described previously, we can calculate the total amount of resources needed for all the services per environment.</p> <p>Keep in mind that in the different environments, there are different numbers of containers, and you may even need to leave some room for autoscaling, build jobs, infrastructure containers, and more. Based on this information and the infrastructure provider, you can choose the most cost-effective compute instances that provide the required resources.</p>"},{"location":"30-100/#discussion_1","title":"Discussion","text":"<p>Containers are useful not only for process isolation and as a packaging format. With identified resource profiles, they are also the building blocks for successful capacity planning. Perform some early tests to discover the resource needs for each container, and use that information as a base for future capacity planning and prediction.</p> <p>Kubernetes can help you here with the Vertical Pod Autoscaler (VPA), which monitors the resource consumption of your Pod over time and gives a recommendation for requests and limits. The VPA is described in detail in \"Vertical Pod Autoscaling\" on page 325.</p> <p>However, more importantly, resource profiles are the way an application communicates with Kubernetes to assist in scheduling and managing decisions. If your application doesn't provide any requests or limits, all Kubernetes can do is treat your containers as opaque boxes that are dropped when the cluster gets full. So it is more or less mandatory for every application to think about and provide these resource declarations.</p> <p>Now that you know how to size our applications, in Chapter 3, \"Declarative Deployment\", you will learn multiple strategies to install and update our applications on Kubernetes.</p>"},{"location":"30-100/#more-information_1","title":"More Information","text":"<ul> <li>Predictable Demands Example</li> <li>Configure a Pod to Use a ConfigMap</li> <li>Kubernetes Best Practices: Resource Requests and Limits</li> <li>Resource Management for Pods and Containers</li> <li>Manage HugePages</li> <li>Configure Default Memory Requests and Limits for a Namespace</li> <li>Node-Pressure Eviction</li> <li>Pod Priority and Preemption</li> <li>Configure Quality of Service for Pods</li> <li>Resource Quality of Service in Kubernetes</li> <li>Resource Quotas</li> <li>Limit Ranges</li> <li>Process ID Limits and Reservations</li> <li>For the Love of God, Stop Using CPU Limits on Kubernetes</li> <li>What Everyone Should Know About Kubernetes Memory Limits</li> </ul>"},{"location":"30-100/#chapter-3-declarative-deployment","title":"Chapter 3: Declarative Deployment","text":"<p>The heart of the Declarative Deployment pattern is the Kubernetes Deployment resource. This abstraction encapsulates the upgrade and rollback processes of a group of containers and makes its execution a repeatable and automated activity.</p>"},{"location":"30-100/#problem_1","title":"Problem","text":"<p>We can provision isolated environments as namespaces in a self-service manner and place the applications in these environments with minimal human intervention through the scheduler. But with a growing number of microservices, continually updating and replacing them with newer versions becomes an increasing burden too.</p> <p>Upgrading a service to a next version involves activities such as starting the new version of the Pod, stopping the old version of a Pod gracefully, waiting and verifying that it has launched successfully, and sometimes rolling it all back to the previous version in the case of failure. These activities are performed either by allowing some downtime but not running concurrent service versions, or with no downtime but increased resource usage due to both versions of the service running during the update process. Performing these steps manually can lead to human errors, and scripting properly can require a significant amount of effort, both of which quickly turn the release process into a bottleneck.</p>"},{"location":"30-100/#solution_1","title":"Solution","text":"<p>Luckily, Kubernetes has automated application upgrades as well. Using the concept of Deployment, we can describe how our application should be updated, using different strategies and tuning the various aspects of the update process. If you consider that you do multiple Deployments for every microservice instance per release cycle (which, depending on the team and project, can span from minutes to several months), this is another effort-saving automation by Kubernetes.</p> <p>In Chapter 2, \"Predictable Demands\", we saw that, to do its job effectively, the scheduler requires sufficient resources on the host system, appropriate placement policies, and containers with adequately defined resource profiles. Similarly, for a Deployment to do its job correctly, it expects the containers to be good cloud native citizens. At the very core of a Deployment is the ability to start and stop a set of Pods predictably. For this to work as expected, the containers themselves usually listen and honor lifecycle events (such as SIGTERM; see Chapter 5, \"Managed Lifecycle\") and also provide health-check endpoints as described in Chapter 4, \"Health Probe\", which indicate whether they started successfully.</p> <p>If a container covers these two areas accurately, the platform can cleanly shut down old containers and replace them by starting updated instances. Then</p>"},{"location":"ansible-rke2/","title":"Installing Ansible on RKE2 Cluster","text":""},{"location":"ansible-rke2/#quick-steps","title":"Quick Steps","text":"<ol> <li>Install Ansible operator prerequisites:    <pre><code>kubectl create namespace ansible-system\n</code></pre></li> <li>Install Ansible Operator:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/operator-framework/operator-lifecycle-manager/master/deploy/upstream/quickstart-catalog.yaml\nkubectl create -f https://operatorhub.io/install/ansible-operator.yaml\n</code></pre></li> <li>Verify installation:    <pre><code>kubectl get pods -n ansible-system\n</code></pre></li> </ol>"},{"location":"ansible-rke2/#architecture-overview","title":"Architecture Overview","text":""},{"location":"ansible-rke2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Running RKE2 cluster named \"default\"</li> <li><code>kubectl</code> access to the cluster</li> <li>Cluster admin privileges</li> </ul>"},{"location":"ansible-rke2/#detailed-installation-steps","title":"Detailed Installation Steps","text":""},{"location":"ansible-rke2/#1-prepare-the-cluster","title":"1. Prepare the Cluster","text":"<p>Create a dedicated namespace for Ansible components: <pre><code>kubectl create namespace ansible-system\n</code></pre></p>"},{"location":"ansible-rke2/#2-install-operator-lifecycle-manager-olm","title":"2. Install Operator Lifecycle Manager (OLM)","text":"<p>OLM is required to manage the Ansible Operator: <pre><code>kubectl apply -f https://raw.githubusercontent.com/operator-framework/operator-lifecycle-manager/master/deploy/upstream/quickstart-catalog.yaml\n</code></pre></p> <p>Wait for OLM to be ready: <pre><code>kubectl wait --for=condition=ready pod -l name=olm-operator -n olm --timeout=90s\n</code></pre></p>"},{"location":"ansible-rke2/#3-install-ansible-operator","title":"3. Install Ansible Operator","text":"<p>Deploy the Ansible Operator using OperatorHub manifest: <pre><code>kubectl create -f https://operatorhub.io/install/ansible-operator.yaml\n</code></pre></p>"},{"location":"ansible-rke2/#4-verify-installation","title":"4. Verify Installation","text":"<p>Check if the Ansible Operator pods are running: <pre><code>kubectl get pods -n ansible-system\n</code></pre></p> <p>Verify the operator deployment: <pre><code>kubectl get deployment -n ansible-system ansible-operator\n</code></pre></p>"},{"location":"ansible-rke2/#5-configure-rbac","title":"5. Configure RBAC","text":"<p>Create necessary RBAC permissions: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: ansible-operator\n  namespace: ansible-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ansible-operator\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: ansible-operator\nsubjects:\n- kind: ServiceAccount\n  name: ansible-operator\n  namespace: ansible-system\nroleRef:\n  kind: ClusterRole\n  name: ansible-operator\n  apiGroup: rbac.authorization.k8s.io\nEOF\n</code></pre></p>"},{"location":"ansible-rke2/#usage-example","title":"Usage Example","text":"<p>Create a simple Ansible playbook as a ConfigMap: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: sample-playbook\n  namespace: ansible-system\ndata:\n  playbook.yml: |\n    - hosts: localhost\n      tasks:\n        - name: Create a namespace\n          k8s:\n            name: example-namespace\n            kind: Namespace\n            state: present\nEOF\n</code></pre></p>"},{"location":"ansible-rke2/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ansible-rke2/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Operator Pod Not Starting <pre><code>kubectl describe pod -n ansible-system -l name=ansible-operator\n</code></pre></p> </li> <li> <p>Permission Issues <pre><code>kubectl describe clusterrolebinding ansible-operator\n</code></pre></p> </li> <li> <p>OLM Installation Problems <pre><code>kubectl get events -n olm\n</code></pre></p> </li> </ol>"},{"location":"ansible-rke2/#maintenance","title":"Maintenance","text":""},{"location":"ansible-rke2/#updating-ansible-operator","title":"Updating Ansible Operator","text":"<p>To update the operator to a newer version: <pre><code>kubectl delete -f https://operatorhub.io/install/ansible-operator.yaml\nkubectl create -f https://operatorhub.io/install/ansible-operator.yaml\n</code></pre></p>"},{"location":"ansible-rke2/#cleanup","title":"Cleanup","text":"<p>To remove Ansible from your cluster: ```bash kubectl delete namespace ansible-system kubectl delete clusterrole ansible-operator kubectl delete clusterrolebinding ansible-operator</p>"},{"location":"cmd-helm/","title":"Helm Commands Guide","text":"<p>This guide demonstrates common Helm commands using <code>grafana/grafana</code> as an example.</p>"},{"location":"cmd-helm/#adding-a-helm-repository","title":"Adding a Helm Repository","text":"<p>To add a Helm repository:</p> <pre><code># Add the Grafana Helm repository\nhelm repo add grafana https://grafana.github.io/helm-charts\n\n# Update the repository to fetch the latest charts\nhelm repo update\n</code></pre>"},{"location":"cmd-helm/#searching-for-chart-versions","title":"Searching for Chart Versions","text":"<p>To search for available versions of a chart:</p> <pre><code># List all available versions of Grafana\nhelm search repo grafana/grafana --versions\n\n# Example output:\n# NAME            VERSION    APP VERSION    DESCRIPTION\n# grafana/grafana 6.58.8    10.1.5         The leading tool for querying and visualizing t...\n# grafana/grafana 6.58.7    10.1.5         The leading tool for querying and visualizing t...\n# grafana/grafana 6.58.6    10.1.4         The leading tool for querying and visualizing t...\n</code></pre>"},{"location":"cmd-helm/#pulling-charts","title":"Pulling Charts","text":"<p>To download and extract a chart in one command:</p> <pre><code># Pull and automatically extract a specific version of Grafana chart\nhelm pull grafana/grafana --version 6.58.8 --untar\n\n# This will create a 'grafana' directory containing the chart files\n# You can examine or modify these files before installation\n</code></pre>"},{"location":"cmd-helm/#installing-charts","title":"Installing Charts","text":"<p>There are several ways to install a Helm chart:</p>"},{"location":"cmd-helm/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with default values\nhelm install my-grafana grafana/grafana\n</code></pre>"},{"location":"cmd-helm/#installation-with-custom-values","title":"Installation with Custom Values","text":"<pre><code># Install with custom values file\nhelm install my-grafana grafana/grafana -f custom-values.yaml\n\n# Install with specific version\nhelm install my-grafana grafana/grafana --version 6.58.8\n\n# Install with set values\nhelm install my-grafana grafana/grafana \\\n  --set service.type=NodePort \\\n  --set persistence.enabled=true \\\n  --set persistence.size=10Gi\n</code></pre>"},{"location":"cmd-helm/#useful-installation-options","title":"Useful Installation Options","text":"<ul> <li><code>--namespace</code>: Install in a specific namespace</li> <li><code>--create-namespace</code>: Create the namespace if it doesn't exist</li> <li><code>--dry-run</code>: Simulate the installation</li> <li><code>--debug</code>: Enable verbose output</li> </ul> <p>Example with multiple options:</p> <pre><code>helm install my-grafana grafana/grafana \\\n  --namespace monitoring \\\n  --create-namespace \\\n  --set persistence.enabled=true \\\n  --set adminPassword=admin123 \\\n  --version 6.58.8\n</code></pre>"},{"location":"cmd-helm/#additional-useful-commands","title":"Additional Useful Commands","text":"<pre><code># List installed releases\nhelm list --all-namespaces\n\n# Get release status\nhelm status my-grafana\n\n# Uninstall a release\nhelm uninstall my-grafana\n\n# Get release values\nhelm get values my-grafana\n\n# Upgrade a release\nhelm upgrade my-grafana grafana/grafana --reuse-values --version 6.58.9\n</code></pre>"},{"location":"kube-airflow-new/","title":"Apache Airflow on RKE2","text":"<p>This guide provides instructions for deploying Apache Airflow on RKE2 Kubernetes cluster.</p>"},{"location":"kube-airflow-new/#architecture-overview","title":"Architecture Overview","text":""},{"location":"kube-airflow-new/#celery-executor-architecture-method-1","title":"Celery Executor Architecture (Method 1)","text":""},{"location":"kube-airflow-new/#installation","title":"Installation","text":"<p>There are two methods to install Airflow on RKE2: using Helm with Celery executor (recommended for production) or using Kubernetes manifests with Kubernetes executor (simpler setup).</p>"},{"location":"kube-airflow-new/#method-1-helm-installation-with-celery-executor","title":"Method 1: Helm Installation with Celery Executor","text":"<p>This method is recommended for production environments as it provides better scalability and worker management.</p> <p>First, create the necessary PersistentVolumes. These must be created before installing Helm chart as they need to exist for the PVCs to bind successfully:</p> <pre><code># Create airflow-pv.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-airflow-postgresql-0\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-postgres\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: airflow-dags\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-dags\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: airflow-logs\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-logs\n    type: DirectoryOrCreate\n</code></pre> <p>Apply the PV configuration:</p> <pre><code># Create the PVs\nkubectl apply -f airflow-pv.yaml\n\n# Create namespace for Airflow\nkubectl create namespace airflow\n\n# Add the official Apache Airflow Helm repository\nhelm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n\n# Install Airflow using Helm with storage configuration\nhelm install airflow apache-airflow/airflow \\\n    --namespace airflow \\\n    --set executor=CeleryExecutor \\\n    --set postgresql.enabled=true \\\n    --set redis.enabled=true \\\n    --set webserver.defaultUser.enabled=true \\\n    --set webserver.defaultUser.username=admin \\\n    --set webserver.defaultUser.password=admin \\\n    --set webserver.defaultUser.email=admin@example.com \\\n    --set dags.persistence.enabled=true \\\n    --set dags.persistence.storageClassName=local-path \\\n    --set dags.persistence.size=1Gi \\\n    --set logs.persistence.enabled=true \\\n    --set logs.persistence.storageClassName=local-path \\\n    --set logs.persistence.size=1Gi \\\n    --set postgresql.persistence.enabled=true \\\n    --set postgresql.persistence.storageClassName=local-path \\\n    --set postgresql.persistence.size=10Gi\n</code></pre> <p>Note: The PV names must match exactly what the Helm chart expects: - <code>data-airflow-postgresql-0</code> for PostgreSQL data - <code>airflow-dags</code> for DAGs storage - <code>airflow-logs</code> for logs storage</p>"},{"location":"kube-airflow-new/#method-2-manifest-installation-with-kubernetes-executor","title":"Method 2: Manifest Installation with Kubernetes Executor","text":"<p>This method uses native Kubernetes resources and the Kubernetes executor, which is simpler to set up and maintain for smaller deployments.</p>"},{"location":"kube-airflow-new/#architecture","title":"Architecture","text":"<pre><code># Create namespace\nkubectl create namespace airflow\n</code></pre> <p>First, create the storage resources:</p> <pre><code># PostgreSQL PV and PVC\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: airflow-postgres-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-postgres\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: airflow-postgres-pvc\n  namespace: airflow\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: local-path\n  resources:\n    requests:\n      storage: 10Gi\n---\n# DAGs PV and PVC\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: airflow-dags-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-dags\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: airflow-dags-pvc\n  namespace: airflow\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: local-path\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <p>Create the ConfigMap for Airflow configuration:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: airflow-config\n  namespace: airflow\ndata:\n  AIRFLOW__CORE__EXECUTOR: KubernetesExecutor\n  AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow\n  AIRFLOW__CORE__LOAD_EXAMPLES: \"false\"\n  AIRFLOW__KUBERNETES__NAMESPACE: airflow\n  AIRFLOW__KUBERNETES__WORKER_CONTAINER_REPOSITORY: apache/airflow\n  AIRFLOW__KUBERNETES__WORKER_CONTAINER_TAG: 2.7.1\n  AIRFLOW__KUBERNETES__DELETE_WORKER_PODS: \"true\"\n</code></pre> <p>Create the PostgreSQL deployment:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airflow-postgres\n  namespace: airflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: airflow-postgres\n  template:\n    metadata:\n      labels:\n        app: airflow-postgres\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:13\n          env:\n            - name: POSTGRES_USER\n              value: airflow\n            - name: POSTGRES_PASSWORD\n              value: airflow\n            - name: POSTGRES_DB\n              value: airflow\n          ports:\n            - containerPort: 5432\n          volumeMounts:\n            - name: postgres-data\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: postgres-data\n          persistentVolumeClaim:\n            claimName: airflow-postgres-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: airflow-postgres\n  namespace: airflow\nspec:\n  selector:\n    app: airflow-postgres\n  ports:\n    - port: 5432\n</code></pre> <p>Create the Airflow webserver and scheduler deployments:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airflow-webserver\n  namespace: airflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: airflow-webserver\n  template:\n    metadata:\n      labels:\n        app: airflow-webserver\n    spec:\n      containers:\n        - name: webserver\n          image: apache/airflow:2.7.1\n          command: [\"airflow\", \"webserver\"]\n          ports:\n            - containerPort: 8080\n          envFrom:\n            - configMapRef:\n                name: airflow-config\n          volumeMounts:\n            - name: dags\n              mountPath: /opt/airflow/dags\n      volumes:\n        - name: dags\n          persistentVolumeClaim:\n            claimName: airflow-dags-pvc\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airflow-scheduler\n  namespace: airflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: airflow-scheduler\n  template:\n    metadata:\n      labels:\n        app: airflow-scheduler\n    spec:\n      containers:\n        - name: scheduler\n          image: apache/airflow:2.7.1\n          command: [\"airflow\", \"scheduler\"]\n          envFrom:\n            - configMapRef:\n                name: airflow-config\n          volumeMounts:\n            - name: dags\n              mountPath: /opt/airflow/dags\n      volumes:\n        - name: dags\n          persistentVolumeClaim:\n            claimName: airflow-dags-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: airflow-webserver\n  namespace: airflow\nspec:\n  selector:\n    app: airflow-webserver\n  ports:\n    - port: 8080\n</code></pre> <p>Initialize the database and create admin user:</p> <pre><code># Create initialization job\nkubectl create job --namespace airflow airflow-init-db --from=deployment/airflow-scheduler -- airflow db init\n\n# Create admin user\nkubectl create job --namespace airflow airflow-create-user --from=deployment/airflow-scheduler -- airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com\n</code></pre> <p>All the above manifests are combined in a single file for easier deployment. You can find it at <code>airflow-manifest.yaml</code>. To deploy:</p> <pre><code># Create the namespace\nkubectl create namespace airflow\n\n# Apply the combined manifest\nkubectl apply -f airflow-manifest.yaml\n</code></pre> <p>The manifest includes all necessary resources: - Persistent Volumes and Claims for storage - ConfigMap for Airflow configuration - PostgreSQL deployment and service - Airflow webserver and scheduler deployments - Ingress configuration</p>"},{"location":"kube-airflow-new/#configuration","title":"Configuration","text":""},{"location":"kube-airflow-new/#environment-variables-and-configmaps","title":"Environment Variables and ConfigMaps","text":"Parameter Description Default Value AIRFLOW__CORE__EXECUTOR The executor class to use CeleryExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN Database connection string postgresql+psycopg2://airflow:airflow@postgresql:5432/airflow AIRFLOW__CELERY__BROKER_URL Redis connection for Celery redis://:@redis:6379/0 AIRFLOW__CELERY__RESULT_BACKEND Celery result backend db+postgresql://airflow:airflow@postgresql:5432/airflow AIRFLOW__WEBSERVER__SECRET_KEY Secret key for the web interface YOUR_SECRET_KEY_HERE AIRFLOW__WEBSERVER__BASE_URL The base URL of the web interface http://localhost:8080 AIRFLOW__CORE__LOAD_EXAMPLES Whether to load example DAGs false AIRFLOW__CORE__DAGS_FOLDER Location of DAG files /opt/airflow/dags"},{"location":"kube-airflow-new/#accessing-the-web-interface","title":"Accessing the Web Interface","text":""},{"location":"kube-airflow-new/#setting-up-ingress","title":"Setting up Ingress","text":"<p>Create an ingress resource to expose the Airflow webserver:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: airflow-ingress\n  namespace: airflow\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: airflow.example.com  # Replace with your domain\n      http:\n        paths:\n          - path: /airflow(/|$)(.*)\n            pathType: Prefix\n            backend:\n              service:\n                name: airflow-webserver\n                port:\n                  number: 8080\n</code></pre> <p>The ingress configuration is included in the combined manifest. After applying the manifest and once the ingress controller is properly configured, you can access the Airflow web interface at <code>http://airflow.example.com/airflow</code>. Login with the default credentials: - Username: admin - Password: admin</p>"},{"location":"kube-airflow-new/#verification","title":"Verification","text":"<p>To verify the installation:</p> <pre><code># Check all pods are running\nkubectl get pods -n airflow\n\n# Check services\nkubectl get services -n airflow\n\n# Check persistent volumes\nkubectl get pvc -n airflow\n</code></pre>"},{"location":"kube-airflow-new/#uninstallation","title":"Uninstallation","text":""},{"location":"kube-airflow-new/#method-1-uninstalling-helm-based-installation","title":"Method 1: Uninstalling Helm-based Installation","text":"<p>To remove the Helm-based Airflow installation:</p> <pre><code># Delete the Helm release\nhelm uninstall airflow -n airflow\n\n# Delete the namespace and all its resources\nkubectl delete namespace airflow\n\n# Delete the PersistentVolumes\nkubectl delete pv data-airflow-postgresql-0\nkubectl delete pv airflow-dags\nkubectl delete pv airflow-logs\n\n# Optional: Remove the Helm repository\nhelm repo remove apache-airflow\n\n# Optional: Clean up the local storage directories\nsudo rm -rf /opt/local-path-provisioner/airflow-postgres\nsudo rm -rf /opt/local-path-provisioner/airflow-dags\nsudo rm -rf /opt/local-path-provisioner/airflow-logs\n</code></pre>"},{"location":"kube-airflow-new/#method-2-uninstalling-manifest-based-installation","title":"Method 2: Uninstalling Manifest-based Installation","text":"<p>To remove the manifest-based Airflow installation:</p> <p>```bash</p>"},{"location":"kube-airflow-new/#delete-all-airflow-resources-using-the-combined-manifest","title":"Delete all Airflow resources using the combined manifest","text":"<p>kubectl delete -f airflow-manifest.yaml</p>"},{"location":"kube-airflow-new/#alternative-delete-the-namespace-this-will-delete-all-resources-in-the-namespace","title":"Alternative: Delete the namespace (this will delete all resources in the namespace)","text":"<p>kubectl delete namespace airflow</p>"},{"location":"kube-airflow-new/#optional-clean-up-the-local-storage-directories","title":"Optional: Clean up the local storage directories","text":"<p>sudo rm -rf /opt/local-path-provisioner/airflow-postgres sudo rm -rf /opt/local-path-provisioner/airflow-dags</p>"},{"location":"kube-airflow/","title":"Apache Airflow on RKE2","text":"<p>This guide provides instructions for deploying Apache Airflow on RKE2 Kubernetes cluster.</p>"},{"location":"kube-airflow/#architecture-overview","title":"Architecture Overview","text":""},{"location":"kube-airflow/#celery-executor-architecture-method-1","title":"Celery Executor Architecture (Method 1)","text":""},{"location":"kube-airflow/#installation","title":"Installation","text":"<p>There are two methods to install Airflow on RKE2: using Helm with Celery executor (recommended for production) or using Kubernetes manifests with Kubernetes executor (simpler setup).</p>"},{"location":"kube-airflow/#method-1-helm-installation-with-celery-executor","title":"Method 1: Helm Installation with Celery Executor","text":"<p>This method is recommended for production environments as it provides better scalability and worker management.</p> <p>First, create the necessary PersistentVolumes. These must be created before installing Helm chart as they need to exist for the PVCs to bind successfully:</p> <pre><code># Create airflow-pv.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: data-airflow-postgresql-0\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-postgres\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: airflow-dags\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-dags\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: airflow-logs\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-logs\n    type: DirectoryOrCreate\n</code></pre> <p>Apply the PV configuration:</p> <pre><code># Create the PVs\nkubectl apply -f airflow-pv.yaml\n\n# Create namespace for Airflow\nkubectl create namespace airflow\n\n# Add the official Apache Airflow Helm repository\nhelm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n\n# Install Airflow using Helm with storage configuration\nhelm install airflow apache-airflow/airflow \\\n    --namespace airflow \\\n    --set executor=CeleryExecutor \\\n    --set postgresql.enabled=true \\\n    --set redis.enabled=true \\\n    --set webserver.defaultUser.enabled=true \\\n    --set webserver.defaultUser.username=admin \\\n    --set webserver.defaultUser.password=admin \\\n    --set webserver.defaultUser.email=admin@example.com \\\n    --set dags.persistence.enabled=true \\\n    --set dags.persistence.storageClassName=local-path \\\n    --set dags.persistence.size=1Gi \\\n    --set logs.persistence.enabled=true \\\n    --set logs.persistence.storageClassName=local-path \\\n    --set logs.persistence.size=1Gi \\\n    --set postgresql.persistence.enabled=true \\\n    --set postgresql.persistence.storageClassName=local-path \\\n    --set postgresql.persistence.size=10Gi\n</code></pre> <p>Note: The PV names must match exactly what the Helm chart expects: - <code>data-airflow-postgresql-0</code> for PostgreSQL data - <code>airflow-dags</code> for DAGs storage - <code>airflow-logs</code> for logs storage</p>"},{"location":"kube-airflow/#method-2-manifest-installation-with-kubernetes-executor","title":"Method 2: Manifest Installation with Kubernetes Executor","text":"<p>This method uses native Kubernetes resources and the Kubernetes executor, which is simpler to set up and maintain for smaller deployments.</p>"},{"location":"kube-airflow/#architecture","title":"Architecture","text":"<pre><code># Create namespace\nkubectl create namespace airflow\n</code></pre> <p>First, create the storage resources:</p> <pre><code># PostgreSQL PV and PVC\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: airflow-postgres-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-postgres\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: airflow-postgres-pvc\n  namespace: airflow\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: local-path\n  resources:\n    requests:\n      storage: 10Gi\n---\n# DAGs PV and PVC\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: airflow-dags-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: local-path\n  hostPath:\n    path: /opt/local-path-provisioner/airflow-dags\n    type: DirectoryOrCreate\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: airflow-dags-pvc\n  namespace: airflow\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: local-path\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <p>Create the ConfigMap for Airflow configuration:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: airflow-config\n  namespace: airflow\ndata:\n  AIRFLOW__CORE__EXECUTOR: KubernetesExecutor\n  AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow\n  AIRFLOW__CORE__LOAD_EXAMPLES: \"false\"\n  AIRFLOW__KUBERNETES__NAMESPACE: airflow\n  AIRFLOW__KUBERNETES__WORKER_CONTAINER_REPOSITORY: apache/airflow\n  AIRFLOW__KUBERNETES__WORKER_CONTAINER_TAG: 2.7.1\n  AIRFLOW__KUBERNETES__DELETE_WORKER_PODS: \"true\"\n</code></pre> <p>Create the PostgreSQL deployment:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airflow-postgres\n  namespace: airflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: airflow-postgres\n  template:\n    metadata:\n      labels:\n        app: airflow-postgres\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:13\n          env:\n            - name: POSTGRES_USER\n              value: airflow\n            - name: POSTGRES_PASSWORD\n              value: airflow\n            - name: POSTGRES_DB\n              value: airflow\n          ports:\n            - containerPort: 5432\n          volumeMounts:\n            - name: postgres-data\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: postgres-data\n          persistentVolumeClaim:\n            claimName: airflow-postgres-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: airflow-postgres\n  namespace: airflow\nspec:\n  selector:\n    app: airflow-postgres\n  ports:\n    - port: 5432\n</code></pre> <p>Create the Airflow webserver and scheduler deployments:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airflow-webserver\n  namespace: airflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: airflow-webserver\n  template:\n    metadata:\n      labels:\n        app: airflow-webserver\n    spec:\n      containers:\n        - name: webserver\n          image: apache/airflow:2.7.1\n          command: [\"airflow\", \"webserver\"]\n          ports:\n            - containerPort: 8080\n          envFrom:\n            - configMapRef:\n                name: airflow-config\n          volumeMounts:\n            - name: dags\n              mountPath: /opt/airflow/dags\n      volumes:\n        - name: dags\n          persistentVolumeClaim:\n            claimName: airflow-dags-pvc\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: airflow-scheduler\n  namespace: airflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: airflow-scheduler\n  template:\n    metadata:\n      labels:\n        app: airflow-scheduler\n    spec:\n      containers:\n        - name: scheduler\n          image: apache/airflow:2.7.1\n          command: [\"airflow\", \"scheduler\"]\n          envFrom:\n            - configMapRef:\n                name: airflow-config\n          volumeMounts:\n            - name: dags\n              mountPath: /opt/airflow/dags\n      volumes:\n        - name: dags\n          persistentVolumeClaim:\n            claimName: airflow-dags-pvc\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: airflow-webserver\n  namespace: airflow\nspec:\n  selector:\n    app: airflow-webserver\n  ports:\n    - port: 8080\n</code></pre> <p>Initialize the database and create admin user:</p> <pre><code># Create initialization job\nkubectl create job --namespace airflow airflow-init-db --from=deployment/airflow-scheduler -- airflow db init\n\n# Create admin user\nkubectl create job --namespace airflow airflow-create-user --from=deployment/airflow-scheduler -- airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com\n</code></pre> <p>All the above manifests are combined in a single file for easier deployment. You can find it at <code>airflow-manifest.yaml</code>. To deploy:</p> <pre><code># Create the namespace\nkubectl create namespace airflow\n\n# Apply the combined manifest\nkubectl apply -f airflow-manifest.yaml\n</code></pre> <p>The manifest includes all necessary resources: - Persistent Volumes and Claims for storage - ConfigMap for Airflow configuration - PostgreSQL deployment and service - Airflow webserver and scheduler deployments - Ingress configuration</p>"},{"location":"kube-airflow/#configuration","title":"Configuration","text":""},{"location":"kube-airflow/#environment-variables-and-configmaps","title":"Environment Variables and ConfigMaps","text":"Parameter Description Default Value AIRFLOW__CORE__EXECUTOR The executor class to use CeleryExecutor AIRFLOW__CORE__SQL_ALCHEMY_CONN Database connection string postgresql+psycopg2://airflow:airflow@postgresql:5432/airflow AIRFLOW__CELERY__BROKER_URL Redis connection for Celery redis://:@redis:6379/0 AIRFLOW__CELERY__RESULT_BACKEND Celery result backend db+postgresql://airflow:airflow@postgresql:5432/airflow AIRFLOW__WEBSERVER__SECRET_KEY Secret key for the web interface YOUR_SECRET_KEY_HERE AIRFLOW__WEBSERVER__BASE_URL The base URL of the web interface http://localhost:8080 AIRFLOW__CORE__LOAD_EXAMPLES Whether to load example DAGs false AIRFLOW__CORE__DAGS_FOLDER Location of DAG files /opt/airflow/dags"},{"location":"kube-airflow/#accessing-the-web-interface","title":"Accessing the Web Interface","text":""},{"location":"kube-airflow/#setting-up-ingress","title":"Setting up Ingress","text":"<p>Create an ingress resource to expose the Airflow webserver:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: airflow-ingress\n  namespace: airflow\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: airflow.example.com  # Replace with your domain\n      http:\n        paths:\n          - path: /airflow(/|$)(.*)\n            pathType: Prefix\n            backend:\n              service:\n                name: airflow-webserver\n                port:\n                  number: 8080\n</code></pre> <p>The ingress configuration is included in the combined manifest. After applying the manifest and once the ingress controller is properly configured, you can access the Airflow web interface at <code>http://airflow.example.com/airflow</code>. Login with the default credentials: - Username: admin - Password: admin</p>"},{"location":"kube-airflow/#verification","title":"Verification","text":"<p>To verify the installation:</p> <pre><code># Check all pods are running\nkubectl get pods -n airflow\n\n# Check services\nkubectl get services -n airflow\n\n# Check persistent volumes\nkubectl get pvc -n airflow\n</code></pre>"},{"location":"kube-airflow/#uninstallation","title":"Uninstallation","text":""},{"location":"kube-airflow/#method-1-uninstalling-helm-based-installation","title":"Method 1: Uninstalling Helm-based Installation","text":"<p>To remove the Helm-based Airflow installation:</p> <pre><code># Delete the Helm release\nhelm uninstall airflow -n airflow\n\n# Delete the namespace and all its resources\nkubectl delete namespace airflow\n\n# Delete the PersistentVolumes\nkubectl delete pv data-airflow-postgresql-0\nkubectl delete pv airflow-dags\nkubectl delete pv airflow-logs\n\n# Optional: Remove the Helm repository\nhelm repo remove apache-airflow\n\n# Optional: Clean up the local storage directories\nsudo rm -rf /opt/local-path-provisioner/airflow-postgres\nsudo rm -rf /opt/local-path-provisioner/airflow-dags\nsudo rm -rf /opt/local-path-provisioner/airflow-logs\n</code></pre>"},{"location":"kube-airflow/#method-2-uninstalling-manifest-based-installation","title":"Method 2: Uninstalling Manifest-based Installation","text":"<p>To remove the manifest-based Airflow installation:</p> <p>```bash</p>"},{"location":"kube-airflow/#delete-all-airflow-resources-using-the-combined-manifest","title":"Delete all Airflow resources using the combined manifest","text":"<p>kubectl delete -f airflow-manifest.yaml</p>"},{"location":"kube-airflow/#alternative-delete-the-namespace-this-will-delete-all-resources-in-the-namespace","title":"Alternative: Delete the namespace (this will delete all resources in the namespace)","text":"<p>kubectl delete namespace airflow</p>"},{"location":"kube-airflow/#optional-clean-up-the-local-storage-directories","title":"Optional: Clean up the local storage directories","text":"<p>sudo rm -rf /opt/local-path-provisioner/airflow-postgres sudo rm -rf /opt/local-path-provisioner/airflow-dags</p>"},{"location":"kube-app/","title":"Configuring an Application in Kubernetes","text":""},{"location":"kube-app/#1-define-the-application-components","title":"1. Define the Application Components","text":"<p>Determine what components your application requires: - Containers (e.g., web server, database). - Configuration (e.g., environment variables, command-line arguments). - Networking (e.g., ports, service exposure). - Storage (e.g., persistent data).</p>"},{"location":"kube-app/#2-create-the-required-kubernetes-resources","title":"2. Create the Required Kubernetes Resources","text":"<p>You need to define the following resources based on your application:</p>"},{"location":"kube-app/#a-pods","title":"a. Pods","text":"<p>Pods are the smallest deployable unit in Kubernetes. Typically, you don't define pods directly but through higher-level resources like Deployments.</p>"},{"location":"kube-app/#b-deployments","title":"b. Deployments","text":"<p>Manages the desired state of pods, ensuring availability and handling updates.</p> <p>Example YAML: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-container\n        image: my-image:latest\n        ports:\n        - containerPort: 8080\n</code></pre></p>"},{"location":"kube-app/#c-services","title":"c. Services","text":"<p>Expose your application within the cluster or to the external world.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  type: LoadBalancer\n</code></pre></p>"},{"location":"kube-app/#d-configmaps","title":"d. ConfigMaps","text":"<p>Store configuration data (non-sensitive) in a key-value format.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-config\ndata:\n  app.properties: |\n    key1=value1\n    key2=value2\n</code></pre></p> <p>Mount or reference this in your pod: <pre><code>    envFrom:\n    - configMapRef:\n        name: my-config\n</code></pre></p>"},{"location":"kube-app/#e-secrets","title":"e. Secrets","text":"<p>Store sensitive data like passwords or API keys securely.</p> <p>Example YAML: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\ntype: Opaque\ndata:\n  username: bXlVc2Vy\n  password: bXlQYXNz\n</code></pre></p> <p>Reference this in your pod: <pre><code>    env:\n    - name: DB_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: username\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n</code></pre></p>"},{"location":"kube-app/#f-persistent-volumes-and-claims","title":"f. Persistent Volumes and Claims","text":"<p>If your application needs to store data persistently.</p> <p>Define a PersistentVolumeClaim (PVC): <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre></p> <p>Mount it in your pod: <pre><code>    volumeMounts:\n    - mountPath: \"/data\"\n      name: data-volume\n  volumes:\n  - name: data-volume\n    persistentVolumeClaim:\n      claimName: my-pvc\n</code></pre></p>"},{"location":"kube-app/#3-apply-the-configuration","title":"3. Apply the Configuration","text":"<p>Use <code>kubectl</code> to apply your configuration files to the cluster: <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\nkubectl apply -f configmap.yaml\nkubectl apply -f secret.yaml\n</code></pre></p>"},{"location":"kube-app/#4-monitor-and-adjust","title":"4. Monitor and Adjust","text":"<ul> <li>Use <code>kubectl get pods</code> or <code>kubectl describe pod [pod-name]</code> to check the pod's status.</li> <li>Adjust configurations or scaling as needed:   <pre><code>kubectl scale deployment my-app --replicas=5\n</code></pre></li> </ul> <p>You now have a fully configured Kubernetes application!</p>"},{"location":"kube-batch-job/","title":"Batch Job","text":"<p>The Batch Job pattern is suited for managing isolated atomic units of work. It is based on the Job resource, which runs short-lived Pods reliably until completion on a distributed environment.</p>"},{"location":"kube-batch-job/#problem","title":"Problem","text":"<p>The main primitive in Kubernetes for managing and running containers is the Pod. There are different ways of creating Pods with varying characteristics:</p> <p>Bare Pod It is possible to create a Pod manually to run containers. However, when the node such a Pod is running on fails, the Pod is not restarted. Running Pods this way is discouraged except for development or testing purposes. This mechanism is also known as unmanaged or naked Pods.</p> <p>ReplicaSet This controller is used for creating and managing the lifecycle of Pods expected to run continuously (e.g., to run a web server container). It maintains a stable set of replica Pods running at any given time and guarantees the availability of a specified number of identical Pods. ReplicaSets are described in detail in Chapter 11, \"Stateless Service\".</p> <p>DaemonSet This controller runs a single Pod on every node and is used for managing platform capabilities such as monitoring, log aggregation, storage containers, and others. See Chapter 9, \"Daemon Service\", for a more detailed discussion.</p> <p>A common aspect of these Pods is that they represent long-running processes that are not meant to stop after a certain time. However, in some cases there is a need to perform a predefined finite unit of work reliably and then shut down the container. For this task, Kubernetes provides the Job resource.</p>"},{"location":"kube-batch-job/#solution","title":"Solution","text":"<p>A Kubernetes Job is similar to a ReplicaSet as it creates one or more Pods and ensures they run successfully. However, the difference is that, once the expected number of Pods terminate successfully, the Job is considered complete, and no additional Pods are started.</p>"},{"location":"kube-batch-job/#example-job-specification","title":"Example Job Specification","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: random-generator\nspec:\n  completions: 5                 # Job should run five Pods to completion, which all must succeed\n  parallelism: 2                 # Two Pods can run in parallel\n  ttlSecondsAfterFinished: 300   # Keep Pods for five minutes (300 seconds) before garbage-collecting them\n  template:\n    metadata:\n      name: random-generator\n    spec:\n      restartPolicy: OnFailure   # Specifying the restartPolicy is mandatory for a Job\n      containers:\n      - image: k8spatterns/random-generator:1.0\n        name: random-generator\n        command: [ \"java\", \"RandomRunner\", \"/numbers.txt\", \"10000\" ]\n</code></pre> <p>One crucial difference between the Job and the ReplicaSet definition is the <code>.spec.template.spec.restartPolicy</code>. The default value for a ReplicaSet is Always, which makes sense for long-running processes that must always be kept running. The value Always is not allowed for a Job, and the only possible options are OnFailure or Never.</p>"},{"location":"kube-batch-job/#benefits-of-using-jobs-over-bare-pods","title":"Benefits of Using Jobs Over Bare Pods","text":"<ol> <li> <p>A Job is not an ephemeral in-memory task but a persisted one that survives cluster restarts.</p> </li> <li> <p>When a Job is completed, it is not deleted but is kept for tracking purposes. The Pods that are created as part of the Job are also not deleted but are available for examination (e.g., to check the container logs).</p> </li> <li> <p>A Job may need to be performed multiple times. Using the <code>.spec.completions</code> field, it is possible to specify how many times a Pod should complete successfully before the Job itself is done.</p> </li> <li> <p>When a Job has to be completed multiple times, it can also be scaled and executed by starting multiple Pods at the same time using the <code>.spec.parallelism</code> field.</p> </li> <li> <p>A Job can be suspended by setting the field <code>.spec.suspend</code> to true. In this case, all active Pods are deleted and restarted if the Job is resumed.</p> </li> <li> <p>If the node fails or when the Pod is evicted for some reason while still running, the scheduler places the Pod on a new healthy node and reruns it.</p> </li> </ol>"},{"location":"kube-batch-job/#types-of-jobs","title":"Types of Jobs","text":"<p>Based on the completions and parallelism parameters, there are the following types of Jobs:</p> <p>Single Pod Jobs - Leave out both <code>.spec.completions</code> and <code>.spec.parallelism</code> or set them to their default values of 1 - Starts only one Pod and is completed when the single Pod terminates successfully</p> <p>Fixed Completion Count Jobs - Set <code>.spec.completions</code> to the number of completions needed - Can set <code>.spec.parallelism</code> or leave it unset (defaults to 1) - Completed after the <code>.spec.completions</code> number of Pods has completed successfully</p> <p>Work Queue Jobs - Leave <code>.spec.completions</code> unset - Set <code>.spec.parallelism</code> to a number greater than one - Completed when at least one Pod has terminated successfully and all other Pods have terminated - Requires Pods to coordinate among themselves</p> <p>Indexed Jobs - Similar to Work queue Jobs but without needing an external work queue - Uses fixed completion count and <code>.spec.completionMode</code> set to Indexed - Each Pod gets an index from 0 to <code>.spec.completions - 1</code> - Index available through annotation or JOB_COMPLETION_INDEX environment variable</p>"},{"location":"kube-batch-job/#example-indexed-job","title":"Example Indexed Job","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: file-split\nspec:\n  completionMode: Indexed     # Enable indexed completion mode\n  completions: 5              # Run five Pods in parallel to completion\n  parallelism: 5\n  template:\n    metadata:\n      name: file-split\n    spec:\n      containers:\n      - image: alpine\n        name: split\n        command:              # Execute shell script to process file ranges\n        - \"sh\"\n        - \"-c\"\n        - |\n          start=$(expr $JOB_COMPLETION_INDEX \\* 10000)      \n          end=$(expr $JOB_COMPLETION_INDEX \\* 10000 + 10000)\n          awk \"NR&gt;=$start &amp;&amp; NR&lt;$end\" /logs/random.log \\    \n              &gt; /logs/random-$JOB_COMPLETION_INDEX.txt\n        volumeMounts:         # Mount input data from external volume\n        - mountPath: /logs    \n          name: log-volume\n      restartPolicy: OnFailure\n</code></pre>"},{"location":"kube-batch-job/#discussion","title":"Discussion","text":"<p>The Job abstraction is a basic but fundamental primitive that other primitives such as CronJobs are based on. Jobs help turn isolated work units into a reliable and scalable unit of execution. However, a Job doesn't dictate how you should map individually processable work items into Jobs or Pods. That is something you have to determine after considering the pros and cons of each option:</p> <p>One Job per work item - Has overhead of creating Kubernetes Jobs - Platform has to manage large number of Jobs consuming resources - Useful when each work item is a complex task that needs independent tracking</p> <p>One Job for all work items - Right for large number of work items that don't need independent tracking - Work items managed from within application via batch framework</p> <p>The Job primitive provides only the minimum basics for scheduling work items. Any complex implementation has to combine the Job primitive with a batch application framework (e.g., Spring Batch, JBeret) to achieve the desired outcome.</p> <p>Not all services must run all the time. Using Jobs can run Pods only when needed and only for the duration of the task execution. Jobs are scheduled on nodes that have the required capacity, satisfy Pod placement policies, and take into account other container dependency considerations. Using Jobs for short-lived tasks rather than long-running abstractions saves resources for other workloads on the platform.</p>"},{"location":"kube-batch-job/#more-information","title":"More Information","text":"<ul> <li>Batch Job Example</li> <li>Jobs Documentation</li> <li>Parallel Processing Using Expansions</li> <li>Coarse Parallel Processing Using a Work Queue</li> <li>Fine Parallel Processing Using a Work Queue</li> <li>Indexed Job for Parallel Processing</li> <li>Spring Batch on Kubernetes</li> <li>JBeret Introduction</li> </ul>"},{"location":"kube-behavioral-patterns-intro/","title":"Behavioral Patterns","text":"<p>The patterns in this category are focused on the communications and interactions between the Pods and the managing platform. Depending on the type of managing controller used, a Pod may run until completion or be scheduled to run periodically. It can run as a daemon or ensure uniqueness guarantees to its replicas. There are different ways to run a Pod on Kubernetes, and picking the right Pod-management primitives requires understanding their behavior.</p> <p>In the following chapters, we explore the patterns:</p> <ul> <li>Chapter 7, \"Batch Job\", describes how to isolate an atomic unit of work and run it until completion.</li> <li>Chapter 8, \"Periodic Job\", allows the execution of a unit of work to be triggered by a temporal event.</li> <li>Chapter 9, \"Daemon Service\", allows you to run infrastructure-focused Pods on specific nodes, before application Pods are placed.</li> <li>Chapter 10, \"Singleton Service\", ensures that only one instance of a service is active at a time and still remains highly available.</li> <li>Chapter 11, \"Stateless Service\", describes the building blocks used for managing identical application instances.</li> <li>Chapter 12, \"Stateful Service\", is all about how to create and manage distributed stateful applications with Kubernetes.</li> <li>Chapter 13, \"Service Discovery\", explains how client services can discover and consume the instances of providing services.</li> <li>Chapter 14, \"Self Awareness\", describes mechanisms for introspection and metadata injection into applications.</li> </ul>"},{"location":"kube-chap-4/","title":"Chapter 4: Health Probe","text":"<p>The Health Probe pattern enables applications to communicate their health state to Kubernetes, allowing automated management of Pods and traffic routing.</p>"},{"location":"kube-chap-4/#problem","title":"Problem","text":"<p>Process status checks alone are insufficient for determining application health, as applications may hang while their processes remain running.</p>"},{"location":"kube-chap-4/#solution","title":"Solution","text":""},{"location":"kube-chap-4/#process-health-checks","title":"Process Health Checks","text":"<ul> <li>Kubelet performs basic container process monitoring</li> <li>Containers are restarted if processes stop running</li> </ul>"},{"location":"kube-chap-4/#liveness-probes","title":"Liveness Probes","text":"<p>Kubelet performs external health checks using: - HTTP probe: HTTP GET request expecting 200-399 response - TCP Socket probe: Requires successful TCP connection - Exec probe: Command execution expecting exit code 0 - gRPC probe: Uses gRPC health checks</p> <p>Configuration parameters: - initialDelaySeconds: Delay before first check - periodSeconds: Interval between checks - timeoutSeconds: Maximum probe duration - failureThreshold: Failed checks before unhealthy status</p> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-liveness-check\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    env:\n    - name: DELAY_STARTUP\n      value: \"20\"\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n    livenessProbe:\n      httpGet:                  \n        path: /actuator/health\n        port: 8080\n      initialDelaySeconds: 30   \n</code></pre></p>"},{"location":"kube-chap-4/#readiness-probes","title":"Readiness Probes","text":"<ul> <li>Uses same methods as liveness probes (HTTP, TCP, Exec, gRPC)</li> <li>Failed checks remove container from service endpoint</li> <li>No container restart on failure</li> </ul> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-readiness-check\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    readinessProbe:\n      exec:\n        command: [ \"stat\", \"/var/run/random-generator-ready\" ]\n</code></pre></p>"},{"location":"kube-chap-4/#custom-pod-readiness-gates","title":"Custom Pod Readiness Gates","text":"<ul> <li>Enables additional Pod-level readiness conditions</li> <li>Useful for external dependencies like load balancers</li> </ul> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nspec:\n  readinessGates:\n  - conditionType: \"k8spatterns.io/load-balancer-ready\"\nstatus:\n  conditions:\n  - type: \"k8spatterns.io/load-balancer-ready\"\n    status: \"False\"\n  - type: Ready\n    status: \"False\"\n</code></pre></p>"},{"location":"kube-chap-4/#startup-probes","title":"Startup Probes","text":"<ul> <li>Designed for applications with long startup times</li> <li>Uses same format as liveness probes with different timing parameters</li> </ul> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-startup-check\nspec:\n  containers:\n  - image: quay.io/wildfly/wildfly\n    name: wildfly\n    startupProbe:\n      exec:\n        command: [ \"stat\", \"/opt/jboss/wildfly/standalone/tmp/startup-marker\" ]\n      initialDelaySeconds: 60\n      periodSeconds: 60\n      failureThreshold: 15\n    livenessProbe:\n      httpGet:\n        path: /health\n        port: 9990\n        periodSeconds: 10\n        failureThreshold: 3\n</code></pre></p>"},{"location":"kube-chap-4/#best-practices","title":"Best Practices","text":"<ul> <li>Log significant events to stdout/stderr</li> <li>Log termination reasons to /dev/termination-log</li> <li>Integrate with tracing and metrics libraries (OpenTracing, Prometheus)</li> </ul>"},{"location":"kube-chap-5/","title":"Chapter 5: Managed Lifecycle","text":"<p>The Managed Lifecycle pattern defines how containerized applications should react to platform lifecycle events.</p>"},{"location":"kube-chap-5/#problem","title":"Problem","text":"<p>Applications need to respond to platform-issued commands for proper lifecycle management beyond basic process monitoring.</p>"},{"location":"kube-chap-5/#solution","title":"Solution","text":""},{"location":"kube-chap-5/#sigterm-signal","title":"SIGTERM Signal","text":"<ul> <li>Sent when Kubernetes initiates container shutdown</li> <li>Allows for clean shutdown before SIGKILL</li> <li>Triggered by Pod shutdown or failed liveness probe</li> </ul>"},{"location":"kube-chap-5/#sigkill-signal","title":"SIGKILL Signal","text":"<ul> <li>Forcefully terminates container if not shut down after SIGTERM</li> <li>Sent 30 seconds after SIGTERM by default</li> </ul>"},{"location":"kube-chap-5/#poststart-hook","title":"PostStart Hook","text":"<p>Executes commands when container starts.</p> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: post-start-hook\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    lifecycle:\n      postStart:\n        exec:\n          command:\n          - sh\n          - -c\n          - sleep 30 &amp;&amp; echo \"Wake up!\" &gt; /tmp/postStart_done\n</code></pre></p>"},{"location":"kube-chap-5/#prestop-hook","title":"PreStop Hook","text":"<p>Blocking call before container termination, similar to SIGTERM.</p> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pre-stop-hook\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    lifecycle:\n      preStop:\n        httpGet:\n          path: /shutdown\n          port: 8080\n</code></pre></p>"},{"location":"kube-chap-5/#lifecycle-controls-comparison","title":"Lifecycle Controls Comparison","text":"Aspect Lifecycle hooks Init containers Activates on Container lifecycle phases Pod lifecycle phases Startup phase postStart command initContainers list Shutdown phase preStop command None Timing Concurrent with ENTRYPOINT Sequential before app containers Use cases Container-specific cleanup Sequential Pod initialization"},{"location":"kube-chap-5/#advanced-container-control","title":"Advanced Container Control","text":"<p>Example of entrypoint rewriting for advanced lifecycle management:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: wrapped-random-generator\nspec:\n  restartPolicy: OnFailure\n  volumes:\n  - name: wrapper\n    emptyDir: { }\n  initContainers:\n  - name: copy-supervisor\n    image: k8spatterns/supervisor\n    volumeMounts:\n    - mountPath: /var/run/wrapper\n      name: wrapper\n    command: [ cp ]\n    args: [ supervisor, /var/run/wrapper/supervisor ]\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    volumeMounts:\n    - mountPath: /var/run/wrapper\n      name: wrapper\n    command:\n    - \"/var/run/wrapper/supervisor\"\n    args:\n    - \"random-generator-runner\"\n    - \"--seed\"\n    - \"42\"\n</code></pre>"},{"location":"kube-chap-5/#best-practices","title":"Best Practices","text":"<ul> <li>Implement graceful startup and shutdown handling</li> <li>Honor platform lifecycle events for reliable operation</li> <li>Application lifecycle is fully automated by the platform</li> </ul>"},{"location":"kube-chap-6/","title":"Chapter 6: Automated Placement","text":"<p>Automated Placement is the core function of the Kubernetes scheduler for assigning new Pods to nodes that match container resource requests and honor scheduling policies. This pattern describes the principles of the Kubernetes scheduling algorithm and how to influence the placement decisions from the outside.</p>"},{"location":"kube-chap-6/#problem","title":"Problem","text":"<p>A reasonably sized microservices-based system consists of tens or even hundreds of isolated processes. Containers and Pods do provide nice abstractions for packaging and deployment but do not solve the problem of placing these processes on suitable nodes. With a large and ever-growing number of microservices, assigning and placing them individually to nodes is not a manageable activity.</p> <p>Containers have dependencies among themselves, dependencies to nodes, and resource demands, and all of that changes over time too. The resources available on a cluster also vary over time, through shrinking or extending the cluster or by having it consumed by already-placed containers. The way we place containers impacts the availability, performance, and capacity of the distributed systems as well. All of that makes scheduling containers to nodes a moving target.</p>"},{"location":"kube-chap-6/#solution","title":"Solution","text":"<p>In Kubernetes, assigning Pods to nodes is done by the scheduler. It is a part of Kubernetes that is highly configurable, and it is still evolving and improving. The Kubernetes scheduler is a potent and time-saving tool. It plays a fundamental role in the Kubernetes platform as a whole, but similar to other Kubernetes components (API Server, Kubelet), it can be run in isolation or not used at all.</p>"},{"location":"kube-chap-6/#available-node-resources","title":"Available Node Resources","text":"<p>First of all, the Kubernetes cluster needs to have nodes with enough resource capacity to run new Pods. Every node has capacity available for running Pods, and the scheduler ensures that the sum of the container resources requested for a Pod is less than the available allocatable node capacity.</p> <p>Node capacity is calculated as: <pre><code>Allocatable [capacity for application pods] =\n    Node Capacity [available capacity on a node]\n        - Kube-Reserved [Kubernetes daemons like kubelet, container runtime]\n        - System-Reserved [Operating System daemons like sshd, udev]\n        - Eviction Thresholds [Reserved memory to prevent system OOMs]\n</code></pre></p> <p>If you don't reserve resources for system daemons that power the OS and Kubernetes itself, the Pods can be scheduled up to the full capacity of the node, which may cause Pods and system daemons to compete for resources, leading to resource starvation issues on the node. Even then, memory pressure on the node can affect all Pods running on it through OOMKilled errors or cause the node to go temporarily offline.</p> <p>Also keep in mind that if containers are running on a node that is not managed by Kubernetes, the resources used by these containers are not reflected in the node capacity calculations by Kubernetes. A workaround is to run a placeholder Pod that doesn't do anything but has only resource requests for CPU and memory corresponding to the untracked containers' resource use amount.</p>"},{"location":"kube-chap-6/#container-resource-demands","title":"Container Resource Demands","text":"<p>Another important requirement for an efficient Pod placement is to define the containers' runtime dependencies and resource demands. We covered that in more detail in Chapter 2, \"Predictable Demands\". It boils down to having containers that declare their resource profiles (with request and limit) and environment dependencies such as storage or ports.</p>"},{"location":"kube-chap-6/#scheduler-configurations","title":"Scheduler Configurations","text":"<p>The scheduler has a default set of predicate and priority policies configured that is good enough for most use cases. In newer versions of Kubernetes, scheduling profiles are used to achieve customization. This approach exposes the different steps of the scheduling process as extension points and allows you to configure plugins that override the default implementations of the steps.</p> <p>Example of a scheduler configuration:</p> <pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - plugins:\n      score:\n        disabled:\n        - name: PodTopologySpread\n        enabled:\n        - name: MyCustomPlugin\n          weight: 2\n</code></pre> <p>By default, the scheduler uses the default-scheduler profile with default plugins. It is also possible to run multiple schedulers on the cluster, or multiple profiles on the scheduler, and allow Pods to specify which profile to use.</p>"},{"location":"kube-chap-6/#node-selection","title":"Node Selection","text":"<p>In most cases, it is better to let the scheduler do the Pod-to-node assignment and not micromanage the placement logic. However, on some occasions, you may want to force the assignment of a Pod to a specific node or group of nodes.</p> <p>Example of using nodeSelector:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: random-generator\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n  nodeSelector:\n    disktype: ssd\n</code></pre> <p>In addition to specifying custom labels to your nodes, you can use some of the default labels that are present on every node. Every node has a unique kubernetes.io/hostname label that can be used to place a Pod on a node by its hostname. Other default labels that indicate the OS, architecture, and instance type can be useful for placement too.</p>"},{"location":"kube-daemon-service/","title":"Daemon Service","text":"<p>The Daemon Service pattern allows you to place and run prioritized, infrastructure-focused Pods on targeted nodes. It is used primarily by administrators to run node-specific Pods to enhance the Kubernetes platform capabilities.</p>"},{"location":"kube-daemon-service/#problem","title":"Problem","text":"<p>The concept of a daemon in software systems exists at many levels:</p> <ul> <li>At an operating system level, a daemon is a long-running, self-recovering computer program that runs as a background process</li> <li>In Unix, daemon names end in d (httpd, named, sshd)</li> <li>Other OS terms: services-started tasks, ghost jobs</li> <li>JVM has daemon threads that provide supporting services to user threads</li> </ul> <p>Similarly, Kubernetes also has the concept of a DaemonSet. Considering that Kubernetes is a distributed platform spread across multiple nodes and with the primary goal of managing application Pods, a DaemonSet is represented by Pods that run on the cluster nodes and provide some background capabilities for the rest of the cluster.</p>"},{"location":"kube-daemon-service/#solution","title":"Solution","text":"<p>ReplicaSet and its predecessor ReplicationController are control structures responsible for making sure a specific number of Pods are running. These controllers constantly monitor the list of running Pods and make sure the actual number of Pods always matches the desired number. In that regard, a DaemonSet is a similar construct and is responsible for ensuring that a certain number of Pods are always running.</p> <p>The difference is that the first two run a specific number of Pods, usually driven by the application requirements of high availability and user load, irrespective of the node count. On the other hand, a DaemonSet is not driven by consumer load in deciding how many Pod instances to run and where to run. Its main purpose is to keep running a single Pod on every node or specific nodes.</p>"},{"location":"kube-daemon-service/#example-daemonset-resource","title":"Example DaemonSet Resource","text":"<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: random-refresher\nspec:\n  selector:\n    matchLabels:\n      app: random-refresher\n  template:\n    metadata:\n      labels:\n        app: random-refresher\n    spec:\n      nodeSelector:            # Use only nodes with the label feature set to value hw-rng\n        feature: hw-rng\n      containers:\n      - image: k8spatterns/random-generator:1.0\n        name: random-generator\n        command: [ \"java\", \"RandomRunner\", \"/numbers.txt\", \"10000\", \"30\" ]\n        volumeMounts:          # DaemonSets often mount a portion of a node's filesystem\n        - mountPath: /host_dev\n          name: devices\n      volumes:\n      - name: devices\n        hostPath:              # hostPath for accessing the node directories directly\n          path: /dev\n</code></pre> <p>Given this behavior, the primary candidates for a DaemonSet are usually infrastructure-related processes, such as: - Cluster storage providers - Log collectors - Metric exporters - kube-proxy</p>"},{"location":"kube-daemon-service/#key-differences-from-replicaset","title":"Key Differences from ReplicaSet","text":"<ol> <li>Pod Placement</li> <li>By default, a DaemonSet places one Pod instance on every node</li> <li> <p>Can be controlled/limited to subset of nodes using nodeSelector or affinity fields</p> </li> <li> <p>Scheduling</p> </li> <li>A Pod created by DaemonSet already has nodeName specified</li> <li>Doesn't require Kubernetes scheduler to run containers</li> <li>Can run before scheduler has started</li> <li> <p>Unschedulable field of a node is not respected by DaemonSet controller</p> </li> <li> <p>Pod Configuration</p> </li> <li>RestartPolicy can only be Always or left unspecified (defaults to Always)</li> <li> <p>This ensures container restart when liveness probe fails</p> </li> <li> <p>Priority</p> </li> <li>Pods managed by DaemonSet are treated with higher priority</li> <li>Descheduler will avoid evicting such Pods</li> <li>Cluster autoscaler manages them separately</li> </ol>"},{"location":"kube-daemon-service/#accessing-daemonset-pods","title":"Accessing DaemonSet Pods","text":"<p>There are several ways to reach Pods managed by DaemonSets:</p> <ol> <li>Service</li> <li>Create a Service with same Pod selector as DaemonSet</li> <li> <p>Use Service to reach daemon Pod load-balanced to random node</p> </li> <li> <p>DNS</p> </li> <li>Create headless Service with same Pod selector</li> <li> <p>Retrieve multiple A records from DNS containing all Pod IPs and ports</p> </li> <li> <p>Node IP with hostPort</p> </li> <li>Pods can specify hostPort to be reachable via node IP addresses</li> <li> <p>Combination of node IP, hostPort and protocol must be unique</p> </li> <li> <p>External Push</p> </li> <li>Application in DaemonSet Pods can push data to external location/service</li> <li>No consumer needs to reach DaemonSet Pods directly</li> </ol>"},{"location":"kube-daemon-service/#static-pods","title":"Static Pods","text":"<p>Another way to run containers similar to DaemonSet is through static Pods:</p> <ul> <li>Kubelet can get resource definitions from local directory</li> <li>Managed by Kubelet only and run on one node only</li> <li>API service doesn't observe these Pods</li> <li>No controller or health checks performed</li> <li>Kubelet watches and restarts them when they crash</li> <li>Kubelet scans configured directory for Pod definition changes</li> </ul> <p>While static Pods can be used to run containerized Kubernetes system processes, DaemonSets are better integrated with the platform and are recommended over static Pods.</p>"},{"location":"kube-daemon-service/#discussion","title":"Discussion","text":"<p>There are other ways to run daemon processes on every node, but they all have limitations:</p> <ul> <li>Static Pods: Managed by Kubelet but can't be managed through Kubernetes APIs</li> <li>Bare Pods: Can't survive accidental deletion/termination or node failure</li> <li>Init Scripts: Require different toolchains for monitoring and management</li> </ul> <p>DaemonSet is somewhere between developer and administrator toolbox, inclining more toward the administrator side. However, it's relevant to application developers as it demonstrates how Kubernetes turns single-node concepts into multinode clustered primitives for managing distributed systems.</p>"},{"location":"kube-daemon-service/#more-information","title":"More Information","text":"<ul> <li>Daemon Service Example</li> <li>DaemonSet Documentation</li> <li>Perform a Rolling Update on a DaemonSet</li> <li>DaemonSets and Jobs</li> <li>Create Static Pods</li> </ul>"},{"location":"kube-deployment-ch3/","title":"Kubernetes Deployment Patterns","text":"<p>foot <sup>1</sup> </p>"},{"location":"kube-deployment-ch3/#declarative-deployment","title":"Declarative Deployment","text":""},{"location":"kube-deployment-ch3/#problem","title":"Problem","text":"<p>As the number of microservices grows, the manual process of upgrading services\u2014including starting the new Pod version, gracefully stopping the old one, verifying successful launches, and rolling back if necessary\u2014becomes burdensome. Manual steps can lead to human errors, and scripting them requires significant effort. Kubernetes addresses this challenge with the Deployment resource, which automates application upgrades and makes them repeatable and efficient [chapter 3]</p>"},{"location":"kube-deployment-ch3/#solution","title":"Solution","text":"<p>The Kubernetes Deployment resource abstracts the complexities of managing upgrades and rollbacks of container groups. By defining the desired state, Kubernetes performs necessary actions to ensure the system conforms to that state. Deployment strategies supported include:</p> <p>Note</p> <p>The default strategy that updates Pods incrementally to ensure zero downtime. It creates new Pods while progressively replacing old ones, adhering to parameters like <code>maxSurge</code> (temporary additional Pods) and <code>maxUnavailable</code> (maximum Pods unavailable during updates).</p> <p>Example</p> Command Description <code>kubectl rollout status</code> Shows the current status of a Deployment\u2019s rollout. <code>kubectl rollout pause</code> Pauses a rolling update so that multiple changes can be applied to a Deployment without retriggering another rollout. <code>kubectl rollout resume</code> Resumes a previously paused rollout. <code>kubectl rollout undo</code> Performs a rollback to a previous revision of a Deployment. A rollback is helpful in case of an error during the update. <p>Example configuration:</p> <p>Lorem ipsum dolor sit amet, (1) consectetur adipiscing elit.</p> <ol> <li>I'm an annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be expressed in Markdown.</li> </ol> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rolling-deployment\nspec:\n  replicas: 3 # (1)\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: rolling-app\n  template:\n    metadata:\n      labels:\n        app: rolling-app\n    spec:\n      containers:\n      - name: rolling-app\n        image: app-image:v1\n        readinessProbe:\n          exec:\n            command: [\"stat\", \"/tmp/app-ready\"]\n</code></pre> { .annotate } 1.  Declaration of three replicas. You need more than one replica for a rolling update to make sense. 2.  Number of Pods that can be run temporarily in addition to the replicas specified during an update. In this example, it could be a maximum of four replicas. 3.  Number of Pods that may be unavailable during the update. Here it could be that only two Pods are available at a time during the update. 4.  Duration in seconds of all readiness probes for a rolled-out Pod needs to be healthy until the rollout continues. 5.  Readiness probes that are very important for a rolling deployment to ensure zero downtime.</p> <p>Figure 3-1. Rolling deployment strategy.</p> <p></p>"},{"location":"kube-deployment-ch3/#recreate-deployment","title":"Recreate Deployment","text":"<p>This strategy stops all existing Pods before starting new ones, ensuring no overlap but introducing downtime.</p> <p>Figure 3-2. Fixed deployment using a Recreate strategy.</p> <p></p>"},{"location":"kube-deployment-ch3/#blue-green-deployment","title":"Blue-Green Deployment","text":"<p>This strategy minimizes downtime by deploying a new version in parallel with the existing one. Once verified, traffic switches to the new version by updating the Service selector. Resources for the old version can then be released.</p> <p>Figure 3-3. Blue-Green release.</p> <p></p>"},{"location":"kube-deployment-ch3/#canary-release","title":"Canary Release","text":"<p>This approach gradually introduces a new version to a subset of users. If successful, it replaces the old version entirely. It involves managing two Deployments and adjusting traffic split using service mesh or ingress controllers</p> <p>Figure 3-4. Canary release.</p> <p></p>"},{"location":"kube-deployment-ch3/#advanced-deployment-strategies","title":"Advanced Deployment Strategies","text":""},{"location":"kube-deployment-ch3/#pre-and-post-deployment-hooks","title":"Pre- and Post-Deployment Hooks","text":"<p>These hooks were proposed to allow custom commands during deployments but are not fully implemented in Kubernetes. Current alternatives include higher-level abstractions using operators.</p>"},{"location":"kube-deployment-ch3/#higher-level-platforms","title":"Higher-Level Platforms","text":"<p>Platforms like Flagger (part of Flux CD), Argo Rollouts, and Knative provide advanced deployment strategies, integrating canary and blue-green mechanisms while supporting traffic splitting and rollbacks.</p>"},{"location":"kube-deployment-ch3/#benefits-of-declarative-deployment","title":"Benefits of Declarative Deployment","text":"<ul> <li>Automation: Reduces manual intervention and human error.</li> <li>Repeatability: Ensures consistent results across environments.</li> <li>Observability: Provides monitoring capabilities and rollback mechanisms.</li> <li>Declarative Nature: Focuses on desired state rather than procedural steps.</li> </ul>"},{"location":"kube-deployment-ch3/#conclusion","title":"Conclusion","text":"<p>Kubernetes\u2019 Deployment abstraction simplifies service upgrades, balancing downtime, resource utilization, and operational risks. Advanced tools further enhance its capabilities, making it suitable for modern, scalable application environments.</p> <p>Figure 3-5. Deployment and release strategies.</p> <p></p> <ol> <li> <p>Kubernetes Patterns Reusable Elements for Designing Cloud Native Applications\u00a0\u21a9</p> </li> </ol>"},{"location":"kube-deployment-opensource/","title":"Open Source Kubernetes Deployment Guide","text":""},{"location":"kube-deployment-opensource/#summary","title":"Summary","text":"<p>This guide describes a production-grade Kubernetes deployment using RKE2 as the distribution, featuring a comprehensive stack of open-source components. The deployment includes a 3-node cluster with Traefik for API and web gateway services, Keycloak for OAuth2 authentication, OpenEBS for storage management, PostgreSQL as a stateful database, and Apache Airflow for orchestrating analysis workloads.</p>"},{"location":"kube-deployment-opensource/#quick-start","title":"Quick Start","text":"<pre><code># Install RKE2 on the first server node\ncurl -sfL https://get.rke2.io | sh -\nsystemctl enable rke2-server.service\nsystemctl start rke2-server.service\n\n# Get the node token from the first server\ncat /var/lib/rancher/rke2/server/node-token\n\n# Install and join additional server nodes\ncurl -sfL https://get.rke2.io | sh -\nmkdir -p /etc/rancher/rke2/\ncat &lt;&lt; EOF &gt; /etc/rancher/rke2/config.yaml\nserver: https://&lt;first-server-ip&gt;:9345\ntoken: &lt;node-token&gt;\nEOF\nsystemctl enable rke2-server.service\nsystemctl start rke2-server.service\n\n# Install kubectl and configure access\nmkdir ~/.kube\ncp /etc/rancher/rke2/rke2.yaml ~/.kube/config\nchmod 600 ~/.kube/config\n\n# Deploy OpenEBS\nkubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml\n\n# Install Helm\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n\n# Add and update required Helm repositories\nhelm repo add traefik https://helm.traefik.io/traefik\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n\n# Deploy Traefik\nhelm install traefik traefik/traefik -n traefik --create-namespace\n\n# Deploy PostgreSQL with OpenEBS storage\nkubectl create namespace database\nhelm install postgresql bitnami/postgresql \\\n  --namespace database \\\n  --set persistence.storageClass=openebs-hostpath \\\n  --set persistence.size=10Gi\n\n# Deploy Keycloak\nkubectl create namespace auth\nhelm install keycloak bitnami/keycloak \\\n  --namespace auth \\\n  --set postgresql.enabled=false \\\n  --set externalDatabase.host=postgresql.database.svc.cluster.local \\\n  --set externalDatabase.port=5432 \\\n  --set externalDatabase.user=postgres \\\n  --set externalDatabase.database=keycloak\n\n# Deploy Airflow\nhelm install airflow bitnami/airflow \\\n  --namespace airflow --create-namespace \\\n  --set postgresql.enabled=false \\\n  --set externalDatabase.host=postgresql.database.svc.cluster.local \\\n  --set externalDatabase.port=5432 \\\n  --set externalDatabase.user=postgres \\\n  --set externalDatabase.database=airflow\n</code></pre>"},{"location":"kube-deployment-opensource/#architecture","title":"Architecture","text":""},{"location":"kube-deployment-opensource/#component-details","title":"Component Details","text":""},{"location":"kube-deployment-opensource/#rke2-cluster","title":"RKE2 Cluster","text":"<ul> <li>3-node highly available control plane</li> <li>Built-in containerd runtime</li> <li>Automated certificate management</li> <li>Integrated security features</li> </ul>"},{"location":"kube-deployment-opensource/#traefik-gateway","title":"Traefik Gateway","text":"<ul> <li>API and Web Gateway services</li> <li>Automatic SSL/TLS termination</li> <li>Rate limiting capabilities</li> <li>Monitoring endpoints</li> <li>Load balancing</li> </ul>"},{"location":"kube-deployment-opensource/#keycloak-oauth2","title":"Keycloak OAuth2","text":"<ul> <li>Centralized authentication</li> <li>OAuth2/OpenID Connect support</li> <li>User federation</li> <li>Role-based access control</li> <li>Single sign-on capabilities</li> </ul>"},{"location":"kube-deployment-opensource/#openebs-storage","title":"OpenEBS Storage","text":"<ul> <li>Dynamic volume provisioning</li> <li>Local and distributed storage options</li> <li>Storage class management</li> <li>Snapshot and backup support</li> </ul>"},{"location":"kube-deployment-opensource/#postgresql-statefulset","title":"PostgreSQL StatefulSet","text":"<ul> <li>Persistent storage with OpenEBS</li> <li>Automated backups</li> <li>High availability configuration</li> <li>Data replication</li> </ul>"},{"location":"kube-deployment-opensource/#airflow-orchestration","title":"Airflow Orchestration","text":"<ul> <li>Dynamic pod spawning</li> <li>Workflow management</li> <li>Scheduled analysis jobs</li> <li>Resource optimization</li> <li>Monitoring and logging</li> </ul>"},{"location":"kube-deployment-opensource/#data-flow","title":"Data Flow","text":""},{"location":"kube-deployment-opensource/#security-considerations","title":"Security Considerations","text":"<ol> <li>Network Security</li> <li>All external traffic routed through Traefik</li> <li>Internal service communication encrypted</li> <li> <p>Network policies for pod isolation</p> </li> <li> <p>Authentication &amp; Authorization</p> </li> <li>Centralized OAuth2 with Keycloak</li> <li>Role-based access control (RBAC)</li> <li>Service account management</li> <li> <p>Token-based authentication</p> </li> <li> <p>Data Security</p> </li> <li>Encrypted storage with OpenEBS</li> <li>Regular database backups</li> <li>Secure credential management</li> <li>Pod security policies</li> </ol>"},{"location":"kube-deployment-opensource/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<ol> <li>Cluster Health</li> <li>Node status monitoring</li> <li>Control plane metrics</li> <li> <p>Resource utilization</p> </li> <li> <p>Application Monitoring</p> </li> <li>Traefik metrics</li> <li>Keycloak audit logs</li> <li>PostgreSQL performance metrics</li> <li> <p>Airflow task monitoring</p> </li> <li> <p>Storage Management</p> </li> <li>Volume health monitoring</li> <li>Capacity planning</li> <li>Backup verification</li> <li>Performance metrics</li> </ol>"},{"location":"kube-deployment-opensource/#scaling-considerations","title":"Scaling Considerations","text":"<ol> <li>Horizontal Scaling</li> <li>Add worker nodes as needed</li> <li>Scale Traefik replicas</li> <li> <p>Increase analysis pod capacity</p> </li> <li> <p>Vertical Scaling</p> </li> <li>Adjust resource limits</li> <li>Optimize pod requests</li> <li> <p>Tune database resources</p> </li> <li> <p>Storage Scaling</p> </li> <li>Expand volume capacity</li> <li>Add storage nodes</li> <li>Optimize storage classes</li> </ol>"},{"location":"kube-deployment-opensource/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Cluster Issues</li> <li>Check node status</li> <li>Verify control plane health</li> <li> <p>Review system logs</p> </li> <li> <p>Application Issues</p> </li> <li>Monitor pod status</li> <li>Check service endpoints</li> <li> <p>Review application logs</p> </li> <li> <p>Storage Issues</p> </li> <li>Verify volume status</li> <li>Check storage provisioner</li> <li>Monitor capacity alerts</li> </ol>"},{"location":"kube-deployment-opensource/#backup-and-disaster-recovery","title":"Backup and Disaster Recovery","text":"<ol> <li>Regular Backups</li> <li>Database dumps</li> <li>Volume snapshots</li> <li> <p>Configuration backups</p> </li> <li> <p>Recovery Procedures</p> </li> <li>Node replacement</li> <li>Data restoration</li> <li> <p>Service recovery</p> </li> <li> <p>High Availability</p> </li> <li>Multi-node redundancy</li> <li>Service replication</li> <li>Automated failover</li> </ol>"},{"location":"kube-harbor/","title":"Harbor Registry Deployment Guide","text":"<p>Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted.</p>"},{"location":"kube-harbor/#quick-steps","title":"Quick Steps","text":"<ol> <li>Prerequisites:</li> <li>Kubernetes cluster</li> <li>Helm 3.x</li> <li>Domain name for Harbor</li> <li> <p>SSL certificate (optional for production)</p> </li> <li> <p>Installation:    <pre><code># Add Harbor Helm repository\nhelm repo add harbor https://helm.goharbor.io\nhelm repo update\n\n# Create namespace\nkubectl create namespace harbor\n\n# Install Harbor\nhelm install harbor harbor/harbor \\\n  --namespace harbor \\\n  --set expose.type=ingress \\\n  --set expose.tls.enabled=true \\\n  --set externalURL=https://harbor.yourdomain.com\n</code></pre></p> </li> <li> <p>Verify Installation:    <pre><code>kubectl get pods -n harbor\n</code></pre></p> </li> </ol>"},{"location":"kube-harbor/#architecture-overview","title":"Architecture Overview","text":""},{"location":"kube-harbor/#detailed-deployment-steps","title":"Detailed Deployment Steps","text":""},{"location":"kube-harbor/#1-prerequisites-setup","title":"1. Prerequisites Setup","text":"<ol> <li> <p>Ensure Kubernetes cluster is running:    <pre><code>kubectl cluster-info\n</code></pre></p> </li> <li> <p>Install Helm if not already installed:    <pre><code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n</code></pre></p> </li> <li> <p>Prepare storage class (if using cloud provider, skip this step):    <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: harbor-storage\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\nEOF\n</code></pre></p> </li> </ol>"},{"location":"kube-harbor/#2-configuration","title":"2. Configuration","text":"<ol> <li> <p>Create values.yaml for Harbor configuration:    <pre><code>expose:\n  type: ingress\n  tls:\n    enabled: true\n  ingress:\n    hosts:\n      core: harbor.yourdomain.com\n    annotations:\n      kubernetes.io/ingress.class: nginx\n\npersistence:\n  enabled: true\n  storageClass: \"harbor-storage\"\n\nharborAdminPassword: \"your-secure-password\"\n\ndatabase:\n  type: internal\n  internal:\n    password: \"your-db-password\"\n</code></pre></p> </li> <li> <p>Install Harbor using Helm:    <pre><code>helm install harbor harbor/harbor \\\n  --namespace harbor \\\n  --create-namespace \\\n  -f values.yaml\n</code></pre></p> </li> </ol>"},{"location":"kube-harbor/#3-post-installation","title":"3. Post-Installation","text":"<ol> <li> <p>Verify all pods are running:    <pre><code>kubectl get pods -n harbor\n</code></pre></p> </li> <li> <p>Configure DNS or add to hosts file:    <pre><code>echo \"$(kubectl get svc harbor-ingress -n harbor -o jsonpath='{.status.loadBalancer.ingress[0].ip}') harbor.yourdomain.com\" | sudo tee -a /etc/hosts\n</code></pre></p> </li> <li> <p>Access Harbor Portal:</p> </li> <li>URL: https://harbor.yourdomain.com</li> <li>Default credentials:<ul> <li>Username: admin</li> <li>Password: Harbor12345 (or the one set in values.yaml)</li> </ul> </li> </ol>"},{"location":"kube-harbor/#4-configure-docker-client","title":"4. Configure Docker Client","text":"<ol> <li> <p>Add Harbor's certificate to Docker:    <pre><code>sudo mkdir -p /etc/docker/certs.d/harbor.yourdomain.com\nsudo cp ca.crt /etc/docker/certs.d/harbor.yourdomain.com/\n</code></pre></p> </li> <li> <p>Login to Harbor:    <pre><code>docker login harbor.yourdomain.com\n</code></pre></p> </li> <li> <p>Push an image:    <pre><code>docker tag nginx:latest harbor.yourdomain.com/library/nginx:latest\ndocker push harbor.yourdomain.com/library/nginx:latest\n</code></pre></p> </li> </ol>"},{"location":"kube-harbor/#security-considerations","title":"Security Considerations","text":"<ol> <li>Change default admin password immediately</li> <li>Enable RBAC and create projects with appropriate permissions</li> <li>Configure vulnerability scanning with Trivy</li> <li>Enable image signing for trusted content</li> <li>Regular backup of Harbor data</li> </ol>"},{"location":"kube-harbor/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li> <p>Pod startup issues:    <pre><code>kubectl describe pod &lt;pod-name&gt; -n harbor\nkubectl logs &lt;pod-name&gt; -n harbor\n</code></pre></p> </li> <li> <p>Storage issues:</p> </li> <li>Check PVC status:      <pre><code>kubectl get pvc -n harbor\n</code></pre></li> <li> <p>Verify StorageClass:      <pre><code>kubectl get sc\n</code></pre></p> </li> <li> <p>Ingress issues:</p> </li> <li>Verify ingress configuration:      ```bash      kubectl get ingress -n harbor      kubectl describe ingress -n harbor</li> </ol>"},{"location":"kube-ingress/","title":"Kubernetes Ingress","text":"<p>An Ingress is a Kubernetes resource that manages external access to services in a cluster. It provides HTTP/HTTPS routing, SSL termination, and name-based virtual hosting.</p>"},{"location":"kube-ingress/#complete-example","title":"Complete Example","text":"<p>Here's a complete example showing the full stack of Pod, ConfigMap, Service, and Ingress:</p> <pre><code># First file: pod-and-configmap.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\n  labels:\n    app: endpoint-test\nspec:\n  containers:\n    - name: web-server\n      image: nginx:alpine\n      ports:\n        - containerPort: 80\n      volumeMounts:\n        - name: html-volume\n          mountPath: /usr/share/nginx/html\n  volumes:\n    - name: html-volume\n      configMap:\n        name: test-html\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: test-html\ndata:\n  index.html: |\n    &lt;html&gt;\n    &lt;head&gt;&lt;title&gt;Test Page&lt;/title&gt;&lt;/head&gt;\n    &lt;body&gt;&lt;h1&gt;Hello, Kubernetes!&lt;/h1&gt;&lt;/body&gt;\n    &lt;/html&gt;\n---\n# Second file: service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-pod-service\nspec:\n  selector:\n    app: endpoint-test\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: ClusterIP\n---\n# Ingress with path\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: fuck\n    http:\n      paths:\n      - path: /test\n        pathType: Prefix\n        backend:\n          service:\n            name: test-pod-service\n            port:\n              number: 80\n</code></pre>"},{"location":"kube-ingress/#how-ingress-endpoints-and-pods-interlink","title":"How Ingress, Endpoints, and Pods Interlink","text":"<p>```plantuml @startuml !theme plain skinparam componentStyle rectangle</p> <p>rectangle \"External Traffic\" as client</p> <p>package \"Kubernetes Cluster\" {   component \"Ingress\\n(Load Balancer)\" as ingress   component \"Service\" as svc   collections \"Endpoints\" as ep</p> <p>rectangle \"Pod 1\" as pod1   rectangle \"Pod 2\" as pod2   rectangle \"Pod 3\" as pod3 }</p> <p>client \u2192 ingress : \"HTTP/HTTPS\" ingress \u2192 svc : \"Routes to Service\" svc \u2192 ep : \"Selects\" ep \u2192 pod1 ep \u2192 pod2 ep \u2192 pod3</p> <p>note right of ingress   Handles:   - SSL Termination   - URL Routing   - Load Balancing end note</p> <p>note right of svc   Service selects pods   using labels end note</p> <p>note right of ep   Endpoints track   pod IP addresses end note @enduml ```</p>"},{"location":"kube-ingress/#understanding-the-connection-flow","title":"Understanding the Connection Flow","text":"<p>Let's analyze how each component in our YAML example works together:</p> <ol> <li>Pod Configuration (First YAML section):    <pre><code>metadata:\n  name: test-pod\n  labels:\n    app: endpoint-test   # This label is key for Service selection\n</code></pre></li> <li>Defines a Pod running nginx with a custom HTML page</li> <li>Uses the label <code>app: endpoint-test</code> which is crucial for Service discovery</li> <li> <p>Mounts a volume from ConfigMap to serve custom content</p> </li> <li> <p>ConfigMap Integration (Second YAML section):    <pre><code>volumes:\n  - name: html-volume\n    configMap:\n      name: test-html   # Links to the ConfigMap\n</code></pre></p> </li> <li>Provides the HTML content that the Pod will serve</li> <li>Connected to the Pod through volume mounting</li> <li> <p>Demonstrates configuration separation from the Pod</p> </li> <li> <p>Service Layer (Third YAML section):    <pre><code>spec:\n  selector:\n    app: endpoint-test   # Matches Pod's label\n  ports:\n    - port: 80          # Service port\n      targetPort: 80    # Pod port\n</code></pre></p> </li> <li>Creates a stable network endpoint for the Pod</li> <li>Uses label selector <code>app: endpoint-test</code> to find our Pod</li> <li> <p>Maps port 80 on the service to port 80 on the Pod</p> </li> <li> <p>Ingress Configuration (Fourth YAML section):    <pre><code>spec:\n  rules:\n  - host: example.local\n    http:\n      paths:\n      - path: /test    # URL path\n        backend:\n          service:\n            name: test-pod-service   # References Service name\n</code></pre></p> </li> <li>Defines external access rules</li> <li>Routes <code>/test</code> path to our Service</li> <li>References the Service by name: <code>test-pod-service</code></li> </ol>"},{"location":"kube-ingress/#key-relationships-in-the-stack","title":"Key Relationships in the Stack","text":"<ol> <li>Pod \u2192 Service Connection:</li> <li>Pod's label <code>app: endpoint-test</code> matches Service's selector</li> <li>Service automatically discovers Pod through this label matching</li> <li> <p>If Pod is recreated with same labels, Service automatically finds it</p> </li> <li> <p>Service \u2192 Ingress Connection:</p> </li> <li>Ingress backend references Service by name: <code>test-pod-service</code></li> <li>Traffic flow: Ingress \u2192 Service \u2192 Pod</li> <li> <p>Service provides stable internal endpoint regardless of Pod changes</p> </li> <li> <p>ConfigMap \u2192 Pod Connection:</p> </li> <li>Pod mounts ConfigMap as volume</li> <li>Changes to ConfigMap can update Pod's content without rebuild</li> <li>Separates configuration from application container</li> </ol> <p>This interconnected system provides: - Dynamic service discovery through labels - Load balancing across multiple Pods (if scaled) - External access control through Ingress rules - Configuration management through ConfigMaps</p>"},{"location":"kube-kind/","title":"Local Kubernetes Development with Kind","text":"Resource Link OpenSuse Hauptseite opensuse.org SUSE Container Guide documentation.suse.com/container SUSE Container Registry registry.suse.com openSUSE Leap 15.6 build.opensuse.org <p>This guide walks through setting up a complete local Kubernetes development environment using Kind (Kubernetes in Docker), along with essential tools and monitoring.</p>"},{"location":"kube-kind/#prerequisites","title":"Prerequisites","text":"<p>This guide is specifically for openSUSE Linux distributions. Ensure you have: - openSUSE Leap 15.6 or newer - Root/sudo access - Terminal access - At least 8GB RAM and 4 CPU cores recommended</p>"},{"location":"kube-kind/#installing-podman","title":"Installing Podman","text":"<p>Podman is the officially supported container engine in SUSE/openSUSE:</p> <ol> <li> <p>Install Podman and required tools: <pre><code>sudo zypper install podman podman-docker cni-plugins\n</code></pre></p> </li> <li> <p>Enable and start Podman socket (for Docker API compatibility): <pre><code>sudo systemctl enable --now podman.socket\n</code></pre></p> </li> <li> <p>Configure Podman for rootless mode: <pre><code>sudo touch /etc/subuid /etc/subgid\nsudo usermod --add-subuids 100000-165535 --add-subgids 100000-165535 $USER\n</code></pre></p> </li> <li> <p>Set up Docker compatibility: <pre><code># Create Docker compatibility symlink\nsudo ln -sf /run/podman/podman.sock /var/run/docker.sock\n\n# Add current user to podman group\nsudo usermod -aG podman $USER\n</code></pre></p> </li> <li> <p>Log out and back in for group changes to take effect, then verify: <pre><code>podman --version\npodman ps\n</code></pre></p> </li> </ol> <p>Note: The <code>docker</code> command will also work due to the podman-docker compatibility package.</p>"},{"location":"kube-kind/#installing-kind","title":"Installing Kind","text":"<p>Kind (Kubernetes in Docker) allows running local Kubernetes clusters using Docker containers as nodes.</p> <pre><code># Download Kind binary\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Verify installation\nkind --version\n</code></pre>"},{"location":"kube-kind/#installing-kubectl","title":"Installing kubectl","text":"<p>Install kubectl using the official Kubernetes repository:</p> <pre><code># Add Kubernetes repository\nsudo zypper addrepo --refresh https://download.opensuse.org/repositories/containers:/kubetools/15.5/containers:kubetools.repo\n\n# Install kubectl\nsudo zypper install kubernetes-client\n\n# Verify installation\nkubectl version --client\n</code></pre>"},{"location":"kube-kind/#installing-helm","title":"Installing Helm","text":"<p>Install Helm using the openSUSE package manager:</p> <pre><code># Add Helm repository\nsudo zypper addrepo https://download.opensuse.org/repositories/home:tlusk:kubectl/15.6/home:tlusk:kubectl.repo\n\n# Install Helm\nsudo zypper refresh\nsudo zypper install helm\n\n# Verify installation\nhelm version\n</code></pre>"},{"location":"kube-kind/#creating-a-kind-cluster","title":"Creating a Kind Cluster","text":"<ol> <li> <p>Create a cluster configuration file <code>kind-config.yaml</code>: <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n- role: worker\n- role: worker\n</code></pre></p> </li> <li> <p>Create the cluster: <pre><code>kind create cluster --name monitoring --config kind-config.yaml\n</code></pre></p> </li> <li> <p>Verify cluster is running: <pre><code>kubectl cluster-info\n</code></pre></p> </li> </ol>"},{"location":"kube-kind/#installing-monitoring-stack","title":"Installing Monitoring Stack","text":"<p>We'll set up a complete monitoring solution with Prometheus, Grafana, and Loki using Helm charts.</p> <ol> <li> <p>Create monitoring namespace: <pre><code>kubectl create namespace monitoring\n</code></pre></p> </li> <li> <p>Add required Helm repositories: <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre></p> </li> <li> <p>Install Prometheus stack: <pre><code>helm install prometheus prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --set grafana.enabled=true \\\n  --set prometheus.service.type=ClusterIP \\\n  --values - &lt;&lt;EOF\ngrafana:\n  service:\n    type: ClusterIP\n  adminPassword: admin123\nprometheus:\n  prometheusSpec:\n    retention: 7d\nEOF\n</code></pre></p> </li> <li> <p>Install Loki stack: <pre><code>helm install loki grafana/loki-stack \\\n  --namespace monitoring \\\n  --set grafana.enabled=false \\\n  --set loki.persistence.enabled=true \\\n  --set loki.persistence.size=10Gi\n</code></pre></p> </li> <li> <p>Verify installations: <pre><code>kubectl get pods -n monitoring\n</code></pre></p> </li> </ol>"},{"location":"kube-kind/#setting-up-nginx-reverse-proxy","title":"Setting up Nginx Reverse Proxy","text":"<p>Create a file named <code>nginx-proxy.yaml</code>: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-config\n  namespace: monitoring\ndata:\n  nginx.conf: |\n    events {\n      worker_connections 1024;\n    }\n    http {\n      server {\n        listen 80;\n\n        location /grafana/ {\n          proxy_pass http://prometheus-grafana.monitoring.svc.cluster.local:80/;\n          proxy_set_header Host $host;\n        }\n\n        location /prometheus/ {\n          proxy_pass http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090/;\n          proxy_set_header Host $host;\n        }\n\n        location /loki/ {\n          proxy_pass http://loki.monitoring.svc.cluster.local:3100/;\n          proxy_set_header Host $host;\n        }\n      }\n    }\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-proxy\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app: nginx-proxy\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx-proxy\n    spec:\n      containers:\n      - name: nginx\n        image: registry.suse.com/suse/nginx:latest\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: nginx-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginx-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-proxy\n  namespace: monitoring\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 80\n    nodePort: 80\n  selector:\n    app: nginx-proxy\n</code></pre></p> <p>Apply the configuration: <pre><code>kubectl apply -f nginx-proxy.yaml\n</code></pre></p>"},{"location":"kube-kind/#accessing-the-dashboards","title":"Accessing the Dashboards","text":"<p>Get the Grafana admin password (if you didn't set it in the values): <pre><code>kubectl get secret -n monitoring prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode\n</code></pre></p> <p>Access the dashboards through the Nginx proxy: - Grafana: http://localhost/grafana   - Username: admin   - Password: admin123 (or from above command if not set in values) - Prometheus: http://localhost/prometheus - Loki: http://localhost/loki</p> <p>Configure Loki as a data source in Grafana: 1. Go to Configuration &gt; Data Sources in Grafana 2. Add a new Loki data source 3. Set the URL to: http://loki:3100 4. Click \"Save &amp; Test\"</p>"},{"location":"kube-kind/#cleaning-up","title":"Cleaning Up","text":"<p>To delete the Kind cluster: <pre><code>kind delete cluster --name monitoring\n</code></pre></p>"},{"location":"kube-kind/#next-steps","title":"Next Steps","text":"<p>With this setup, you now have:</p> <ul> <li>A local multi-node Kubernetes cluster</li> <li>The Kubernetes CLI (kubectl) for cluster management</li> <li>Helm for package management</li> <li>A complete monitoring stack with Prometheus and Grafana</li> </ul> <p>You can now:</p> <ol> <li>Deploy applications to your cluster</li> <li>Monitor cluster and application metrics</li> <li>Set up alerts based on metrics</li> <li>Explore Grafana dashboards for visualization</li> </ol>"},{"location":"kube-kind/#troubleshooting","title":"Troubleshooting","text":""},{"location":"kube-kind/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Podman service not running   Check Podman socket status: <code>sudo systemctl status podman.socket</code>   Start Podman if stopped: <code>sudo systemctl start podman.socket</code>   Verify Podman is working: <code>podman ps</code></p> </li> <li> <p>Permission issues   Check subuid/subgid mappings: <code>grep $USER /etc/subuid /etc/subgid</code>   Verify Podman socket permissions: <code>ls -l /run/podman/podman.sock</code>   Ensure your user is in the podman group: <code>groups $USER</code>   Run: <code>podman system migrate</code> to update container storage</p> </li> <li> <p>Port conflicts   Check if port 80 is in use: <code>sudo netstat -tulpn | grep :80</code>   Ensure no other web servers are running on port 80   You may need root privileges to bind to port 80</p> </li> <li> <p>Resource constraints   Check available resources: <code>free -h</code> and <code>nproc</code>   Close unnecessary applications   Consider adding swap space if needed   Verify cgroup settings: <code>cat /proc/cmdline | grep cgroup</code></p> </li> <li> <p>Repository issues    Refresh repositories: <code>sudo zypper refresh</code>    Check for repository errors: <code>sudo zypper repos -u</code>    Verify network connectivity to repository URLs</p> </li> </ol>"},{"location":"kube-loki/","title":"Installing Grafana Loki on RKE2","text":"<p>This guide provides step-by-step instructions for installing Grafana Loki on an RKE2 cluster with existing Prometheus and Rancher installations.</p>"},{"location":"kube-loki/#prerequisites","title":"Prerequisites","text":"<ul> <li>RKE2 cluster up and running</li> <li>Helm v3.x installed</li> <li>kubectl configured to access your cluster</li> <li>Prometheus installed</li> <li>Rancher installed</li> <li>Grafana installed (typically comes with Rancher monitoring)</li> </ul>"},{"location":"kube-loki/#installation-steps","title":"Installation Steps","text":""},{"location":"kube-loki/#1-add-grafana-helm-repository","title":"1. Add Grafana Helm Repository","text":"<pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre>"},{"location":"kube-loki/#2-create-custom-values-file","title":"2. Create Custom Values File","text":"<p>Create a file named <code>loki-values.yaml</code> with the following content:</p> <pre><code>loki:\n  auth_enabled: false\n  commonConfig:\n    replication_factor: 1\n  storage:\n    type: filesystem\n\nmonitoring:\n  selfMonitoring:\n    enabled: true\n    grafanaAgent:\n      installOperator: false\n\n  lokiCanary:\n    enabled: false\n\nscrape_configs:\n  - job_name: kubernetes-pods\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: pod\n      - source_labels: [__meta_kubernetes_container_name]\n        action: replace\n        target_label: container\n\n  - job_name: kubernetes-nodes\n    kubernetes_sd_configs:\n      - role: node\n    relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - source_labels: [__meta_kubernetes_node_name]\n        action: replace\n        target_label: node\n\n  - job_name: kubernetes-system-containers\n    kubernetes_sd_configs:\n      - role: pod\n        namespaces:\n          names: ['kube-system']\n    relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: pod\n</code></pre>"},{"location":"kube-loki/#3-install-loki-using-helm","title":"3. Install Loki Using Helm","text":"<pre><code># Create namespace for Loki\nkubectl create namespace loki-stack\n\n# Install Loki using Helm with custom values\nhelm install loki grafana/loki-stack \\\n  --namespace loki-stack \\\n  --values loki-values.yaml \\\n  --set grafana.enabled=false\n</code></pre>"},{"location":"kube-loki/#4-configure-grafana-data-source","title":"4. Configure Grafana Data Source","text":"<ol> <li>Access your Grafana instance through Rancher UI</li> <li>Go to Configuration &gt; Data Sources</li> <li>Click \"Add data source\"</li> <li>Select \"Loki\"</li> <li>Set the URL to: <code>http://loki.loki-stack.svc.cluster.local:3100</code></li> <li>Click \"Save &amp; Test\"</li> </ol>"},{"location":"kube-loki/#5-verify-installation","title":"5. Verify Installation","text":"<p>Check if all pods are running:</p> <pre><code>kubectl get pods -n loki-stack\n</code></pre> <p>Verify Loki is collecting logs:</p> <pre><code># Get one of the Loki pod names\nLOKI_POD=$(kubectl get pods -n loki-stack -l app=loki -o jsonpath=\"{.items[0].metadata.name}\")\n\n# Check Loki logs\nkubectl logs -n loki-stack $LOKI_POD\n</code></pre>"},{"location":"kube-loki/#6-using-loki-in-grafana","title":"6. Using Loki in Grafana","text":"<ol> <li>In Grafana, go to \"Explore\"</li> <li>Select \"Loki\" as the data source</li> <li>Try a simple LogQL query:    <pre><code>{namespace=\"kube-system\"}\n</code></pre></li> </ol>"},{"location":"kube-loki/#troubleshooting","title":"Troubleshooting","text":""},{"location":"kube-loki/#common-issues","title":"Common Issues","text":"<ol> <li> <p>If pods are not starting: <pre><code>kubectl describe pod -n loki-stack &lt;pod-name&gt;\n</code></pre></p> </li> <li> <p>If logs are not appearing:</p> </li> <li> <p>Verify Promtail configuration: <pre><code>kubectl get configmap -n loki-stack\nkubectl describe configmap -n loki-stack loki-promtail\n</code></pre></p> </li> <li> <p>If Grafana can't connect to Loki:</p> </li> <li>Verify the Loki service is running: <pre><code>kubectl get svc -n loki-stack\n</code></pre></li> <li>Check Loki is accessible within the cluster: <pre><code>kubectl run curl --image=curlimages/curl -i --tty --rm -- curl http://loki.loki-stack.svc.cluster.local:3100/ready\n</code></pre></li> </ol>"},{"location":"kube-loki/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Limits: Adjust resource limits based on your cluster size and log volume</li> <li>Retention: Configure log retention period based on your requirements</li> <li>Monitoring: Set up alerts for Loki component health</li> <li>Backup: Regularly backup Loki storage if using persistent storage</li> </ol>"},{"location":"kube-loki/#additional-configuration","title":"Additional Configuration","text":""},{"location":"kube-loki/#scaling-loki","title":"Scaling Loki","text":"<p>For production environments, consider:</p> <ol> <li>Using object storage (S3, GCS) instead of filesystem</li> <li>Implementing retention policies</li> <li>Setting up proper resource limits</li> <li>Configuring index and chunk storage separately</li> </ol> <p>Example production values:</p> <pre><code>loki:\n  auth_enabled: true\n  storage:\n    type: s3\n    s3:\n      endpoint: your-s3-endpoint\n      bucketnames: your-bucket\n      access_key_id: your-access-key\n      secret_access_key: your-secret-key\n  limits_config:\n    retention_period: 30d\n    ingestion_rate_mb: 10\n    ingestion_burst_size_mb: 20\n</code></pre> <p>Remember to adjust these values according to your specific requirements and infrastructure setup.</p>"},{"location":"kube-patterns-part3/","title":"Discussion (Chapter 3 - Declarative Deployment)","text":"<p>The Deployment primitive is an example of Kubernetes turning the tedious process of manually updating applications into a declarative activity that can be repeated and automated. The out-of-the-box deployment strategies (rolling and recreate) control the replacement of old containers by new ones, and the advanced release strategies (Blue-Green and canary) control how the new version becomes available to service consumers. The latter two release strategies are based on a human decision for the transition trigger and as a consequence are not fully automated by Kubernetes but require human interaction.</p> <p>All software is different, and deploying complex systems usually requires additional steps and checks. The techniques discussed in this chapter cover the Pod update process, but do not include updating and rolling back other Pod dependencies such as ConfigMaps, Secrets, or other dependent services.</p>"},{"location":"kube-patterns-part3/#pre-and-post-deployment-hooks","title":"Pre and Post Deployment Hooks","text":"<p>In the past, there has been a proposal for Kubernetes to allow hooks in the deployment process. Pre and Post hooks would allow the execution of custom commands before and after Kubernetes has executed a deployment strategy. Such commands could perform additional actions while the deployment is in progress and would additionally be able to abort, retry, or continue a deployment. Those hooks are a good step toward new automated deployment and release strategies. Unfortunately, this effort has been stalled for some years (as of 2023), so it is unclear whether this feature will ever come to Kubernetes.</p> <p>One approach that works today is to create a script to manage the update process of services and their dependencies using the Deployment and other primitives discussed in this book. However, this imperative approach that describes the individual update steps does not match the declarative nature of Kubernetes.</p> <p>As an alternative, higher-level declarative approaches have emerged on top of Kubernetes. The most important platforms are described in the sidebar that follows. Those techniques work with operators (see Chapter 28, \"Operator\") that take a declarative description of the rollout process and perform the necessary actions on the server side, some of them also including automatic rollbacks in case of an update error. For advanced, production-ready rollout scenarios, it is recommended to look at one of those extensions.</p>"},{"location":"kube-patterns-part3/#higher-level-deployments","title":"Higher-Level Deployments","text":"<p>The Deployment resource is a good abstraction over ReplicaSets and Pods to allow a simple declarative rollout that a handful of parameters can tune. However, as we have seen, Deployment does not support more sophisticated strategies like canary or Blue-Green deployments directly. There are higher-level abstractions that enhance Kubernetes by introducing new resource types, enabling the declaration of more flexible deployment strategies. Those extensions all leverage the Operator pattern described in Chapter 28 and introduce their own custom resources for describing the desired rollout behavior.</p> <p>As of 2023, the most prominent platforms that support higher-level Deployments include the following:</p>"},{"location":"kube-patterns-part3/#flagger","title":"Flagger","text":"<p>Flagger implements several deployment strategies and is part of the Flux CD GitOps tools. It supports canary and Blue-Green deployments and integrates with many ingress controllers and service meshes to provide the necessary traffic split between your app's old and new versions. It can also monitor the status of the rollout process based on a custom metric and detect if the rollout fails so that it can trigger an automatic rollback.</p>"},{"location":"kube-patterns-part3/#argo-rollouts","title":"Argo Rollouts","text":"<p>The focus on this part of the Argo family of tools is on providing a comprehensive and opinionated continuous delivery (CD) solution for Kubernetes. Argo Rollouts support advanced deployment strategies, like Flagger, and integrate into many ingress controllers and service meshes. It has very similar capabilities to Flagger, so the decision about which one to use should be based on which CD solution you prefer, Argo or Flux.</p>"},{"location":"kube-patterns-part3/#knative","title":"Knative","text":"<p>Knative is a serverless platform on top of Kubernetes. A core feature of Knative is traffic-driven autoscaling support, which is described in detail in Chapter 29, \"Elastic Scale\". Knative also provides a simplified deployment model and traffic splitting, which is very helpful for supporting high-level deployment rollouts. The support for rollout or rollbacks is not as advanced as with Flagger or Argo Rollouts but is still a substantial improvement over the rollout capabilities of Kubernetes Deployments. If you are using Knative anyway, the intuitive way of splitting traffic between two application versions is a good alternative to Deployments.</p> <p>Like Kubernetes, all of these projects are part of the Cloud Native Computing Foundation (CNCF) project and have excellent community support.</p> <p>Regardless of the deployment strategy you are using, it is essential for Kubernetes to know when your application Pods are up and running to perform the required sequence of steps to reach the defined target deployment state. The next pattern, Health Probe, describes how your application can communicate its health state to Kubernetes.</p>"},{"location":"kube-patterns-part3/#more-information","title":"More Information","text":"<ul> <li>Declarative Deployment Example</li> <li>Performing a Rolling Update</li> <li>Deployments</li> <li>Run a Stateless Application Using a Deployment</li> <li>Blue-Green Deployment</li> <li>Canary Release</li> <li>Flagger: Deployment Strategies</li> <li>Argo Rollouts</li> <li>Knative: Traffic Management</li> </ul>"},{"location":"kube-patterns-part3/#chapter-4-health-probe","title":"Chapter 4: Health Probe","text":"<p>The Health Probe pattern indicates how an application can communicate its health state to Kubernetes. To be fully automatable, a cloud native application must be highly observable by allowing its state to be inferred so that Kubernetes can detect whether the application is up and whether it is ready to serve requests. These observations influence the lifecycle management of Pods and the way traffic is routed to the application.</p>"},{"location":"kube-patterns-part3/#problem","title":"Problem","text":"<p>Kubernetes regularly checks the container process status and restarts it if issues are detected. However, from practice, we know that checking the process status is not sufficient to determine the health of an application. In many cases, an application hangs, but its process is still up and running. For example, a Java application may throw an OutOfMemoryError and still have the JVM process running. Alternatively, an application may freeze because it runs into an infinite loop, deadlock, or some thrashing (cache, heap, process). To detect these kinds of situations, Kubernetes needs a reliable way to check the health of applications\u2014that is, not to understand how an application works internally, but to check whether the application is functioning as expected and capable of serving consumers.</p>"},{"location":"kube-patterns-part3/#solution","title":"Solution","text":"<p>The software industry has accepted the fact that it is not possible to write bug-free code. Moreover, the chances for failure increase even more when working with distributed applications. As a result, the focus for dealing with failures has shifted from avoiding them to detecting faults and recovering. Detecting failure is not a simple task that can be performed uniformly for all applications, as everyone has different definitions of a failure. Also, various types of failures require different corrective actions. Transient failures may self-recover, given enough time, and some other failures may need a restart of the application. Let's look at the checks Kubernetes uses to detect and correct failures.</p>"},{"location":"kube-patterns-part3/#process-health-checks","title":"Process Health Checks","text":"<p>A process health check is the simplest health check the Kubelet constantly performs on the container processes. If the container processes are not running, the container is restarted on the node to which the Pod is assigned. So even without any other health checks, the application becomes slightly more robust with this generic check. If your application is capable of detecting any kind of failure and shutting itself down, the process health check is all you need. However, for most cases, that is not enough, and other types of health checks are also necessary.</p>"},{"location":"kube-patterns-part3/#liveness-probes","title":"Liveness Probes","text":"<p>If your application runs into a deadlock, it is still considered healthy from the process health check's point of view. To detect this kind of issue and any other types of failure according to your application business logic, Kubernetes has liveness probes\u2014regular checks performed by the Kubelet agent that asks your container to confirm it is still healthy. It is important to have the health check performed from the outside rather than in the application itself, as some failures may prevent the application watchdog from reporting its failure. Regarding corrective action, this health check is similar to a process health check, since if a failure is detected, the container is restarted. However, it offers more flexibility regarding which methods to use for checking the application health, as follows:</p> <ul> <li>HTTP probe: Performs an HTTP GET request to the container IP address and expects a successful HTTP response code between 200 and 399.</li> <li>TCP Socket probe: Assumes a successful TCP connection.</li> <li>Exec probe: Executes an arbitrary command in the container's user and kernel namespace and expects a successful exit code (0).</li> <li>gRPC probe: Leverages gRPC's intrinsic support for health checks.</li> </ul> <p>In addition to the probe action, the health check behavior can be influenced with the following parameters:</p> <ul> <li>initialDelaySeconds: Specifies the number of seconds to wait until the first liveness probe is checked.</li> <li>periodSeconds: The interval in seconds between liveness probe checks.</li> <li>timeoutSeconds: The maximum time allowed for a probe check to return before it is considered to have failed.</li> <li>failureThreshold: Specifies how many times a probe check needs to fail in a row until the container is considered to be unhealthy and needs to be restarted.</li> </ul> <p>Example of an HTTP-based liveness probe:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-liveness-check\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    env:\n    - name: DELAY_STARTUP\n      value: \"20\"\n    ports:\n    - containerPort: 8080\n      protocol: TCP\n    livenessProbe:\n      httpGet:                  \n        path: /actuator/health\n        port: 8080\n      initialDelaySeconds: 30   \n</code></pre> <p>Depending on the nature of your application, you can choose the method that is most suitable for you. It is up to your application to decide whether it considers itself healthy or not. However, keep in mind that the result of not passing a health check is that your container will restart. If restarting your container does not help, there is no benefit to having a failing health check as Kubernetes restarts your container without fixing the underlying issue.</p>"},{"location":"kube-patterns-part3/#readiness-probes","title":"Readiness Probes","text":"<p>Liveness checks help keep applications healthy by killing unhealthy containers and replacing them with new ones. But sometimes, when a container is not healthy, restarting it may not help. A typical example is a container that is still starting up and is not ready to handle any requests. Another example is an application that is still waiting for a dependency like a database to be available. Also, a container can be overloaded, increasing its latency, so you want it to shield itself from the additional load for a while and indicate that it is not ready until the load decreases.</p> <p>For this kind of scenario, Kubernetes has readiness probes. The methods (HTTP, TCP, Exec, gRPC) and timing options for performing readiness checks are the same as for liveness checks, but the corrective action is different. Rather than restarting the container, a failed readiness probe causes the container to be removed from the service endpoint and not receive any new traffic.</p> <p>Example of a readiness probe:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-readiness-check\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    readinessProbe:\n      exec:\n        command: [ \"stat\", \"/var/run/random-generator-ready\" ]\n</code></pre>"},{"location":"kube-patterns-part3/#custom-pod-readiness-gates","title":"Custom Pod Readiness Gates","text":"<p>Readiness probes work on a per-container level, and a Pod is considered ready to serve requests when all containers pass their readiness probes. In some situations, this is not good enough\u2014for example, when an external load balancer like the AWS LoadBalancer needs to be reconfigured and ready too. In this case, the readinessGates field of a Pod's specification can be used to specify extra conditions that need to be met for the Pod to become ready.</p> <p>Example of a readiness gate:</p> <pre><code>apiVersion: v1\nkind: Pod\nspec:\n  readinessGates:\n  - conditionType: \"k8spatterns.io/load-balancer-ready\"\nstatus:\n  conditions:\n  - type: \"k8spatterns.io/load-balancer-ready\"\n    status: \"False\"\n  - type: Ready\n    status: \"False\"\n</code></pre>"},{"location":"kube-patterns-part3/#startup-probes","title":"Startup Probes","text":"<p>When applications take minutes to start (for example, Jakarta EE application servers), Kubernetes provides startup probes. Startup probes are configured with the same format as liveness probes but allow for different values for the probe action and the timing parameters.</p> <p>Example of a startup probe:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-startup-check\nspec:\n  containers:\n  - image: quay.io/wildfly/wildfly\n    name: wildfly\n    startupProbe:\n      exec:\n        command: [ \"stat\", \"/opt/jboss/wildfly/standalone/tmp/startup-marker\" ]\n      initialDelaySeconds: 60\n      periodSeconds: 60\n      failureThreshold: 15\n    livenessProbe:\n      httpGet:\n        path: /health\n        port: 9990\n        periodSeconds: 10\n        failureThreshold: 3\n</code></pre>"},{"location":"kube-patterns-part3/#discussion","title":"Discussion","text":"<p>To be fully automatable, cloud native applications must be highly observable by providing a means for the managing platform to read and interpret the application health, and if necessary, take corrective actions. Health checks play a fundamental role in the automation of activities such as deployment, self-healing, scaling, and others.</p> <p>The obvious and old method for this purpose is through logging. It is a good practice for containers to log any significant events to system out and system error and have these logs collected to a central location for further analysis. Apart from logging to standard streams, it is also a good practice to log the reason for exiting a container to /dev/termination-log.</p> <p>Containers provide a unified way for packaging and running applications by treating them like opaque systems. However, any container that is aiming to become a cloud native citizen must provide APIs for the runtime environment to observe the container health and act accordingly.</p> <p>Even-better-behaving applications must also provide other means for the managing platform to observe the state of the containerized application by integrating with tracing and metrics-gathering libraries such as OpenTracing or Prometheus.</p>"},{"location":"kube-patterns-part3/#more-information_1","title":"More Information","text":"<ul> <li>Health Probe Example</li> <li>Configure Liveness, Readiness, and Startup Probes</li> <li>Kubernetes Best Practices: Setting Up Health Checks with Readiness and Liveness Probes</li> <li>Graceful Shutdown with Node.js and Kubernetes</li> <li>Kubernetes Startup Probe\u2014Practical Guide</li> <li>Improving Application Availability with Pod Readiness Gates</li> <li>Customizing the Termination Message</li> <li>SmallRye Health</li> <li>Spring Boot Actuator: Production-Ready Features</li> <li>Advanced Health Check Patterns in Kubernetes</li> </ul>"},{"location":"kube-patterns-part3/#chapter-5-managed-lifecycle","title":"Chapter 5: Managed Lifecycle","text":"<p>Containerized applications managed by cloud native platforms have no control over their lifecycle, and to be good cloud native citizens, they have to listen to the events emitted by the managing platform and adapt their lifecycles accordingly. The Managed Lifecycle pattern describes how applications can and should react to these lifecycle events.</p>"},{"location":"kube-patterns-part3/#problem_1","title":"Problem","text":"<p>In Chapter 4, \"Health Probe\", we explained why containers have to provide APIs for the different health checks. Health-check APIs are read-only endpoints the platform is continually probing to get application insight. It is a mechanism for the platform to extract information from the application.</p> <p>In addition to monitoring the state of a container, the platform sometimes may issue commands and expect the application to react to them. Driven by policies and external factors, a cloud native platform may decide to start or stop the applications it is managing at any moment. It is up to the containerized application to determine which events are important to react to and how to react.</p>"},{"location":"kube-patterns-part3/#solution_1","title":"Solution","text":"<p>We saw that checking only the process status is not a good enough indication of the health of an application. That is why there are different APIs for monitoring the health of a container. Similarly, using only the process model to run and stop a process is not good enough. Real-world applications require more fine-grained interactions and lifecycle management capabilities.</p>"},{"location":"kube-patterns-part3/#sigterm-signal","title":"SIGTERM Signal","text":"<p>Whenever Kubernetes decides to shut down a container, whether that is because the Pod it belongs to is shutting down or simply because a failed liveness probe causes the container to be restarted, the container receives a SIGTERM signal. SIGTERM is a gentle poke for the container to shut down cleanly before Kubernetes sends a more abrupt SIGKILL signal.</p>"},{"location":"kube-patterns-part3/#sigkill-signal","title":"SIGKILL Signal","text":"<p>If a container process has not shut down after a SIGTERM signal, it is shut down forcefully by the following SIGKILL signal. Kubernetes does not send the SIGKILL signal immediately but waits 30 seconds by default after it has issued a SIGTERM signal.</p>"},{"location":"kube-patterns-part3/#poststart-hook","title":"PostStart Hook","text":"<p>Using only process signals for managing lifecycles is somewhat limited. That is why additional lifecycle hooks such as postStart and preStop are provided by Kubernetes.</p> <p>Example of a postStart hook:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: post-start-hook\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    lifecycle:\n      postStart:\n        exec:\n          command:\n          - sh\n          - -c\n          - sleep 30 &amp;&amp; echo \"Wake up!\" &gt; /tmp/postStart_done\n</code></pre>"},{"location":"kube-patterns-part3/#prestop-hook","title":"PreStop Hook","text":"<p>The preStop hook is a blocking call sent to a container before it is terminated. It has the same semantics as the SIGTERM signal and should be used to initiate a graceful shutdown of the container when reacting to SIGTERM is not possible.</p> <p>Example of a preStop hook:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pre-stop-hook\nspec:\n  containers:\n  - image: k8spatterns/random-generator:1.0\n    name: random-generator\n    lifecycle:\n      preStop:\n        httpGet:\n          path: /shutdown\n          port: 8080\n</code></pre>"},{"location":"kube-patterns-part3/#discussion_1","title":"Discussion","text":"<p>One of the main benefits the cloud native platform provides is the ability to run and scale applications reliably and predictably on top of potentially unreliable cloud infrastructure. These platforms provide a set of constraints and contracts for an application running on them. It is in the interest of the application to honor these contracts to benefit from all of the capabilities offered by the cloud native platform.</p> <p>Besides managing the application lifecycle, the other big duty of orchestration platforms like Kubernetes is to distribute containers over a fleet of nodes. The next pattern, Automated Placement, explains the options to influence the scheduling decisions from the outside.</p>"},{"location":"kube-patterns-part3/#more-information_2","title":"More Information","text":"<ul> <li>Managed Lifecycle Example</li> <li>Container Lifecycle Hooks</li> <li>Attach Handlers to Container Lifecycle Events</li> <li>Kubernetes Best Practices: Terminating with Grace</li> <li>Graceful Shutdown of Pods with Kubernetes</li> <li>Argo and Tekton: Pushing the Boundaries of the Possible on Kubernetes</li> <li>Russian Doll: Extending Containers with Nested Processes</li> </ul>"},{"location":"kube-patterns-part3/#chapter-6-automated-placement","title":"Chapter 6: Automated Placement","text":"<p>Automated Placement is the core function of the Kubernetes scheduler for assigning new Pods to nodes that match container resource requests and honor scheduling policies. This pattern describes the principles of the Kubernetes scheduling algorithm and how to influence the placement decisions from the outside.</p>"},{"location":"kube-patterns-part3/#problem_2","title":"Problem","text":"<p>A reasonably sized microservices-based system consists of tens or even hundreds of isolated processes. Containers and Pods do provide nice abstractions for packaging and deployment but do not solve the problem of placing these processes on suitable nodes. With a large and ever-growing number of microservices, assigning and placing them individually to nodes is not a manageable activity.</p> <p>Containers have dependencies among themselves, dependencies to nodes, and resource demands, and all of that changes over time too. The resources available on a cluster also vary over time, through shrinking or extending the cluster or by having it consumed by already-placed containers. The way we place containers impacts the availability, performance, and capacity of the distributed systems as well. All of that makes scheduling containers to nodes a moving target.</p>"},{"location":"kube-patterns-part3/#solution_2","title":"Solution","text":"<p>In Kubernetes, assigning Pods to nodes is done by the scheduler. It is a part of Kubernetes that is highly configurable, and it is still evolving and improving. The Kubernetes scheduler is a potent and time-saving tool. It plays a fundamental role in the Kubernetes platform as a whole, but similar to other Kubernetes components (API Server, Kubelet), it can be run in isolation or not used at all.</p>"},{"location":"kube-patterns-part3/#available-node-resources","title":"Available Node Resources","text":"<p>First of all, the Kubernetes cluster needs to have nodes with enough resource capacity to run new Pods. Every node has capacity available for running Pods, and the scheduler ensures that the sum of the container resources requested for a Pod is less than the available allocatable node capacity.</p> <p>Node capacity is calculated as: <pre><code>Allocatable [capacity for application pods] =\n    Node Capacity [available capacity on a node]\n        - Kube-Reserved [Kubernetes daemons like kubelet, container runtime]\n        - System-Reserved [Operating System daemons like sshd, udev]\n        - Eviction Thresholds [Reserved memory to prevent system OOMs]\n</code></pre></p>"},{"location":"kube-patterns-part3/#container-resource-demands","title":"Container Resource Demands","text":"<p>Another important requirement for an efficient Pod placement is to define the containers' runtime dependencies and resource demands. We covered that in more detail in Chapter 2, \"Predictable Demands\". It boils down to having containers that declare their resource profiles (with request and limit) and environment dependencies such as storage or ports.</p>"},{"location":"kube-patterns-part3/#scheduler-configurations","title":"Scheduler Configurations","text":"<p>The scheduler has a default set of predicate and priority policies configured that is good enough for most use cases. In newer versions of Kubernetes, scheduling profiles are used to achieve customization. This approach exposes the different steps of the scheduling process as extension points and allows you to configure plugins that override the default implementations of the steps.</p> <p>Example of a scheduler configuration:</p> <pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - plugins:\n      score:\n        disabled:\n        - name: PodTopologySpread\n        enabled:\n        - name: MyCustomPlugin\n          weight: 2\n</code></pre> <p>By default, the scheduler uses the default-scheduler profile with default plugins. It is also possible to run multiple schedulers on the cluster, or multiple profiles on the scheduler, and allow Pods to specify which profile to use.</p>"},{"location":"kube-patterns-part3/#node-selection","title":"Node Selection","text":"<p>In most cases, it is better to let the scheduler do the Pod-to-node assignment and not micromanage the placement logic. However, on some occasions, you may want to force the assignment of a Pod to a specific node or group of nodes.</p> <p>Example of using nodeSelector:</p> <p>```yaml apiVersion: v1 kind: Pod metadata:   name: random-generator spec:   containers:   - image: k8spatterns/random-generator:1.0     name: random-generator   nodeSelector:     disktype: ssd</p>"},{"location":"kube-periodic-job/","title":"Periodic Job","text":"<p>The Periodic Job pattern extends the Batch Job pattern by adding a time dimension and allowing the execution of a unit of work to be triggered by a temporal event.</p>"},{"location":"kube-periodic-job/#problem","title":"Problem","text":"<p>In the world of distributed systems and microservices, there is a clear tendency toward real-time and event-driven application interactions using HTTP and lightweight messaging. However, regardless of the latest trends in software development, job scheduling has a long history, and it is still relevant. Periodic jobs are commonly used for automating system maintenance or administrative tasks. They are also relevant to business applications requiring specific tasks to be performed periodically. Typical examples here are business-to-business integration through file transfer, application integration through database polling, sending newsletter emails, and cleaning up and archiving old files.</p> <p>The traditional way of handling periodic jobs for system maintenance purposes has been to use specialized scheduling software or cron. However, specialized software can be expensive for simple use cases, and cron jobs running on a single server are difficult to maintain and represent a single point of failure. That is why, very often, developers tend to implement solutions that can handle both the scheduling aspect and the business logic that needs to be performed.</p> <p>For example, in the Java world, libraries such as Quartz, Spring Batch, and custom implementations with the ScheduledThreadPoolExecutor class can run temporal tasks. But similar to cron, the main difficulty with this approach is making the scheduling capability resilient and highly available, which leads to high resource consumption. Also, with this approach, the time-based job scheduler is part of the application, and to make the scheduler highly available, the whole application must be highly available. Typically, that involves running multiple instances of the application and at the same time ensuring that only a single instance is active and schedules jobs\u2014which involves leader election and other distributed systems challenges.</p> <p>In the end, a simple service that has to copy a few files once a day may end up requiring multiple nodes, a distributed leader election mechanism, and more. Kubernetes CronJob implementation solves all that by allowing scheduling of Job resources using the well-known cron format and letting developers focus only on implementing the work to be performed rather than the temporal scheduling aspect.</p>"},{"location":"kube-periodic-job/#solution","title":"Solution","text":"<p>A CronJob instance is similar to one line of a Unix crontab (cron table) and manages the temporal aspects of a Job. It allows the execution of a Job periodically at a specified point in time.</p>"},{"location":"kube-periodic-job/#example-cronjob-resource","title":"Example CronJob Resource","text":"<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: random-generator\nspec:\n  schedule: \"*/3 * * * *\"  # Cron specification for running every three minutes\n  jobTemplate:\n    spec:\n      template:            # Job template that uses the same specification as a regular Job\n        spec:\n          containers:\n          - image: k8spatterns/random-generator:1.0\n            name: random-generator\n            command: [ \"java\", \"RandomRunner\", \"/numbers.txt\", \"10000\" ]\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"kube-periodic-job/#additional-cronjob-fields","title":"Additional CronJob Fields","text":"<ol> <li><code>.spec.schedule</code></li> <li>Crontab entry for specifying the Job's schedule</li> <li> <p>Can use shortcuts like @daily or @hourly</p> </li> <li> <p><code>.spec.startingDeadlineSeconds</code></p> </li> <li>Deadline (in seconds) for starting the Job if it misses scheduled time</li> <li>Don't use fewer than 10 seconds</li> <li> <p>Useful when task is only valid within certain timeframe</p> </li> <li> <p><code>.spec.concurrencyPolicy</code></p> </li> <li>Manages concurrent executions of Jobs from same CronJob</li> <li> <p>Options:</p> <ul> <li>Allow (default): Creates new Jobs even if previous not completed</li> <li>Forbid: Skip next run if current one not completed</li> <li>Replace: Cancel current Job and start new one</li> </ul> </li> <li> <p><code>.spec.suspend</code></p> </li> <li>Suspends subsequent executions without affecting started executions</li> <li> <p>Different from Job's .spec.suspend</p> </li> <li> <p><code>.spec.successfulJobsHistoryLimit</code> and <code>.spec.failedJobsHistoryLimit</code></p> </li> <li>Specify how many completed/failed Jobs to keep for auditing</li> </ol>"},{"location":"kube-periodic-job/#discussion","title":"Discussion","text":"<p>A CronJob is a simple primitive that adds clustered, cron-like behavior to the existing Job definition. When combined with other primitives such as Pods, container resource isolation, and features like Automated Placement or Health Probe, it becomes a powerful job-scheduling system.</p> <p>This enables developers to focus solely on the problem domain and implement a containerized application responsible only for the business logic. The scheduling is performed outside the application, as part of the platform with benefits like: - High availability - Resiliency - Capacity management - Policy-driven Pod placement</p> <p>Similar to Job implementation, when implementing a CronJob container, your application must consider all corner and failure cases: - Duplicate runs - No runs - Parallel runs - Cancellations</p>"},{"location":"kube-periodic-job/#more-information","title":"More Information","text":"<ul> <li>Periodic Job Example</li> <li>CronJob Documentation</li> <li>Cron</li> <li>Crontab Specification</li> <li>Cron Expression Generator</li> </ul>"},{"location":"kube-portworkx/","title":"Portworx on SUSE Leap 15.5 with RKE2","text":"<p>This guide provides instructions for deploying Portworx on SUSE Leap 15.5 with RKE2 Kubernetes cluster.</p>"},{"location":"kube-portworkx/#quick-start","title":"Quick Start","text":"<pre><code># Install required packages\nexport KUBECTL_SERVER_VERSION=$(kubectl version -oyaml | grep serverVersion: -A5 | grep gitVersion: | awk '{print $2}' | sed 's/+.*//')\n\nkubectl apply -f \"https://install.portworx.com/3.2?comp=pxoperator&amp;kbver=$KUBECTL_SERVER_VERSION&amp;ns=portworx\"\n\nzypper install -y docker python3-pip\n\n# Start and enable docker service\nsystemctl enable --now docker\n\n# Download Portworx Spec Generator\ncurl -o px-spec.yaml \"https://install.portworx.com/3.2?operator=true&amp;mc=false&amp;kbver=$KUBECTL_SERVER_VERSION&amp;ns=portworx&amp;b=true&amp;iop=6&amp;r=17001&amp;c=px-cluster-d7b5a3e7-93c6-4fb5-9bed-3623c2df15c5&amp;osft=true&amp;stork=true&amp;csi=true&amp;tel=true&amp;st=k8s\"\n\n# Apply the spec\nkubectl apply -f px-spec.yaml\n</code></pre>"},{"location":"kube-portworkx/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"kube-portworkx/#manifest-components","title":"Manifest Components","text":"Component Type Purpose portworx-operator Deployment Manages the Portworx cluster lifecycle portworx DaemonSet Runs Portworx storage on each node px-cluster StorageCluster Defines the Portworx storage cluster configuration stork Deployment Storage orchestration for Kubernetes stork-scheduler Deployment Custom scheduler for Portworx volumes px-api Service Exposes Portworx API portworx-service Service Internal communication service"},{"location":"kube-portworkx/#available-storageclasses","title":"Available StorageClasses","text":"<p>Portworx creates several StorageClasses by default:</p> StorageClass Description px-replicated Replication factor of 3 for high availability px-secure-sc Encrypted volumes with replication px-db-sc Optimized for database workloads px-shared-sc ReadWriteMany volumes for shared access"},{"location":"kube-portworkx/#sample-pvc-definitions","title":"Sample PVC Definitions","text":"<pre><code># High Availability PVC\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: px-ha-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: px-replicated\n  resources:\n    requests:\n      storage: 10Gi\n---\n# Encrypted Volume PVC\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: px-secure-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: px-secure-sc\n  resources:\n    requests:\n      storage: 20Gi\n---\n# Shared Volume PVC\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: px-shared-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: px-shared-sc\n  resources:\n    requests:\n      storage: 50Gi\n</code></pre>"},{"location":"kube-portworkx/#verification","title":"Verification","text":"<p>To verify the Portworx installation:</p> <pre><code># Check Portworx pods\nkubectl get pods -n portworx\n\n# Verify StorageClasses\nkubectl get sc\n\n# Check Portworx status\nPX_POD=$(kubectl get pods -l name=portworx -n portworx -o jsonpath='{.items[0].metadata.name}')\nkubectl exec -n portworx $PX_POD -- /opt/pwx/bin/pxctl status\n</code></pre>"},{"location":"kube-presets/","title":"Important precoditions","text":""},{"location":"kube-presets/#1-disable-swap-files-for-the-node","title":"1. disable swap files for the node:","text":"<p><pre><code>free -h\ncat /proc/swaps\n</code></pre> <pre><code># Edit /etc/fstab and comment out any swap lines\nsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\n</code></pre></p>"},{"location":"kube-traefik/","title":"Traefik on RKE2","text":""},{"location":"kube-traefik/#summary","title":"Summary","text":"<p>This guide demonstrates how to deploy Traefik as an ingress controller on RKE2 Kubernetes cluster, including sample configurations for routing traffic to applications.</p>"},{"location":"kube-traefik/#quick-start","title":"Quick Start","text":"<pre><code># Deploy sample web application\nkubectl create deployment whoami --image=traefik/whoami\nkubectl expose deployment whoami --port=80\n\n# Create test ingress route\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: whoami-route\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: PathPrefix(`/test`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\nEOF\n\n# Create Longhorn ingress route\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: longhorn-route\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: PathPrefix(`/longhorn`)\n      kind: Rule\n      services:\n        - name: longhorn-frontend\n          port: 80\nEOF\n</code></pre>"},{"location":"kube-traefik/#architecture","title":"Architecture","text":"<p>```plantuml @startuml !include  <p>Person(user, \"User\", \"External user accessing applications\") Container(traefik, \"Traefik\", \"Ingress Controller\", \"Routes external traffic to services\") Container(whoami, \"Whoami\", \"Demo Application\", \"Simple web service showing request info\") Container(longhorn, \"Longhorn UI\", \"Storage Dashboard\", \"Web interface for Longhorn storage\")</p> <p>Rel(user, traefik, \"HTTP requests\", \"/test, /longhorn\") Rel(traefik, whoami, \"Routes /test\", \"HTTP\") Rel(traefik, longhorn, \"Routes /longhorn\", \"HTTP\") @enduml ```</p>"},{"location":"kube-traefik/#detailed-configuration","title":"Detailed Configuration","text":""},{"location":"kube-traefik/#1-verify-traefik-installation","title":"1. Verify Traefik Installation","text":"<p>RKE2 comes with Traefik installed by default. Verify the installation:</p> <pre><code>kubectl get pods -n kube-system | grep traefik\n</code></pre>"},{"location":"kube-traefik/#2-deploy-sample-application","title":"2. Deploy Sample Application","text":"<p>Deploy the whoami application which will serve as our test endpoint:</p> <pre><code># whoami-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: whoami\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: whoami\n  template:\n    metadata:\n      labels:\n        app: whoami\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: whoami\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: whoami\n</code></pre> <p>Apply the configuration: <pre><code>kubectl apply -f whoami-deployment.yaml\n</code></pre></p>"},{"location":"kube-traefik/#3-configure-test-route","title":"3. Configure Test Route","text":"<p>Create an IngressRoute for the whoami service:</p> <pre><code># whoami-route.yaml\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: whoami-route\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: PathPrefix(`/test`)\n      kind: Rule\n      services:\n        - name: whoami\n          port: 80\n</code></pre> <p>Apply the route: <pre><code>kubectl apply -f whoami-route.yaml\n</code></pre></p>"},{"location":"kube-traefik/#4-configure-longhorn-ui-route","title":"4. Configure Longhorn UI Route","text":"<p>Create an IngressRoute for the Longhorn frontend:</p> <pre><code># longhorn-route.yaml\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: longhorn-route\nspec:\n  entryPoints:\n    - web\n  routes:\n    - match: PathPrefix(`/longhorn`)\n      kind: Rule\n      services:\n        - name: longhorn-frontend\n          port: 80\n</code></pre> <p>Apply the route: <pre><code>kubectl apply -f longhorn-route.yaml\n</code></pre></p>"},{"location":"kube-traefik/#5-verify-routes","title":"5. Verify Routes","text":"<p>Test the configurations:</p> <pre><code># Get Traefik service NodePort\nkubectl get svc -n kube-system traefik\n\n# Test whoami endpoint\ncurl http://&lt;node-ip&gt;:&lt;nodeport&gt;/test\n\n# Access Longhorn UI\ncurl http://&lt;node-ip&gt;:&lt;nodeport&gt;/longhorn\n</code></pre>"},{"location":"kube-traefik/#troubleshooting","title":"Troubleshooting","text":""},{"location":"kube-traefik/#check-traefik-logs","title":"Check Traefik Logs","text":"<pre><code>kubectl logs -n kube-system -l app.kubernetes.io/name=traefik\n</code></pre>"},{"location":"kube-traefik/#verify-ingressroutes","title":"Verify IngressRoutes","text":"<pre><code>kubectl get ingressroutes\nkubectl describe ingressroute whoami-route\nkubectl describe ingressroute longhorn-route\n</code></pre>"},{"location":"kube-traefik/#common-issues","title":"Common Issues","text":"<ol> <li>404 Not Found: Verify service names and ports in IngressRoute configuration</li> <li>503 Service Unavailable: Check if target pods are running and ready</li> <li>Path not working: Ensure PathPrefix in IngressRoute matches the request path</li> </ol>"},{"location":"kubecheat/","title":"Kubectl Common Commands Cheatsheet","text":"Command Description Example <code>kubectl get all -n &lt;namespace&gt;</code> Lists all resources in a namespace (omit -n for current namespace) <code>kubectl get all -n default</code> <code>kubectl get services -n &lt;namespace&gt;</code> Lists services in a namespace (omit -n for current namespace) <code>kubectl get services -n kube-system</code> <code>kubectl get namespaces</code> Lists all namespaces in the cluster <code>kubectl get namespaces</code> <code>kubectl get pods -n &lt;namespace&gt;</code> Lists pods in a namespace (omit -n for current namespace) <code>kubectl get pods -n default</code> <code>kubectl get pods -o wide</code> Lists pods with additional details including node name and IP <code>kubectl get pods -o wide</code> <code>kubectl get pods --watch</code> Watch real-time changes to pods <code>kubectl get pods --watch</code> <code>kubectl get ingress -n &lt;namespace&gt;</code> Lists ingress resources in a namespace (omit -n for current namespace) <code>kubectl get ingress -n default</code> <code>kubectl describe pod &lt;pod-name&gt;</code> Shows detailed information about a specific pod <code>kubectl describe pod nginx-pod</code> <code>kubectl describe service &lt;service-name&gt;</code> Shows detailed information about a specific service <code>kubectl describe service my-service</code> <code>kubectl logs &lt;pod-name&gt;</code> Shows logs for a specific pod <code>kubectl logs nginx-pod</code> <code>kubectl logs -f &lt;pod-name&gt;</code> Follows/streams logs for a specific pod <code>kubectl logs -f nginx-pod</code> <code>kubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;</code> Shows logs for a specific container in a pod <code>kubectl logs nginx-pod -c nginx</code> <code>kubectl exec &lt;pod-name&gt; -- env</code> Lists all environment variables in a pod <code>kubectl exec nginx-pod -- env</code> <code>kubectl apply -f &lt;file&gt;</code> Creates/updates resources defined in a file <code>kubectl apply -f deployment.yaml</code> <code>kubectl apply -f &lt;directory&gt;</code> Creates/updates resources defined in all files in a directory <code>kubectl apply -f ./configs</code> <code>kubectl cluster-info</code> Displays cluster information <code>kubectl cluster-info</code> <code>kubectl cluster-info dump</code> Dumps cluster state for debugging <code>kubectl cluster-info dump</code> <code>kubectl rollout restart deployment &lt;deployment-name&gt;</code> Restarts all pods in a deployment <code>kubectl rollout restart deployment nginx-deployment</code> <code>kubectl rollout status deployment &lt;deployment-name&gt;</code> Shows the status of a deployment rollout <code>kubectl rollout status deployment nginx-deployment</code> <code>kubectl delete pod &lt;pod-name&gt; -n &lt;namespace&gt;</code> Deletes a pod in a namespace (omit -n for current namespace) <code>kubectl delete pod nginx-pod -n default</code> <code>kubectl delete deployment &lt;deployment-name&gt;</code> Deletes a deployment and its pods <code>kubectl delete deployment nginx-deployment</code> <code>kubectl delete service &lt;service-name&gt;</code> Deletes a service <code>kubectl delete service my-service</code> <code>kubectl delete -f &lt;file&gt;</code> Deletes resources defined in a file <code>kubectl delete -f deployment.yaml</code> <code>kubectl delete -l key=value</code> Deletes resources matching a label <code>kubectl delete pods -l app=nginx</code> <code>kubectl delete all --all -n &lt;namespace&gt;</code> Deletes all resources in a namespace <code>kubectl delete all --all -n test-namespace</code>"},{"location":"mkdocscheat/","title":"Markdown cheatsheet","text":"Feature Markdown Syntax Rendered Output Example Headers <code># Header 1</code> <code>## Header 2</code> <code>### Header 3</code> # Header 1  ## Header 2  ### Header 3 Bold <code>**Bold Text**</code> Bold Text Italic <code>*Italic Text*</code> Italic Text Bold &amp; Italic <code>***Bold &amp; Italic***</code> Bold &amp; Italic Blockquote <code>&gt; Blockquote</code> &gt; Blockquote Lists <code>- Item 1</code> <code>- Item 2</code> <code>- Sub-item 2.1</code> - Item 1  - Item 2  \u00a0\u00a0- Sub-item 2.1 Numbered Lists <code>1. Item 1</code> <code>2. Item 2</code> 1. Item 1  2. Item 2 Code (Inline) <code>`inline code`</code> <code>inline code</code> Code Block <code> <code>yaml&lt;br&gt;key: value&lt;br&gt;</code></code> <code>yaml&lt;br&gt;key: value&lt;br&gt;</code> Links <code>[Link Text](https://example.com)</code> Link Text Images <code>![Alt Text](https://example.com/image.png)</code> Horizontal Rule <code>---</code> --- Footnotes <code>Text with footnote[^1]</code> <code>[^1]: Footnote content</code> Text with footnote\u00b9  \u00b9 Footnote content Tables <code>| Column 1 | Column 2 |</code> <code>|----------|----------|</code> <code>| Data 1   | Data 2   |</code> See below for rendered example Task Lists <code>- [ ] Task 1</code> <code>- [x] Completed Task</code> - [ ] Task 1  - [x] Completed Task Admonitions <code>!!! note</code> <code>This is a note.</code> See below for rendered example Code Tabs (Tabs) <code>{% tabs %}</code> <code>{% tab \"Tab 1\" %}</code> <code>Content for Tab 1</code> <code>{% endtab %}</code> <code>{% endtabs %}</code> Tabs (specific to MkDocs Material configuration)"},{"location":"opensuse-disk-management/","title":"OpenSUSE Linux USB Drive Partitioning Guide","text":""},{"location":"opensuse-disk-management/#quick-steps","title":"Quick Steps","text":"<p>Replace <code>X</code> in the commands below with your device letter (b, c, d, etc.):</p> <pre><code># List all disks to find your USB device\nsudo fdisk -l\n\n# Unmount USB if mounted\nsudo umount /dev/sdX1\n\n# Create new partition table and partition\nsudo fdisk /dev/sdX\n# Type these commands in fdisk:\n# d    (delete partition)\n# n    (new partition)\n# p    (primary)\n# 1    (partition number)\n# Enter (default first sector)\n# Enter (default last sector)\n# w    (write changes)\n\n# Format the new partition\nsudo mkfs.ext4 /dev/sdX1\n</code></pre>"},{"location":"opensuse-disk-management/#detailed-instructions","title":"Detailed Instructions","text":""},{"location":"opensuse-disk-management/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenSUSE Linux system</li> <li>Root privileges (sudo access)</li> <li>USB drive to partition</li> </ul>"},{"location":"opensuse-disk-management/#1-identify-your-usb-drive","title":"1. Identify Your USB Drive","text":"<p><pre><code># List all disk devices\nsudo fdisk -l\n</code></pre> Look for your USB drive - it will typically be listed as <code>/dev/sdX</code> where X is a letter (b, c, d, etc.). The drive can be identified by its size and manufacturer information.</p> <p>Warning: Make sure you identify the correct drive to avoid data loss!</p>"},{"location":"opensuse-disk-management/#2-using-fdisk-command-line-method","title":"2. Using fdisk (Command Line Method)","text":""},{"location":"opensuse-disk-management/#start-fdisk","title":"Start fdisk","text":"<pre><code>sudo fdisk /dev/sdX  # Replace X with your device letter\n</code></pre>"},{"location":"opensuse-disk-management/#common-fdisk-commands","title":"Common fdisk Commands","text":"<ul> <li><code>m</code> - Show help menu</li> <li><code>p</code> - Print partition table</li> <li><code>d</code> - Delete a partition</li> <li><code>n</code> - Create new partition</li> <li><code>w</code> - Write changes to disk</li> <li><code>q</code> - Quit without saving</li> </ul>"},{"location":"opensuse-disk-management/#step-by-step-process","title":"Step-by-Step Process","text":"<ol> <li> <p>Delete existing partitions:    <pre><code>Command (m for help): d\nPartition number (1-4): 1\n</code></pre>    Repeat for each partition</p> </li> <li> <p>Create new partition:    <pre><code>Command (m for help): n\nPartition type: p (primary)\nPartition number (1-4): 1\nFirst sector: [Press Enter for default]\nLast sector: [Press Enter for default]\n</code></pre></p> </li> <li> <p>Write changes:    <pre><code>Command (m for help): w\n</code></pre></p> </li> </ol>"},{"location":"opensuse-disk-management/#3-format-the-partition","title":"3. Format the Partition","text":""},{"location":"opensuse-disk-management/#common-filesystem-types","title":"Common Filesystem Types","text":"<ul> <li>ext4 (Linux standard):   <pre><code>sudo mkfs.ext4 /dev/sdX1\n</code></pre></li> <li>FAT32 (Cross-platform compatibility):   <pre><code>sudo mkfs.vfat -F 32 /dev/sdX1\n</code></pre></li> <li>NTFS (Windows compatibility):   <pre><code>sudo mkfs.ntfs /dev/sdX1\n</code></pre></li> </ul>"},{"location":"opensuse-disk-management/#4-using-parted-alternative-method","title":"4. Using parted (Alternative Method)","text":"<p>parted is another command-line tool that's more modern than fdisk:</p> <pre><code># Start parted\nsudo parted /dev/sdX\n\n# Common parted commands\n(parted) print         # Show current partition table\n(parted) mklabel gpt  # Create new GPT partition table\n(parted) mkpart primary ext4 0% 100%  # Create partition\n(parted) quit\n</code></pre>"},{"location":"opensuse-disk-management/#5-using-yast-gui-alternative","title":"5. Using YaST (GUI Alternative)","text":"<ol> <li>Open YaST from system menu</li> <li>Select \"Partitioner\"</li> <li>Find your USB device</li> <li>Use the graphical interface to:</li> <li>Delete existing partitions</li> <li>Create new partitions</li> <li>Format partitions</li> <li>Apply changes</li> </ol>"},{"location":"opensuse-disk-management/#tips","title":"Tips","text":"<ul> <li>Use <code>lsblk</code> to see block devices and their mount points</li> <li>Use <code>blkid</code> to see filesystem types and UUIDs</li> <li>Always unmount drives before partitioning:   <pre><code>sudo umount /dev/sdX1\n</code></pre></li> </ul>"},{"location":"opensuse-disk-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"opensuse-disk-management/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Device is busy:    <pre><code>sudo fuser -m /dev/sdX  # Find processes using the device\nsudo umount -l /dev/sdX  # Force unmount if needed\n</code></pre></p> </li> <li> <p>Permission denied:</p> </li> <li>Ensure you're using sudo</li> <li>Check if device is mounted</li> <li> <p>Verify device permissions</p> </li> <li> <p>Device not showing up:    <pre><code>sudo dmesg | tail  # Check system messages\n</code></pre></p> </li> </ol>"},{"location":"opensuse-disk-management/#safety-tips","title":"Safety Tips","text":"<ul> <li>Always backup important data before partitioning</li> <li>Double-check device name before executing commands</li> <li>Use <code>lsblk</code> or <code>fdisk -l</code> to verify changes</li> <li>Never partition a mounted drive</li> </ul>"},{"location":"opensuse-usb-devices/","title":"OpenSUSE USB and Storage Device Information Guide","text":"<p>This guide explains how to list and get detailed information about USB and storage devices on OpenSUSE Linux using various command-line tools.</p>"},{"location":"opensuse-usb-devices/#prerequisites","title":"Prerequisites","text":"<p>Some tools required for device listing are not installed by default on OpenSUSE Tumbleweed. Install them using:</p> <p><pre><code># Install usbutils for lsusb command\nsudo zypper install usbutils\n\n# Install util-linux for lsblk command\nsudo zypper install util-linux\n</code></pre> thumbleweed manual</p>"},{"location":"opensuse-usb-devices/#basic-device-listing","title":"Basic Device Listing","text":""},{"location":"opensuse-usb-devices/#list-usb-devices","title":"List USB Devices","text":"<pre><code># List all USB devices\nlsusb\n\n# Show detailed information for all USB devices\nlsusb -v\n\n# Show USB device tree\nlsusb -t\n</code></pre>"},{"location":"opensuse-usb-devices/#list-block-devices","title":"List Block Devices","text":"<pre><code># List all block devices with basic info\nlsblk\n\n# Show full device information including filesystem type\nlsblk -f\n\n# Show device size information\nlsblk -b\n\n# Show device topology\nlsblk -t\n</code></pre>"},{"location":"opensuse-usb-devices/#detailed-device-information","title":"Detailed Device Information","text":""},{"location":"opensuse-usb-devices/#using-udevadm","title":"Using udevadm","text":"<pre><code># Get detailed information about a specific device (replace sdX with your device)\nudevadm info --query=all --name=/dev/sdX\n\n# Monitor USB device events in real-time\nudevadm monitor --udev --property\n</code></pre>"},{"location":"opensuse-usb-devices/#using-blkid","title":"Using blkid","text":"<pre><code># Show all block devices with UUID and filesystem type\nsudo blkid\n\n# Get information for a specific device\nsudo blkid /dev/sdX\n</code></pre>"},{"location":"opensuse-usb-devices/#using-dmesg","title":"Using dmesg","text":"<pre><code># Show recent device messages\ndmesg | grep -i usb\n\n# Watch for new USB device connections\ndmesg -w | grep -i usb\n</code></pre>"},{"location":"opensuse-usb-devices/#storage-device-types","title":"Storage Device Types","text":""},{"location":"opensuse-usb-devices/#identify-device-types","title":"Identify Device Types","text":"<pre><code># Show device types and models\nlsblk -d -o name,rota,type,tran,model\n\n# List only USB storage devices\nlsblk -S | grep \"usb\"\n</code></pre> <p>The output columns mean: - <code>rota</code>: 1 for rotational (HDD), 0 for non-rotational (SSD) - <code>type</code>: disk, part (partition), rom, etc. - <code>tran</code>: transport type (sata, usb, nvme) - <code>model</code>: device model name</p>"},{"location":"opensuse-usb-devices/#smart-status-for-storage-devices","title":"Smart Status for Storage Devices","text":"<pre><code># Install smartmontools if not installed\nsudo zypper install smartmontools\n\n# Get SMART information for a device\nsudo smartctl -i /dev/sdX\n\n# Run SMART self-test\nsudo smartctl -t short /dev/sdX\n</code></pre>"},{"location":"opensuse-usb-devices/#examples-with-output","title":"Examples with Output","text":""},{"location":"opensuse-usb-devices/#example-lsusb-output","title":"Example: lsusb Output","text":"<pre><code>Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 001 Device 003: ID 0781:5567 SanDisk Corp. Cruzer Blade\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n</code></pre>"},{"location":"opensuse-usb-devices/#example-lsblk-output","title":"Example: lsblk Output","text":"<pre><code>NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda      8:0    0 931.5G  0 disk \n\u251c\u2500sda1   8:1    0   512M  0 part /boot/efi\n\u251c\u2500sda2   8:2    0    32G  0 part [SWAP]\n\u2514\u2500sda3   8:3    0   899G  0 part /\nsdb      8:16   1    32G  0 disk\n\u2514\u2500sdb1   8:17   1    32G  0 part /media/usb\n</code></pre>"},{"location":"opensuse-usb-devices/#tips-and-tricks","title":"Tips and Tricks","text":"<ol> <li> <p>Quick Device Identification:    <pre><code># Combine tools for quick overview\necho \"=== USB Devices ===\"; lsusb; echo -e \"\\n=== Block Devices ===\"; lsblk -f\n</code></pre></p> </li> <li> <p>Monitor Device Changes:    <pre><code># Watch for device changes in real-time\nwatch -n 1 lsblk\n</code></pre></p> </li> <li> <p>Find Device Serial Numbers:    <pre><code># Get serial numbers for USB devices\nlsusb -v 2&gt;/dev/null | grep -A 2 -B 2 Serial\n</code></pre></p> </li> <li> <p>Check Device Speed:    <pre><code># Show USB device speed\nlsusb -t\n</code></pre></p> </li> </ol>"},{"location":"opensuse-usb-devices/#troubleshooting","title":"Troubleshooting","text":""},{"location":"opensuse-usb-devices/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Device Not Showing Up:    <pre><code># Check system messages\ndmesg | tail\n# Check USB controller status\nlspci | grep -i usb\n</code></pre></p> </li> <li> <p>Permission Issues:    <pre><code># Check device permissions\nls -l /dev/sdX\n# Check if you're in the disk group\ngroups\n</code></pre></p> </li> <li> <p>USB Port Problems:    <pre><code># Check USB host controllers\nlspci -v | grep -A 7 -i \"usb\"\n</code></pre></p> </li> </ol>"},{"location":"opensuse-usb-devices/#safety-tips","title":"Safety Tips","text":"<ul> <li>Always verify device names before running commands</li> <li>Use <code>sudo</code> when required</li> <li>Be careful with device names to avoid data loss</li> <li>Monitor system logs if devices aren't recognized</li> </ul>"},{"location":"rke2-deployment/","title":"RKE2 Default Deployment","text":"<p>rke2 version v1.31.4+rke2r1 30.12.2024</p> <p>RKE2 Quickstart</p> <p>kubeconfig env vaiable required to target kubectl to the file: <pre><code>export KUBECONFIG=/etc/rancher/rke2/rke2.yaml\nkubectl get pods --all-namespaces\nhelm ls --all-namespaces\n</code></pre> cleanup_completed.sh: <pre><code>#!/bin/bash\n\n# List all namespaces\nnamespaces=$(kubectl get namespaces -o jsonpath=\"{.items[*].metadata.name}\")\n\n# Loop through each namespace and delete completed pods\nfor ns in $namespaces; do\n    echo \"Cleaning up completed pods in namespace: $ns\"\n    kubectl delete pod --namespace=$ns --field-selector=status.phase=Succeeded\ndone\n</code></pre> or <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: cleanup-completed-pods\n  namespace: kube-system\nspec:\n  schedule: \"0 0 * * *\"  # Run daily at midnight\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: kubectl\n            image: bitnami/kubectl\n            command:\n            - /bin/sh\n            - -c\n            - |\n              kubectl delete pod --all-namespaces --field-selector=status.phase=Succeeded\n          restartPolicy: OnFailure\n</code></pre></p> <p>This document describes the default deployment of RKE2 components in the <code>kube-system</code> namespace.</p>"},{"location":"rke2-deployment/#core-components","title":"Core Components","text":""},{"location":"rke2-deployment/#control-plane-components","title":"Control Plane Components","text":"<ul> <li>kube-apiserver: The API server is the front-end for the Kubernetes control plane, handling all API operations and serving as the gateway for the cluster.</li> <li>kube-controller-manager: Runs controller processes that regulate the state of the cluster.</li> <li>kube-scheduler: Handles pod scheduling across the cluster nodes.</li> <li>etcd: The distributed key-value store that maintains cluster state.</li> </ul>"},{"location":"rke2-deployment/#networking-components","title":"Networking Components","text":"<ul> <li>rke2-canal: Provides networking and network policy implementation using Calico and Flannel.</li> <li>kube-proxy: Maintains network rules on nodes for pod communication.</li> <li>rke2-ingress-nginx-controller: The NGINX ingress controller for handling incoming traffic to the cluster.</li> </ul>"},{"location":"rke2-deployment/#dns-services","title":"DNS Services","text":"<ul> <li>rke2-coredns: Provides DNS services within the cluster.</li> <li>rke2-coredns-autoscaler: Automatically scales CoreDNS deployments based on cluster load.</li> </ul>"},{"location":"rke2-deployment/#monitoring-and-metrics","title":"Monitoring and Metrics","text":"<ul> <li>rke2-metrics-server: Collects resource metrics from kubelets for horizontal pod autoscaling.</li> </ul>"},{"location":"rke2-deployment/#storage-management","title":"Storage Management","text":"<ul> <li>rke2-snapshot-controller: Manages volume snapshots in the cluster.</li> <li>rke2-snapshot-validation-webhook: Validates volume snapshot operations.</li> </ul>"},{"location":"rke2-deployment/#cloud-integration","title":"Cloud Integration","text":"<ul> <li>cloud-controller-manager: Integrates with underlying cloud provider APIs.</li> </ul>"},{"location":"rke2-deployment/#deployment-diagram","title":"Deployment Diagram","text":""},{"location":"rke2-deployment/#installation-process","title":"Installation Process","text":"<p>The deployment shows several completed Helm installations that set up these components: - helm-install-rke2-canal - helm-install-rke2-coredns - helm-install-rke2-ingress-nginx - helm-install-rke2-metrics-server - helm-install-rke2-snapshot-controller - helm-install-rke2-snapshot-validation-webhook</p> <p>Each component is deployed and managed as part of the RKE2 system, ensuring a production-ready Kubernetes cluster with all necessary services for networking, DNS, metrics collection, and storage management.</p>"},{"location":"rke2-operating-etcd/","title":"RKE2 Operating etcd","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/</p>"},{"location":"rke2-troubleshoot1/","title":"RKE2 Hangs Resolution:","text":"<p>https://forum.uipath.com/t/rke2-stuck-in-activating/570680</p> <p>First, activating can take some time. It is normal for it to take 2-5 minutes. Usually, the logs RKE2 logs need to be checked to diagnose the issue. However, in some cases, this can be caused if there are orphaned RKE2 processes stuck in a running state. Usually, this will not cause an issue, however, if there was a misconfiguration in RKE2 and the process was restarted after the configuration was corrected, these orphaned processes may need to be killed. To take care of this, try running the drain command and then kill all orphaned processes. First, try to drain the node. This command should fail but should be attempted in case any processes need to be terminated. /opt/node-drain.sh (NOTE: It will take around 1.5 minutes to timeout and will end with hostname cannot be found). If the above command does succeed, most likely the service was simply taking a long time to start. In such a case, recheck the status of the RKE2 service. For servers: systemctl restart rke2-server For agents: systemctl restart rke2-agent. If the service is now running, there was no issue but activation took a long time. Run the following command to uncordon the node. /opt/node-drain.sh uncordon If the drain command failed, or the service is not in a running state, then run the following command to kill stop RKE2 and kill any orphaned processes. rke2-killlall.sh Once the above steps are done, try restarting the service. For servers: systemctl restart rke2-server For agents: systemctl restart rke2-agent. Check the status of the service to see if it was able to start. For servers: systemctl restart rke2-server For agents: systemctl restart rke2-agent. If it was still not able to start, then we need to take a look at the logs. For servers: systemctl status rke2-server For agents: systemctl status rke2-agent. If it remains active, something else is wrong and the logs need to be analyzed. First, check to see if the service keeps restarting. journalctl | grep 'Started Rancher Kubernetes Engine' If there are multiple restarts present around the current time, that probably means that the service keeps getting restarted. If this is the case, then the issue is the RKE2 is not starting and is not that the service is stuck in activating. In this scenario see KB article RKE2 Fails To Start. Next, check the journal logs of the machine. To do this run the following command: For a server: journalctl -r -u rke2-server For an agent: journalctl -r -u rke2-agent Analyzing the RKE2 logs in depth is beyond the scope of this article. However here are some simple tips: Look for error messages. Sometimes they explain the exact issue. Failed to test data store connection: this server is not a member of the etcd cluster If this message is encountered, it most likely means the node IP has changed. For multinodes, delete the node and rejoin it. See: Removing A Node From The Cluster For single-node, contact UiPath Support, providing the logs mentioned at the bottom of this article. The etcd store uses the node ip for registration. If that IP changes, then the instance will be seen as unregistered, causing issues. Failed to test the data store connection: connection refused/etcd is not ready Check for the etcd pod logs using crictl to see the errors and that could help find the possible cause. export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml crictl ps -a</p> <p>check the logs for the etcd pod</p> <p>crictl logs </p> <p>Look for the following string to find fatal errors 'level=warning' If none of the above helps identify the issue, generate an RKE2 bundle and open a ticket with UiPath. To generate an RKE2 bundle, follow the steps here: The Rancher v2.x Linux Log Collector Script Also check our docs, in case our troubleshooting guide has been updated for the installation-specific support bundle. If it has, use that instead of the Suse tool. (the RKE2 bundle will be incorporated into our tool in the future). Using Support Bundle Tool. In the ticket opened with UiPath please include the support bundle, the steps taken so for and any details discovered going through this KB article.</p>"},{"location":"streamlit-guide/","title":"Understanding Streamlit for Web App Development","text":""},{"location":"streamlit-guide/#introduction","title":"Introduction","text":"<p>Streamlit is a powerful Python library that enables developers to create web applications with minimal effort. It transforms Python scripts into shareable web apps through a simple, yet powerful API that lets you create everything from data visualizations to machine learning applications.</p>"},{"location":"streamlit-guide/#key-features-and-capabilities","title":"Key Features and Capabilities","text":"<p>Streamlit excels at creating data-focused web applications by providing:</p> <ol> <li>Direct integration with popular data science libraries like Pandas, NumPy, and scikit-learn</li> <li>Real-time data updates and interactive widgets</li> <li>Built-in support for charts and visualizations</li> <li>Automatic responsive layouts</li> <li>Session state management for maintaining user data</li> <li>Easy deployment options through Streamlit Cloud or other platforms</li> </ol>"},{"location":"streamlit-guide/#basic-example","title":"Basic Example","text":"<p>Here's a simple example that demonstrates how to create a Streamlit app:</p> <pre><code>import streamlit as st\nimport pandas as pd\nimport numpy as np\n\n# Create a title for your app\nst.title('My First Streamlit App')\n\n# Add some text\nst.write(\"Welcome to this demonstration of Streamlit's capabilities!\")\n\n# Create a DataFrame with random data\ndf = pd.DataFrame({\n    'Column 1': np.random.randn(5),\n    'Column 2': np.random.randn(5)\n})\n\n# Display the DataFrame\nst.dataframe(df)\n\n# Add a chart\nst.line_chart(df)\n\n# Add an interactive widget\nuser_input = st.slider('Select a value', 0, 100, 50)\nst.write(f'You selected: {user_input}')\n</code></pre>"},{"location":"streamlit-guide/#common-use-cases","title":"Common Use Cases","text":"<p>Streamlit is particularly well-suited for:</p> <ul> <li>Data Visualization Dashboards</li> <li>Machine Learning Model Demonstrations</li> <li>Data Exploration Tools</li> <li>Interactive Reports</li> <li>Prototype Applications</li> <li>Internal Tools</li> </ul>"},{"location":"streamlit-guide/#advantages","title":"Advantages","text":"<p>Streamlit offers several benefits that make it an excellent choice for web app development:</p>"},{"location":"streamlit-guide/#development-speed","title":"Development Speed","text":"<ul> <li>No frontend experience required</li> <li>Single-file applications</li> <li>Hot-reloading during development</li> <li>Minimal boilerplate code</li> </ul>"},{"location":"streamlit-guide/#python-native","title":"Python-Native","text":"<ul> <li>Works seamlessly with Python libraries</li> <li>Familiar syntax for data scientists</li> <li>Easy integration with existing Python code</li> </ul>"},{"location":"streamlit-guide/#interactive-features","title":"Interactive Features","text":"<ul> <li>Built-in widgets and components</li> <li>Real-time updates</li> <li>Session state management</li> <li>File uploaders and downloads</li> </ul>"},{"location":"streamlit-guide/#limitations","title":"Limitations","text":"<p>While powerful, Streamlit does have some constraints:</p> <ul> <li>Less flexible than traditional web frameworks</li> <li>Limited custom styling options</li> <li>Not ideal for complex, multi-page applications</li> <li>May have performance limitations with very large datasets</li> </ul>"},{"location":"streamlit-guide/#getting-started","title":"Getting Started","text":"<p>To begin developing with Streamlit:</p> <pre><code># Install Streamlit\npip install streamlit\n\n# Create a new file (app.py)\n# Run your app\nstreamlit run app.py\n</code></pre>"},{"location":"streamlit-guide/#best-practices","title":"Best Practices","text":"<p>When developing Streamlit applications:</p> <ol> <li>Structure your code logically with clear sections</li> <li>Cache computationally expensive operations using <code>@st.cache</code></li> <li>Use appropriate widgets for user interaction</li> <li>Consider mobile responsiveness</li> <li>Implement error handling for robust applications</li> </ol>"},{"location":"streamlit-guide/#deployment-options","title":"Deployment Options","text":"<p>Streamlit applications can be deployed through:</p> <ul> <li>Streamlit Cloud (formerly Streamlit Sharing)</li> <li>Heroku</li> <li>AWS</li> <li>Google Cloud Platform</li> <li>Private servers</li> </ul>"},{"location":"streamlit-guide/#conclusion","title":"Conclusion","text":"<p>Streamlit represents a significant advancement in creating data-focused web applications with Python. Its simplicity, combined with powerful features, makes it an excellent choice for developers who want to quickly create interactive web applications without dealing with traditional web development complexities.</p> <p>For more information and detailed documentation, visit the official Streamlit website and documentation.</p>"},{"location":"vim-commands/","title":"Vim Quick Reference","text":"Command Description <code>D</code> Delete from cursor to end of line <code>dd</code> Delete current line <code>R</code> Enter Replace mode (overwrite existing text) <code>:w</code> Save changes <code>:q</code> Quit (fails if unsaved changes) <code>:wq</code> or <code>ZZ</code> Save and quit <code>:q!</code> Quit without saving (force quit)"},{"location":"windows-disk-management/","title":"Windows 11 USB Drive Partitioning Guide","text":""},{"location":"windows-disk-management/#quick-steps","title":"Quick Steps","text":"<ol> <li>Press <code>Windows + X</code> \u2192 Select \"Disk Management\"</li> <li>Right-click your USB drive partitions \u2192 Delete Volume</li> <li>Right-click unallocated space \u2192 New Simple Volume</li> <li>Follow wizard: set size \u2192 assign letter \u2192 format (NTFS/FAT32)</li> <li>Click Finish</li> </ol>"},{"location":"windows-disk-management/#detailed-instructions","title":"Detailed Instructions","text":""},{"location":"windows-disk-management/#prerequisites","title":"Prerequisites","text":"<ul> <li>A USB drive that you want to partition</li> <li>Windows 11 operating system</li> <li>Administrative privileges</li> </ul>"},{"location":"windows-disk-management/#steps-to-partition-usb-drive","title":"Steps to Partition USB Drive","text":""},{"location":"windows-disk-management/#1-open-disk-management","title":"1. Open Disk Management","text":"<ol> <li>Right-click on the Start button (Windows icon)</li> <li>Select \"Disk Management\" from the menu</li> <li>Alternatively, press <code>Windows + X</code> and select \"Disk Management\"</li> <li>Or type \"Create and format hard disk partitions\" in the Windows search</li> </ol>"},{"location":"windows-disk-management/#2-identify-your-usb-drive","title":"2. Identify Your USB Drive","text":"<ol> <li>Look for your USB drive in the list of drives</li> <li>Verify it's the correct drive by checking the size</li> <li>IMPORTANT: Make sure you select the correct drive to avoid data loss</li> </ol>"},{"location":"windows-disk-management/#3-delete-existing-partitions-if-needed","title":"3. Delete Existing Partitions (if needed)","text":"<ol> <li>Right-click on each partition of the USB drive</li> <li>Select \"Delete Volume\"</li> <li>Click \"Yes\" to confirm</li> <li>Warning: This will erase all data on the selected partition</li> </ol>"},{"location":"windows-disk-management/#4-create-new-partitions","title":"4. Create New Partitions","text":"<ol> <li>Right-click on the unallocated space</li> <li>Select \"New Simple Volume\"</li> <li>Follow the New Simple Volume Wizard:</li> <li>Choose partition size</li> <li>Assign a drive letter</li> <li>Format the partition (usually NTFS or FAT32)</li> <li>Name your volume (optional)</li> <li>Click \"Finish\"</li> </ol>"},{"location":"windows-disk-management/#5-create-additional-partitions-if-desired","title":"5. Create Additional Partitions (if desired)","text":"<ul> <li>Repeat step 4 for any remaining unallocated space</li> <li>Adjust partition sizes as needed</li> </ul>"},{"location":"windows-disk-management/#tips","title":"Tips","text":"<ul> <li>FAT32 is more compatible but limited to 4GB file size</li> <li>NTFS is Windows-native and supports larger files</li> <li>Consider ExFAT for cross-platform compatibility with large files</li> </ul>"},{"location":"windows-disk-management/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you can't delete partitions, ensure the drive isn't in use</li> <li>If partition creation fails, try:</li> <li>Closing and reopening Disk Management</li> <li>Disconnecting and reconnecting the USB drive</li> <li>Using Command Prompt's <code>diskpart</code> utility as an alternative</li> </ul>"},{"location":"windows-disk-management/#alternative-method-using-diskpart","title":"Alternative Method Using DiskPart","text":"<p>If Disk Management doesn't work, you can use Command Prompt:</p> <ol> <li>Open Command Prompt as Administrator</li> <li>Type <code>diskpart</code></li> <li><code>list disk</code></li> <li><code>select disk X</code> (replace X with your USB drive number)</li> <li><code>clean</code></li> <li><code>create partition primary</code></li> <li><code>format fs=ntfs quick</code></li> <li><code>assign</code></li> </ol> <p>Warning: Be extremely careful with DiskPart commands as selecting the wrong disk can lead to data loss.</p>"},{"location":"wsl2-linux-mint/","title":"Installing Linux Mint with GUI on WSL2 (Windows 11)","text":"<p>This guide walks you through the process of installing Linux Mint with a graphical user interface (GUI) on Windows Subsystem for Linux 2 (WSL2) for Windows 11.</p>"},{"location":"wsl2-linux-mint/#prerequisites","title":"Prerequisites","text":"<ul> <li>Windows 11 (Build 22000 or later)</li> <li>Administrator access to your Windows system</li> <li>At least 8GB of free disk space</li> <li>A working internet connection</li> </ul>"},{"location":"wsl2-linux-mint/#step-1-enable-wsl2","title":"Step 1: Enable WSL2","text":"<ol> <li>Open PowerShell as Administrator</li> <li>Enable WSL and Virtual Machine Platform:    <pre><code>dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\ndism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n</code></pre></li> <li>Restart your computer</li> <li>Set WSL2 as the default version:    <pre><code>wsl --set-default-version 2\n</code></pre></li> </ol>"},{"location":"wsl2-linux-mint/#step-2-download-linux-mint-iso","title":"Step 2: Download Linux Mint ISO","text":"<ol> <li>Visit the Linux Mint download page</li> <li>Download the latest Linux Mint Cinnamon Edition ISO</li> <li>Create a temporary directory to store the ISO:    <pre><code>mkdir C:\\temp\\mint\n</code></pre></li> </ol>"},{"location":"wsl2-linux-mint/#step-3-create-wsl-distro-from-iso","title":"Step 3: Create WSL Distro from ISO","text":"<ol> <li> <p>Download the WSL Distro Creator tool:    <pre><code>curl.exe -L -o C:\\temp\\mint\\WSL-Distro-Creator.zip https://github.com/sileshn/WSL-Distro-Creator/archive/refs/heads/master.zip\nExpand-Archive C:\\temp\\mint\\WSL-Distro-Creator.zip C:\\temp\\mint\n</code></pre></p> </li> <li> <p>Create the WSL distro:    <pre><code>cd C:\\temp\\mint\\WSL-Distro-Creator-master\n.\\WSL-Distro-Creator.cmd\n</code></pre></p> </li> <li> <p>Follow the prompts:</p> </li> <li>Select the Linux Mint ISO</li> <li>Choose a name for your distribution (e.g., \"LinuxMint\")</li> <li>Select installation location</li> </ol>"},{"location":"wsl2-linux-mint/#step-4-initialize-the-wsl-instance","title":"Step 4: Initialize the WSL Instance","text":"<ol> <li>Launch your new Linux Mint distribution from the Start menu</li> <li>Create a user account when prompted</li> <li>Update the system:    <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre></li> </ol>"},{"location":"wsl2-linux-mint/#step-5-install-gui-components","title":"Step 5: Install GUI Components","text":"<ol> <li> <p>Install X server and desktop environment:    <pre><code>sudo apt install -y xfce4 xfce4-goodies\n</code></pre></p> </li> <li> <p>Install additional required packages:    <pre><code>sudo apt install -y dbus-x11 x11-utils x11-xserver-utils\n</code></pre></p> </li> </ol>"},{"location":"wsl2-linux-mint/#step-6-install-x-server-on-windows","title":"Step 6: Install X Server on Windows","text":"<ol> <li>Download and install VcXsrv</li> <li>Launch XLaunch from the Start menu</li> <li>Configure XLaunch:</li> <li>Choose \"Multiple windows\"</li> <li>Display number: 0</li> <li>Start no client</li> <li>Enable \"Disable access control\"</li> <li>Save configuration for future use</li> </ol>"},{"location":"wsl2-linux-mint/#step-7-configure-gui-launch","title":"Step 7: Configure GUI Launch","text":"<ol> <li> <p>Add these lines to your <code>~/.bashrc</code>:    <pre><code>export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):0\nexport LIBGL_ALWAYS_INDIRECT=1\n</code></pre></p> </li> <li> <p>Apply the changes:    <pre><code>source ~/.bashrc\n</code></pre></p> </li> </ol>"},{"location":"wsl2-linux-mint/#step-8-launch-the-gui","title":"Step 8: Launch the GUI","text":"<ol> <li>Start VcXsrv using your saved configuration</li> <li>In your WSL Linux Mint terminal, start Xfce:    <pre><code>startxfce4\n</code></pre></li> </ol>"},{"location":"wsl2-linux-mint/#creating-a-startup-script","title":"Creating a Startup Script","text":"<p>To make launching the GUI easier, you can create a startup script:</p> <ol> <li> <p>Create a new script file:    <pre><code>nano ~/.start-gui.sh\n</code></pre></p> </li> <li> <p>Add the following content:    <pre><code>#!/bin/bash\n\n# Check if VcXsrv is running\nif ! tasklist.exe | grep -q \"vcxsrv.exe\"; then\n    echo \"Starting VcXsrv...\"\n    # Replace the path below with your actual VcXsrv config file path\n    \"/mnt/c/Program Files/VcXsrv/vcxsrv.exe\" :0 -multiwindow -ac &amp;\n    sleep 2\nfi\n\n# Set display\nexport DISPLAY=$(grep -m 1 nameserver /etc/resolv.conf | awk '{print $2}'):0.0\nexport LIBGL_ALWAYS_INDIRECT=1\n\n# Start Xfce\nstartxfce4\n</code></pre></p> </li> <li> <p>Make the script executable:    <pre><code>chmod +x ~/.start-gui.sh\n</code></pre></p> </li> <li> <p>Create an alias for easy access (add to ~/.bashrc):    <pre><code>echo \"alias startgui='~/.start-gui.sh'\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre></p> </li> </ol> <p>Now you can start the GUI environment simply by typing: <pre><code>startgui\n</code></pre></p> <p>The script will: - Check if VcXsrv is already running - Start VcXsrv if needed - Set up the display environment - Launch the Xfce desktop environment</p>"},{"location":"wsl2-linux-mint/#troubleshooting","title":"Troubleshooting","text":""},{"location":"wsl2-linux-mint/#display-issues","title":"Display Issues","text":"<p>If you encounter display issues: <pre><code>export DISPLAY=$(grep -m 1 nameserver /etc/resolv.conf | awk '{print $2}'):0.0\n</code></pre></p>"},{"location":"wsl2-linux-mint/#sound-issues","title":"Sound Issues","text":"<p>To enable sound: <pre><code>sudo apt install -y pulseaudio\n</code></pre></p>"},{"location":"wsl2-linux-mint/#performance-tips","title":"Performance Tips","text":"<ul> <li>Adjust memory usage in <code>.wslconfig</code>:   <pre><code>[wsl2]\nmemory=4GB\nprocessors=2\n</code></pre></li> </ul>"},{"location":"wsl2-linux-mint/#additional-notes","title":"Additional Notes","text":"<ul> <li>The GUI will need to be restarted each time you reboot Windows</li> <li>Save any important work before closing the GUI session</li> </ul>"},{"location":"scrum-master/","title":"Practical Scrum Master Training Program","text":""},{"location":"scrum-master/#overview","title":"Overview","text":"<p>This training program is designed for aspiring Scrum Masters who will be working in software delivery environments. Unlike certification-focused courses, this program emphasizes practical implementation and real-world scenarios that Scrum Masters encounter in software development projects.</p>"},{"location":"scrum-master/#target-audience","title":"Target Audience","text":"<ul> <li>Project managers transitioning to Scrum Master roles</li> <li>Team leads interested in Agile methodologies</li> <li>Professionals seeking practical Scrum implementation knowledge</li> <li>Software development team members moving into Scrum Master positions</li> </ul>"},{"location":"scrum-master/#prerequisites","title":"Prerequisites","text":"<p>Participants should have: - Basic understanding of software development lifecycle - Familiarity with project management concepts - Experience working in software development teams - Basic knowledge of version control and development tools</p>"},{"location":"scrum-master/#program-structure","title":"Program Structure","text":"<ul> <li>Duration: 7 days</li> <li>Format: Interactive workshops and practical exercises</li> <li>Class Size: 8-12 participants (recommended)</li> <li>Delivery: In-person or virtual</li> </ul>"},{"location":"scrum-master/#how-to-use-these-materials","title":"How to Use These Materials","text":""},{"location":"scrum-master/#for-trainers","title":"For Trainers","text":"<ol> <li>Review the complete syllabus to understand the progression of topics</li> <li>Prepare practical exercises based on the outlined modules</li> <li>Adapt the content to your audience's experience level</li> <li>Focus on hands-on activities and real-world scenarios</li> <li>Encourage group discussions and experience sharing</li> </ol>"},{"location":"scrum-master/#for-organizations","title":"For Organizations","text":"<ol> <li>Use as a framework for internal Scrum Master development</li> <li>Customize content based on your technology stack</li> <li>Incorporate your organization's specific processes and tools</li> <li>Plan for post-training mentoring and support</li> </ol>"},{"location":"scrum-master/#training-delivery-guidelines","title":"Training Delivery Guidelines","text":""},{"location":"scrum-master/#preparation","title":"Preparation","text":"<ul> <li>Review participant backgrounds before the session</li> <li>Prepare relevant case studies from your organization</li> <li>Set up required tools and environments</li> <li>Create breakout groups for exercises</li> </ul>"},{"location":"scrum-master/#during-training","title":"During Training","text":"<ul> <li>Maintain 60/40 ratio of practical to theoretical content</li> <li>Use real-world examples from software delivery projects</li> <li>Encourage peer learning and experience sharing</li> <li>Provide immediate feedback on exercises</li> <li>Adapt pace based on group progress</li> </ul>"},{"location":"scrum-master/#post-training","title":"Post-Training","text":"<ul> <li>Provide resources for continued learning</li> <li>Set up mentoring partnerships</li> <li>Plan follow-up sessions</li> <li>Gather feedback for program improvement</li> </ul>"},{"location":"scrum-master/#success-metrics","title":"Success Metrics","text":"<ul> <li>Participant ability to facilitate Scrum events</li> <li>Understanding of software delivery practices</li> <li>Practical application in real projects</li> <li>Team feedback on Scrum implementation</li> <li>Continuous improvement initiatives</li> </ul>"},{"location":"scrum-master/#customization","title":"Customization","text":"<p>This training program can be customized based on: - Organization size and structure - Development methodology - Technology stack - Team maturity level - Industry-specific requirements</p>"},{"location":"scrum-master/#support-materials","title":"Support Materials","text":"<p>Refer to the syllabus.md file for detailed content structure and module information.</p>"},{"location":"scrum-master/#contributing","title":"Contributing","text":"<p>Feel free to submit improvements or customizations to this training program through pull requests. Please ensure any modifications maintain the practical, software delivery focus of the program.</p>"},{"location":"scrum-master/scrum-master/","title":"Scrum Master: Aufgaben und Definition im Detail","text":""},{"location":"scrum-master/scrum-master/#inhaltsverzeichnis","title":"Inhaltsverzeichnis","text":"<ul> <li>Definition Scrum Master (deutsch)</li> <li>Warum braucht man diese Rolle?</li> <li>Scrum Master vs Manager</li> <li>Scrum Master vs Agile Coach und weitere Begriffe</li> <li>Grundlegendes Vorgehen</li> <li>Scrum Master Aufgaben</li> <li>Lehren</li> <li>Mentoring</li> <li>Coaching</li> <li>Mediation von Konflikten</li> </ul> <p>Was ist ein Scrum Master? Aufgabe, Eigenschaften, Ziele und Herausforderungen: Wir starten mit einer kurzen Definition und beschreiben dann im Detail, was es bedeutet eine SM:in zu sein.</p>"},{"location":"scrum-master/scrum-master/#was-ist-ein-scrum-master-definition-und-ubersetzung-auf-deutsch","title":"Was ist ein Scrum Master? Definition und \u00dcbersetzung auf Deutsch","text":"<p>Der Scrum Master ist eine der drei Kernrollen im Scrum Framework. Als Servant Leader unterst\u00fctzt er das Entwicklungsteam, den Product Owner und die Organisation bei der effektiven Anwendung von Scrum. Die Rolle kombiniert F\u00fchrung durch Erm\u00f6glichung mit der Vermittlung und F\u00f6rderung agiler Werte und Prinzipien.</p>"},{"location":"scrum-master/scrum-master/#hauptverantwortlichkeiten","title":"Hauptverantwortlichkeiten","text":"<ol> <li>F\u00fcr das Scrum Team</li> <li>Coaching in Selbstorganisation</li> <li>F\u00f6rderung der funktions\u00fcbergreifenden Zusammenarbeit</li> <li>Beseitigung von Hindernissen</li> <li>Schutz vor \u00e4u\u00dferen St\u00f6rungen</li> <li> <p>F\u00f6rderung der Produktivit\u00e4t</p> </li> <li> <p>F\u00fcr den Product Owner</p> </li> <li>Unterst\u00fctzung bei der Product Backlog-Verwaltung</li> <li>F\u00f6rderung effektiver Kommunikation</li> <li> <p>Hilfe bei der agilen Produktplanung</p> </li> <li> <p>F\u00fcr die Organisation</p> </li> <li>F\u00fchrung bei der Scrum-Einf\u00fchrung</li> <li>Unterst\u00fctzung bei organisatorischen Ver\u00e4nderungen</li> <li>Training und Coaching von Teams und Stakeholdern</li> </ol>"},{"location":"scrum-master/scrum-master/#kernkompetenzen","title":"Kernkompetenzen","text":"<ul> <li>Tiefes Verst\u00e4ndnis der Scrum-Theorie</li> <li>Exzellente Kommunikationsf\u00e4higkeiten</li> <li>Moderationskompetenz</li> <li>Coaching-F\u00e4higkeiten</li> <li>Konfliktmanagement</li> <li>Change Management</li> </ul>"},{"location":"scrum-master/scrum-master/#herausforderungen-und-best-practices","title":"Herausforderungen und Best Practices","text":"<p>H\u00e4ufige Herausforderungen: - Widerstand gegen Ver\u00e4nderungen - Unklare Rollenabgrenzung - Organisatorische Hindernisse - Mangelndes Verst\u00e4ndnis agiler Werte</p> <p>Best Practices: - Regelm\u00e4\u00dfige Retrospektiven - Kontinuierliche Weiterbildung - Aufbau eines Scrum Master Netzwerks - Dokumentation von Lessons Learned</p>"},{"location":"scrum-master/scrum-master/#fazit","title":"Fazit","text":"<p>Die Rolle des Scrum Masters ist entscheidend f\u00fcr den Erfolg von Scrum-Teams und agilen Transformationen. Durch die Kombination von F\u00fchrungsqualit\u00e4ten, methodischem Wissen und Soft Skills tr\u00e4gt der Scrum Master ma\u00dfgeblich zur kontinuierlichen Verbesserung und zum Erfolg des Teams bei.</p>"},{"location":"scrum-master/syllabus/","title":"Scrum Master Training Syllabus","text":""},{"location":"scrum-master/syllabus/#software-delivery-focus","title":"Software Delivery Focus","text":""},{"location":"scrum-master/syllabus/#course-overview","title":"Course Overview","text":"<p>This practical training program is designed to equip participants with the necessary skills and knowledge to effectively serve as Scrum Masters in software delivery projects. The focus is on real-world implementation rather than certification preparation.</p>"},{"location":"scrum-master/syllabus/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this training, participants will be able to: - Facilitate Scrum events effectively - Coach teams in Agile practices - Remove impediments and foster continuous improvement - Guide successful software delivery using Scrum framework - Promote effective collaboration between development teams and stakeholders</p>"},{"location":"scrum-master/syllabus/#module-1-agile-and-scrum-foundations-1-day","title":"Module 1: Agile and Scrum Foundations (1 Day)","text":"<ul> <li>Agile Manifesto and Principles</li> <li>Scrum Framework Overview</li> <li>Empirical Process Control</li> <li>Value-Driven Delivery</li> </ul>"},{"location":"scrum-master/syllabus/#module-2-scrum-master-role-deep-dive-1-day","title":"Module 2: Scrum Master Role Deep Dive (1 Day)","text":"<ul> <li>Core Responsibilities</li> <li>Servant Leadership in Practice</li> <li>Stakeholder Management</li> <li>Common Challenges and Solutions</li> </ul>"},{"location":"scrum-master/syllabus/#module-3-practical-project-facilitation-2-days","title":"Module 3: Practical Project Facilitation (2 Days)","text":""},{"location":"scrum-master/syllabus/#day-1-scrum-events-in-practice","title":"Day 1: Scrum Events in Practice","text":"<ul> <li>Sprint Planning Techniques</li> <li>Daily Scrum Facilitation</li> <li>Sprint Review Best Practices</li> <li>Retrospective Techniques</li> </ul>"},{"location":"scrum-master/syllabus/#day-2-artifacts-and-tracking","title":"Day 2: Artifacts and Tracking","text":"<ul> <li>Product Backlog Management</li> <li>Sprint Backlog Maintenance</li> <li>Definition of Done</li> <li>Progress Tracking Tools</li> </ul>"},{"location":"scrum-master/syllabus/#module-4-software-delivery-excellence-2-days","title":"Module 4: Software Delivery Excellence (2 Days)","text":""},{"location":"scrum-master/syllabus/#day-1-technical-practices","title":"Day 1: Technical Practices","text":"<ul> <li>CI/CD Pipeline Understanding</li> <li>Quality Practices in Scrum</li> <li>Technical Debt Management</li> <li>Release Planning</li> </ul>"},{"location":"scrum-master/syllabus/#day-2-team-dynamics","title":"Day 2: Team Dynamics","text":"<ul> <li>Cross-functional Team Building</li> <li>Developer-QA Collaboration</li> <li>DevOps Culture</li> <li>Handling Dependencies</li> </ul>"},{"location":"scrum-master/syllabus/#module-5-advanced-topics-1-day","title":"Module 5: Advanced Topics (1 Day)","text":"<ul> <li>Scaling Scrum Practices</li> <li>Metrics and KPIs</li> <li>Risk Management</li> <li>Continuous Improvement</li> </ul>"},{"location":"scrum-master/syllabus/#practical-components","title":"Practical Components","text":"<ul> <li>Role-playing exercises</li> <li>Real-world case studies</li> <li>Team simulation activities</li> <li>Tool demonstrations</li> </ul>"},{"location":"scrum-master/syllabus/#assessment-methods","title":"Assessment Methods","text":"<ul> <li>Daily practical exercises</li> <li>Team-based assignments</li> <li>Final project simulation</li> <li>Continuous feedback</li> </ul>"},{"location":"scrum-master/syllabus/#required-tools-and-materials","title":"Required Tools and Materials","text":"<ul> <li>Agile project management tools (Jira/Trello)</li> <li>Collaboration tools (Miro/Mural)</li> <li>Version control basics (Git)</li> <li>Documentation tools</li> </ul>"},{"location":"scrum-master/syllabus/#references-and-resources","title":"References and Resources","text":"<ul> <li>Scrum Guide</li> <li>Agile Project Management Resources</li> <li>Technical Practice Guidelines</li> <li>Recommended Reading List</li> </ul>"},{"location":"scrum-master/syllabus/#notes-for-trainers","title":"Notes for Trainers","text":"<ul> <li>Focus on practical examples over theoretical concepts</li> <li>Encourage participant sharing of real experiences</li> <li>Adapt content based on group's software delivery background</li> <li>Include regular hands-on exercises</li> <li>Maintain balance between process and technical aspects</li> </ul>"}]}